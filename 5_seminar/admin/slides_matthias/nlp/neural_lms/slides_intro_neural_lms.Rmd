## What is a "language model"?
```{r, include=FALSE, cache=FALSE}
ap = adjust_path(paste0(getwd(), "/figure"))
```

- Wikipedia says:  
    "A statistical language model is a probability distribution over sequences of words"
- This means

    (a) assigning a marginal probability to a sequence of words, e.g.  
    \[
    P(\mbox{\it "we are all interested in NLP"})
    \]
    
    (b) assigning a conditional probability to the likelihood of a word given a sequence of words, e.g.
    \[
    P(\mbox{\it "NLP"}|\mbox{\it "we are all interested in"})
    \]
    
## Making use of the Markov-Assumption

- The Markov-Assumption:

    + "The future is independent of the past given the present"
    + In NLP context:  
      $\rightarrow$ Next word only depends on the $k$ previous words  
      $\rightarrow$ $k$th order markov assumption with k to be chosen manually

- "Traditional" count-based models:

    + Good baselines, but severe shortcomings
<!--- + Application of different smoothing techniques (e.g. *Kneser-Ney*)
    + Use of the "back-off" technique (e.g. *Katz's back-off model*)
--->
    + Lacking the ability to generalize 
    
## Problems of the Markov-Assumption

- Curse of dimensionality:

    + Increasing context size leads to exponential increase in $\#$parameters
    + E.g. considering a vocabulary of size $|V| = 1.000.000$:  
      $\rightarrow$ Already for bi-grams we have $|V|^2 = 10^{12}$ possible combinations
      
- Sparsity:

    + Again, considering $|V| = 1.000.000$ \& bi-grams as context
    + Unlikely to observe all of them

        (a) ever
        (b) often
 
## Bengio et al. (NIPS 2001; JMLR 2003)

\begin{figure}
\centering
\includegraphics[width = 10cm]{`r ap("bengio01.png")`}\\ 
\footnotesize{Source:} \href{https://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model.pdf}{\footnotesize Bengio et al. (2001)}
\end{figure}

* First researchers to propose a __neural__ language model
* Computational power was prohibitive at this time
* Not possible to train these models on large vocabularies
        
## Neural probabilistic language model

- Neural networks induce non-linearity and overcome the shortcomings of traditional models
    
    (a) Only linear increase in #parameters with increasing context size
    (b) Better generalization
<!--- 
    (c) No need to manually design back-off orders 
--->
        
- Input: 
    + *Context of $(n-1)$ words* \hfill $[w_{(t-n+1):(t-1)}]$
- Hidden layer:
    + *Look-up table* \hfill $[C(w_{t-n+1}); .. ; C(w_{t-2}); C(w_{t-1})]$
    + *Non-linearity* \hfill e.g. tanh, ReLU
        
- Output:
    + *Probability distribution* \hfill $P(w_t = i|w_{(t-n+1):(t-1)})$
       
## Bengio et al. (2003)

\begin{figure}
\centering
\includegraphics[width = 8.5cm]{`r ap("bengio03.png")`}\\ 
\footnotesize{Source:} \href{http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf}{\footnotesize Bengio et al. (2003)}
\end{figure}
    
## What could be problematic?

- Compuational cost

    + Cross-entropy loss is computationally expensive
    + Proposed solutions:
      1. Hierarchical softmax (\href{http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=108935687561E6B7785F22984718A439?doi=10.1.1.88.9794&rep=rep1&type=pdf}{\it Morin and Bengio (2005)})
      2. Sampling approaches (Next chapters)

- Still relying on the markov assumption

    + Context window has to be specified manually
    
