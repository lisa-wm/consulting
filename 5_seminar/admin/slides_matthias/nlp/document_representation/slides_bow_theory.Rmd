## One-hot encoding
```{r, include=FALSE, cache=FALSE}
ap = adjust_path(paste0(getwd(), "/figure"))
```

- An easy way to represent categorial covariates  
(e.g. characters or words)
- A covariate with $k$ categories is represented by $k-1$ new variables
- An example on character-level:

\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{c||cccccccccccccccc}
  & $a$ & ... & $e$ & ... & $h$ & ... & $l$ & ... & $o$ & ... & $z$ & . & , & ! & ?\\
\hline 
h	& 0   & ... & 0   & ... & 1   & ... & 0   & ... & 0   & ... & 0   & 0 & 0 & 0 & 0\\
e & 0   & ... & 1   & ... & 0   & ... & 0   & ... & 0   & ... & 0   & 0 & 0 & 0 & 0\\ 
l & 0   & ... & 0   & ... & 0   & ... & 1   & ... & 0   & ... & 0   & 0 & 0 & 0 & 0\\
l & 0   & ... & 0   & ... & 0   & ... & 1   & ... & 0   & ... & 0   & 0 & 0 & 0 & 0\\
o & 0   & ... & 0   & ... & 0   & ... & 0   & ... & 1   & ... & 0   & 0 & 0 & 0 & 0\\
$\vdots$ &&&&&&&&&&&&&&&\\ 
\end{tabular}

## One-hot encoding

- It is also easy to do this on the word-level:

\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{c||cccccccccccccc}
        & ate & ... & cake & ... & I   & ... & some & ... & zebra & . & ! & ?\\
\hline 
I       & 0   & ... & 0    & ... & 1   & ... & 0    & ... & 0     & 0 & 0 & 0\\
ate     & 1   & ... & 0    & ... & 0   & ... & 0    & ... & 0     & 0 & 0 & 0\\ 
some    & 0   & ... & 0    & ... & 0   & ... & 1    & ... & 0     & 0 & 0 & 0\\
cake    & 0   & ... & 1    & ... & 0   & ... & 0    & ... & 0     & 0 & 0 & 0\\
$\vdots$ &&&&&&&&&&&&&\\ 
\end{tabular}

## One-hot encoding

- Straight-forward extension for documents (on the word-level) would be:
    + One row per document 
    + One column per word in the vocabulary
- Entries of the matrix are binary indicators of the word occurence,  
  so either take the value zero or one

\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{c||cccccc}
  & $word_1$ & $word_2$ & $word_3$ & ... & $word_V$\\
\hline 
$doc_1$	& 1   & 1 & 0 & ... & 0\\
$doc_2$ & 0   & 1 & 1 & ... & 1\\ 
$doc_3$ & 1   & 0 & 1 & ... & 1\\
$\vdots$ &&&&&\\ 
$doc_n$ & 1   & 0 & 0 & ... & \\
\end{tabular}

## Bag-of-words model

- Extending one-hot encoding by using _counts_ instead of binary indicators
- Represent the corpus as a matrix with dimension $n \times V$
- Example corpus:

```{python}
a = "i like watching football on tv"
b = "football players play football every saturday"
corpus = [a, b]
```  

- Resulting Bag-of-words representation

\footnotesize
\centering
\begin{tabular}{c|ccccccccccc}
	    &every	&football	&i	&like	&on	&play	&players	&Saturday	&tv	&watching\\
	    \hline
a	&0	    &1		    &1	&1	    &1	&0 	    &0		    &0		    &1  &1\\
b	&1	    &2		    &0	&0	    &0 	&1	    &1		    &1	    	&0  &0
\end{tabular}  

- This matrix is called the __Document-Term-Matrix (DTM)__

## Bag-of-words model

__Document-Term-Matrix (DTM):__

- One row per document in the training corpus
- Columns contain the (ordered) words from the training vocabulary
- Words counts are displayed in the cells  
  (Binary indicators also possible)
- We have obtained a perfectly $n \times p$ data set structure
    + Documents are the _observations_
    + Word counts/occurences are the _covariates_