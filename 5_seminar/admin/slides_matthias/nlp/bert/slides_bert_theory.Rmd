## BERT \href{https://arxiv.org/pdf/1810.04805.pdf}{\beamergotobutton{Devlin et al. (2018)}}
```{r, include=FALSE, cache=FALSE}
ap = adjust_path(paste0(getwd(), "/figure"))
```

\begin{figure}
\centering
\includegraphics[width = 3cm]{`r ap("bert.jpeg")`}
\end{figure}

__*B*idirectional *E*ncoder *R*epresentations from *T*ransformers:__

- Bidirectionally contextual model
- Introduces new self-supervised objective
- Completely replaces recurrent architectures by Self-Attention  
  $+$ simultaneously able to include bidirectionality


## BERT \href{https://arxiv.org/pdf/1810.04805.pdf}{\beamergotobutton{Devlin et al. (2018)}}

\begin{figure}
\centering
\includegraphics[width = 3.2cm]{`r ap("bert-top.png")`}\\ 
\includegraphics[width = 3cm]{`r ap("bert-bottom.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1706.03762.pdf}{\footnotesize Vaswani et al. (2017)}
\end{figure}


## Predecessors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=1]{`r ap("transfer_learning_timeline1_nlp.pdf")`}}

## Predecessors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=2]{`r ap("transfer_learning_timeline1_nlp.pdf")`}}
\addtocounter{framenumber}{-1}

## Predecessors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=3]{`r ap("transfer_learning_timeline1_nlp.pdf")`}}
\addtocounter{framenumber}{-1}

## Predecessors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=4]{`r ap("transfer_learning_timeline1_nlp.pdf")`}}
\addtocounter{framenumber}{-1}

## Predecessors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=5]{`r ap("transfer_learning_timeline1_nlp.pdf")`}}
\addtocounter{framenumber}{-1}


## BERT \href{https://arxiv.org/pdf/1810.04805.pdf}{\beamergotobutton{Devlin et al. (2018)}}

__Since 2019:__

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("google-bert.png")`}\\ 
\footnotesize{Source:} \href{https://syncedreview.com/2019/10/25/milestone-bert-boosts-google-search/}{\footnotesize Synced}
\end{figure}

\vspace{.3cm}

Corresponding blog post by Google:  
\href{https://www.blog.google/products/search/search-language-understanding-bert/}{\scriptsize https://www.blog.google/products/search/search-language-understanding-bert/}
  

## A remark on Self-Supervision

__Causality is an issue!__

- _Remember:_ Input and target sequences are the same  
  $\rightarrow$ We modify the input to create a meaningful task
- A sequence is used to predict itself again
- Bidirectionality at a lower layer would allow a word to see itself at later hidden layers  
  $\rightarrow$ The model would be allowed to cheat!  
  $\rightarrow$ This would not lead to meaningful internal representations


## GPT vs. ELMo vs. BERT

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("comparison-bert.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1810.04805.pdf}{\footnotesize Devlin et al. (2018)}
\end{figure}

__Major architectural differences:__

- ELMo uses to separate unidirectional models to achieve bidirectionality
  $\rightarrow$ Only "_shallow_" bidirectionality
- GPT is not bidirectional, thus no issues concerning causality
- BERT combines the best of both worlds: $$Self\text{-}Attention + (Deep)\;Bidirectionality$$

## Masked Language Modeling (_MLM_)

__First of all:__

- It has _nothing to do_ with Masked Self-Attenion  
  $\rightarrow$ Masked Self-Attention is an architectural detail in the decoder of a Transformer, i.e. used by e.g. GPT
- Masked Self-Attention as a way to induce causality in the decoder
- MLM is a modeling objective introduced to couple Self-Attention and (deep) bidirectionality without violating causality


## Masked Language Modeling (_MLM_)

__Masked Language Modeling:__

- _Training objective:_ $$\text{Given a sentence, predict \texttt{[MASK]}ed tokens}$$
- _Generation of samples:_ $$\text{Randomly replace* a fraction of the words by \texttt{[MASK]}}$$
  \scriptsize *Sample 15% of the tokens; replace 80% of them by \texttt{[MASK]}, 10% by a random token & leave 10% unchanged
- \normalsize _Input:_  

\footnotesize
\begin{tabular}{|cccccccccc|}
\hline
The & quick & brown & \cellcolor{blue!65}\texttt{[MASK]} & jumps & over & the & \cellcolor{blue!65}\texttt{[MASK]} & dog & . \\
\hline
\end{tabular}

- \normalsize _Targets:_ $$(fox,\; lazy)$$


## Next Sentence Prediction (_NSP_)

__Next Sentence Prediction:__

- _Training objective:_ $$\text{Given two sentences, predict whether $s_2$ follows $s_1$}$$
- _Generation of samples:_ $$\text{Randomly sample negative examples (cf. word2vec)}$$
- _Full Input:_  

\footnotesize
\begin{center}
\begin{tabular}{|cccccccc|}
\hline
\cellcolor{blue!15}\texttt{[CLS]} & The & \cellcolor{blue!65}\texttt{[MASK]} & is & quick & . & \cellcolor{blue!15}\texttt{[SEP]} &\\\hline\hline It & jumps & over & the & \cellcolor{blue!65}\texttt{[MASK]} & dog & . & \cellcolor{blue!15}\texttt{[SEP]} \\
\hline
\end{tabular}
\end{center}

- \normalsize \texttt{[CLS]} token as sequence representation for classification
- \texttt{[SEP]} token for separation of the two input sequences


## Pre-training BERT

__Ingredients:__

- Massive lexical resources (BooksCorpus $+$ Eng. Wikipedia)  
  $\rightarrow$ 13 GB in total
- 4 (16) \href{https://cloud.google.com/tpu/}{Cloud TPUs} for 4 days for the BASE (LARGE) variant
- 12 (24) Transformer encoder blocks with 12 (16) attenzion heads and an embedding size of 768 (1024)  
  $\rightarrow$ 110M (340M) model parameters in total
- Loss function: $$Loss_{BERT} = Loss_{MLM} + Loss_{NSP}$$
- Byte-Pair encoding \href{https://www.aclweb.org/anthology/P16-1162.pdf}{\beamergotobutton{Sennrich et al. (2015)}} for the inputs  
  $\rightarrow$ Vocabulary of 30.000 tokens


## Fine-tuning BERT

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("bert-tasks.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1810.04805.pdf}{\footnotesize Devlin et al. (2018)}
\end{figure}


## Fine-tuning BERT

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("bert-sota.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1810.04805.pdf}{\footnotesize Devlin et al. (2018)}
\end{figure}

- Performance of BERT on the \href{https://gluebenchmark.com/}{\beamergotobutton{GLUE Benchmark}}
- Beats all of the previous state-of-the-art models
- In the meantime: Other model better than BERT


## Successors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=1]{`r ap("transfer_learning_timeline2_nlp.pdf")`}}

## Successors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=2]{`r ap("transfer_learning_timeline2_nlp.pdf")`}}
\addtocounter{framenumber}{-1}

## Successors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=3]{`r ap("transfer_learning_timeline2_nlp.pdf")`}}
\addtocounter{framenumber}{-1}

## Successors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=4]{`r ap("transfer_learning_timeline2_nlp.pdf")`}}
\addtocounter{framenumber}{-1}

## Successors of BERT

\hbox{\hspace{-6.5em} \includegraphics[width=14cm,page=5]{`r ap("transfer_learning_timeline2_nlp.pdf")`}}
\addtocounter{framenumber}{-1}


## RoBERTa \href{https://arxiv.org/pdf/1907.11692.pdf}{\beamergotobutton{Liu et al., 2019}}

__Improvements in Pre-Training:__

- Authors claim that BERT is seriously undertrained
- Change of the \texttt{MASK}ing strategy  
  $\rightarrow$ BERT masks the sequences once before pre-training  
  $\rightarrow$ RoBERTa uses dynamic \texttt{MASK}ing  
  $\Rightarrow$ RoBERTa sees the same sequence \texttt{MASK}ed differently
- RoBERTa does not use the additional NSP objective during pre-training
- 160 GB of pre-training resources instead of 13 GB
- Pre-training is performed with larger batch sizes


## RoBERTa \href{https://arxiv.org/pdf/1907.11692.pdf}{\beamergotobutton{Liu et al., 2019}}

__Architectural differences:__

- Architecture (layers, heads, embedding size) identical to BERT
- 50k token BPE vocabulary instead of 30k
- Model size differs (due to the larger embedding matrix)  
  $\Rightarrow$ $\sim$ 125M (360M) for the BASE (LARGE) variant 

__Performance differences:__

\vspace{-.5cm}

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("roberta-sota.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1907.11692.pdf}{\footnotesize Liu et al. (2019)}
\end{figure}
\footnotesize
- _Note:_ Liu et al. (2019) report the accuracy for QQP while Devlin et al. (2018) report the F1 score (cf. results displayed on slide 65)


## ALBERT \href{https://arxiv.org/pdf/1909.11942.pdf}{\beamergotobutton{Lan et al., 2019}}

\textbf{Changes in the architecture:}

- Disentanglement of embedding size $E$ and hidden layer size $H$  
    $\rightarrow$ WordPiece-Embeddings (size $E$) context-independent  
		$\rightarrow$ Hidden-Layer-Embeddings (size $H$) context-dependent  
		$\Rightarrow$ Setting $H >> E$ enlargens model capacity without increasing the size of the embedding matrix,  
		since $O(V \times H) > O(V \times E +  E \times H)$ if $H >> E$.
- Cross-Layer parameter sharing
- Change of the pre-training NSP loss  
    $\rightarrow$ Introduction of \textit{Sentence-Order Prediction} (SOP)  
		$\rightarrow$ Positive examples created alike to those from NSP  
		$\rightarrow$ Negative examples: Just swap the ordering of sentences  
- $n-gram$ masking for the MLM task



## ALBERT \href{https://arxiv.org/pdf/1909.11942.pdf}{\beamergotobutton{Lan et al., 2019}}

\textbf{Performance differences:}

\begin{figure}
		\centering
		\includegraphics[width = 11cm]{`r ap("albert-sota.png")`}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1907.11942.pdf}{\footnotesize Lan et al. (2019)}
\end{figure}

\textbf{Notes:}

- In General: Smaller model size (because of parameter sharing)
- Nevertheless: Scale model up to almost similar size (\texttt{xxlarge} version)
- Strong performance compared to BERT



## A note on Google's T5 \href{https://arxiv.org/pdf/1910.10683.pdf}{\beamergotobutton{Raffel et al., 2019}}

__Post-BERT architectures:__

- Most architectures still rely on either an encoder- _or_ a decoder-style type of model (e.g. \href{https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}{\beamergotobutton{GPT2}}, \href{https://arxiv.org/pdf/1906.08237.pdf}{\beamergotobutton{XLNet}})
- _BERTology:_ Many papers/models which aim at ..
    - .. explanining BERT (e.g. \href{https://arxiv.org/pdf/1906.02715.pdf}{\beamergotobutton{Coenen et al., 2019}}, \href{https://arxiv.org/pdf/1905.10650.pdf}{\beamergotobutton{Michel et al., 2019}})
    - .. improving BERT (\href{https://arxiv.org/pdf/1907.11692.pdf}{\beamergotobutton{RoBERTa}}, \href{https://arxiv.org/pdf/1909.11942.pdf}{\beamergotobutton{ALBERT}})
    - .. making BERT more efficient (\href{https://arxiv.org/pdf/1909.11942.pdf}{\beamergotobutton{ALBERT}}, \href{https://arxiv.org/pdf/1910.01108.pdf}{\beamergotobutton{DistilBERT}})
- Overview on the many different papers:  
  \href{https://github.com/tomohideshibata/BERT-related-papers}{https://github.com/tomohideshibata/BERT-related-papers}
  
    
__Text-to-Text Transfer Transformer:__

- A complete encoder-decoder Transformer architecture
- All tasks reformulated as text-to-text tasks
- From BERT-size up to 11 Billion parameters


## T5 visualized

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("t5.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1910.10683.pdf}{\footnotesize Raffel et al. (2019)}
\end{figure}



## Model distillation \href{https://arxiv.org/pdf/1503.02531.pdf}{\beamergotobutton{Hinton et al. (2015)}}

\textbf{Model compression scheme:}

- Motivation comes from having computationally expensive, cumbersome ensemble models. \href{http://www.niculescu-mizil.org/papers/rtpp364-bucila.rev2.pdf}{\beamergotobutton{Bucila et al. (2006)}}
- Compressing the knowlegde of the ensemble into a single model has the benefit of easier deployment and better generalization
- Reasoning:
    + Cumbersome model generalizes well, because it is the average of an ensemble.
		+ Small model trained to generalize in the same way typically better than small model trained "the normal way".




## DistilBERT \href{https://arxiv.org/pdf/1910.01108.pdf}{\beamergotobutton{Sanh et al. (2019)}}

\textbf{Motivation:}

\begin{figure}
		\centering
		\includegraphics[width = 11cm]{`r ap("distilbert-motivation.png")`}\\ 
		{\footnotesize Source: \href{https://arxiv.org/pdf/1910.01108.pdf}{Sanh et al. (2019)}}
\end{figure}




## DistilBERT \href{https://arxiv.org/pdf/1910.01108.pdf}{\beamergotobutton{Sanh et al. (2019)}}

\textbf{Student architecture (\textit{DistilBERT}):}

- Half the number of layers compared to BERT
- Half of the size of BERT, but retains 97\% of the performance
- Initialize from BERT (taking one out of two hidden layers)
- Same pre-training data as BERT (Wiki + BooksCorpus)

\vspace{.3cm}

\textbf{Training and performance}

\begin{itemize}
	\item Distillation loss $L_{ce} = \sum_i t_i \cdot \log(s_i)$ + MLM-Loss $L_{mlm}$ + \\
				Cosine-Embedding-Loss $L_{cos}$
	\item Drops NSP, use dynamic masking, train with large batches
\end{itemize}



## DistilBERT \href{https://arxiv.org/pdf/1910.01108.pdf}{\beamergotobutton{Sanh et al. (2019)}}

\small
\textbf{Performance differences to BERT:}

\begin{figure}
		\centering
		\includegraphics[width = 9cm]{`r ap("distilbert-vs-sota.png")`}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1910.01108.pdf}{\footnotesize Sanh et al. (2019)}
\end{figure}

\textbf{Ablation study regarding the loss:}

\begin{figure}
		\centering
		\includegraphics[width = 9cm]{`r ap("distilbert-ablation.png")`}\\ 
		\footnotesize{Source:} \href{https://arxiv.org/pdf/1910.01108.pdf}{\footnotesize Sanh et al. (2019)}
\end{figure}