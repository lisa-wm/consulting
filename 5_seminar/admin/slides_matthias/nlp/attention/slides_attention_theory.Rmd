## Before Attention
```{r, include=FALSE, cache=FALSE}
ap = adjust_path(paste0(getwd(), "/figure"))
```

__Where we are right now:__

- Special network for the sequential structure of text (RNNs)
- Learn long-range dependencies of arbitrary length (in theory)
- Information bottleneck  
  $\rightarrow$ Source sentence compressed in a single hidden state vector
- Words are processed in a sequential fashion  
  $\rightarrow$ Difficult to parallelize


## Long-range dependencies

\begin{figure}
\centering
\includegraphics[width = 10cm]{`r ap("bahdanau14.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1409.0473.pdf}{\footnotesize Bahdanau et al. (2014)}
\end{figure}

\scriptsize
- RNNenc (classical encoder-decoder); RNNsearch (with Attention)
- BLEU score: Measure for translation quality (higher is better)

## Bahdanau et al. (2014)

\begin{figure}
\centering
\includegraphics[width = 7.5cm]{`r ap("bahdanau-attention.png")`}
\end{figure}

Use weighted combinations of _all_ the (concatenated) hidden states.

## Attention

__How to determine these weights?__

- _Encoder:_ 
    - Takes sequence $[x_1, .., x_{T_{in}}]$ as input
    - Outputs encoded sequence $[h_1, .., h_{T_{in}}]$

- _Decoder:_ 
    - Outputs a sequence $[o_1, .., o_{T_{out}}]$ ..
    - .. based on its hidden states $s_i$ 

\vspace{.3cm}

__How do we compute the hidden states $s_i$?__


## Attention 

1. __The alignment model__
    - How well do inputs around $x_j$ and output at position $i$ match?
    - $e_{ij} = a(s_{i-1}, h_j)$ with $a(.)$ as alignment model for calculating similarity scores $e_{ij}$

\vspace{.15cm}

2. __Normalize the weights__
    - Compute the softmax-normalized weights from the scores $e_{ij}$
    $$\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_{in}} \exp(e_{ik})}$$

3. __Calculate the context vector__
    - Context $c_i$ as weighted sum of the encoder hidden stats $h_j$
    $$c_i = \sum_{j=1}^{T_{in}} \alpha_{ij} \cdot h_j$$

## Attention 

4. __Compute the decoder hidden state__
    - Take context, previous encoder hidden state & previous output into account
    $$s_{i} = f(s_{i-1}, o_{i-1}, c_i)$$  
      with $f$ as non-linear function (e.g. a LSTM)

5. __Predict the next output word__
    $$p(o_i|o_1, .., o_{i-1}, x_1, .., x_{T{in}}) = g(o_{i-1}, s_i, c_i)$$
      with $g$ as non-linear function (e.g. the Softmax)
    

## Attention for Machine Translation

__Bahdanau et al. (2014) used this mechanism for Neural MT:__

\begin{figure}
\centering
\includegraphics[width = 7cm]{`r ap("bahdanau14attention_weights.png")`}\\ 
\footnotesize{Normalized Attention weights (Source:} \href{https://arxiv.org/pdf/1409.0473.pdf}{\footnotesize Bahdanau et al. (2014)}\footnotesize{)}
\end{figure}


## Attention Subtleties

__Still a lot of options/choices:__

- Other RNNs (like e.g. GRUs) could also be used with Attention
- Calculation of the alignment scores can be modified  
  $$a(s_{i-1}, h_j) = 
    \begin{cases} 
        s_{i-1}^\intercal h_j \qquad\qquad\qquad\qquad\quad\; dot\\ 
        s_{i-1}^\intercal W_a h_j \qquad\qquad\qquad\; general\\ 
        v_a^\intercal \tanh(W_a[s_{i-1}^\intercal h_j]) \qquad concat\\
    \end{cases}$$

    \footnotesize \href{https://arxiv.org/pdf/1508.04025.pdf}{Luong et al. (2015)}, notation adapteted to  \href{https://arxiv.org/pdf/1409.0473.pdf}{Bahdanau et al. (2014)}
\normalsize
- Global vs. Local Attention mechanisms  
  (see also \href{https://arxiv.org/pdf/1508.04025.pdf}{Luong et al. (2015)})


## Self-Attention

__We can view regular Attention in the following way:__

- A _query_ is compared to a set of _keys_ regarding their similarity  
  $\rightarrow$ $s_{i-1}$ is the _query_ for which the alignment model calculates the similarity to the $h_j$s (_keys_)
- The resulting (normalized) similarity-score is used to calculate a weighted combination of some _values_  
  $\rightarrow$ The _values_ $h_j$ are weighted by $\alpha_{ij}$ and summed up
  
\vspace{.3cm}
  
__Generalizations of this mechanism:__

- Keys & Values __do not__ necessarily have to be the same vector  
  $\rightarrow$ Arbitrary (contextually meaningful) vectors are possible
- Querys & Keys __can__ refer to the same underlying sequence  
  $\rightarrow$ That's what we will call __Self-Attention__!
  

## The Transformer

\begin{figure}
\centering
\includegraphics[width = 10cm]{`r ap("allyouneed.png")`}
\end{figure}

__The basic ingredients:__

- Model architecture introduced by \href{https://arxiv.org/pdf/1706.03762.pdf}{Vaswani et al. (2017)}
- Encoder-Decoder framework relying completely on Self-Attention  $\rightarrow$ __No__ recurrence at any place in the network
- Requires large matrix multiplications, __but:__ parallelizable
- Initial use case: Machine Translation

\vspace{.3cm}

__Further use:__

_Kick-starts a new era of unsupervised representation learning._


## Transformer -- The Encoder

__How it works:__

- Each token $x_t$ of the input sequence is mapped to a $d$-dimensional embedding
- Each embedding is (linearly) projected to a query-, key- & value-vector of dimensions $d_q$, $d_k$, $d_v$ (where $d_q = d_k$)
- This results in three matrices:  
  $\rightarrow$ __Q__ $\in\;\mathbb{R}^{T\times d_q}$; \quad __K__ $\in\;\mathbb{R}^{T\times d_k}$; \quad __V__ $\in\;\mathbb{R}^{T\times d_v}$
- For every token compute the similarity to all other tokens
- Use those (normalized) similarity scores to compute weighted combination of the values  

$\rightarrow$ __Obtain "new" (context sensitive) embeddings of size $d_v$__


## Transformer -- The Encoder 

1. Alignment model:  
  __Scaled dot-product attention__
    - For one query & one key: $e_{ij} = a(q,k) = \frac{q^{\intercal}k}{\sqrt{d_k}}$ 
    - Parallelized: $E = a(Q,K) = \frac{QK^{\intercal}}{\sqrt{d_k}}$ 

\vspace{.15cm}

2. __Normalize the weights:__ $softmax \left(\frac{QK^{\intercal}}{\sqrt{d_k}}\right)$

\vspace{.15cm}

3. __Self-Attention__  
    $$\mbox{Self-Attention}(Q,K,V) = softmax \left(\frac{QK^{\intercal}}{\sqrt{d_k}}\right)V$$
    

## Transformer -- The Encoder (cont'd)

4. __Multiple Heads__  
    - Multiple projections of embeddings to query, key & value
    - Perform scaled dot-product attention __in parallel__
    - Concatenate resulting embeddings from different heads
    
\vspace{.15cm}

5. __Further Subtleties:__
    - Input sequence of fixed length required $\rightarrow$ Padding
    - Positional encodings added to initial embeddings
    - Residual connections & layer normalization
    - Additional Feed-Forward layer at the end of the encoder
    - Six identical encoder layers stacked on top of each other
    
## Transformer -- The Encoder (completed)

\begin{figure}
\centering
\includegraphics[width = 4cm]{`r ap("trafo-enc.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1706.03762.pdf}{\footnotesize Vaswani et al. (2017)}
\end{figure}


## Self-Attention -- Example

- Sequence: \vspace{-.25cm}$$[how,\; are,\; you]$$
- Queries: \vspace{-.25cm}$$q_1 = (1,4,3,-1);\quad q_2 = (6,1,5,9);\quad q_3 = (2,-3,4,0)$$
- Keys: \vspace{-.25cm}$$k_1 = (1,2,3,4);\quad k_2 = (3,0,1,-2);\quad k_3 = (5,0,-1,-4)$$
- Values: \vspace{-.25cm}$$v_1 = (-1,3,2);\quad v_2 = (5,0,7);\quad v_3 = (4,1,3)$$
$\rightarrow$ _Perform Self-Attention for the first token (by hand)!_

Nice illustration: \href{http://jalammar.github.io/illustrated-transformer/}{http://jalammar.github.io/illustrated-transformer/}


## Self-Attention -- Example

__Self-Attention displayed:__

\begin{figure}
\centering
\includegraphics[width = 10cm]{`r ap("attention.png")`}\\ 
\footnotesize{Source:} \href{http://jalammar.github.io/illustrated-transformer/}{\footnotesize jay alammar's blog}
\end{figure}


## Transformer -- The Decoder

__Self-Attention in the decoder:__

- _Causality_ is a problem!
- A RNN decoder at time point $i$ just has access to
    - the complete encoded input sequence
    - the output of the decoder until time point $i-1$  
  $\rightarrow$ Causal model structure
- Pure Self-Attention has access to the complete sequence

__Masked Self-Attention__

- Manipulate the calculated alignment scores:
    - Introduce "causal" pattern to the score-matrix $E$
    - Mask every entry above the diagonal by $-\infty$
    - Prohibits the model to "look to the future"
    

## Transformer -- The Decoder

__Modular structure:__

1. Embeddings of the output sequence + positional encoding  
  (+ outputs shifted to the right, so the decoder is only able to attend to the previous positions)
2. Masked (Multi-Head) Self-Attenion
3. Multi-Head Attention over encoder outputs
4. Residual connections, layer normalization,  
   Feed-Forward layer on top
5. Six replicates of this decoder block

## Transformer -- Encoder & Decoder

\begin{figure}
\centering
\includegraphics[width = 6cm]{`r ap("trafo-enc-dec.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1706.03762.pdf}{\footnotesize Vaswani et al. (2017)}
\end{figure}


## Transformer -- Final Layer

- Six encoder blocks followed by six decoder blocks
- Followed by a linear pooling layer
- And a final softmax layer to output a probability distribution over the vocabulary.
  
\begin{figure}
\centering
\includegraphics[width = 6.5cm]{`r ap("trafo-final.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1706.03762.pdf}{\footnotesize Vaswani et al. (2017)}
\end{figure}


## Computational complexity

- Recurrent layers were difficult to parallelize
- CNN/Self-Attention layers easily parallelizable
  
\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("trafo-complexity.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1706.03762.pdf}{\footnotesize Vaswani et al. (2017)}
\end{figure}