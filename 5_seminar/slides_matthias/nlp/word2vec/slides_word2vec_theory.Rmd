## Recap
```{r, include=FALSE, cache=FALSE}
ap = adjust_path(paste0(getwd(), "/figure"))
```

__Where we are right now:__

- Words are discrete units composed of characters
- Can be represented as (high-dimensional) one hot vectors
- Difficult to capture similarity between synonyms
- Documents can be represented as a vector of word occurrences
- These BoW-representations are high-dimensional and inefficient

## Notation

__Introducing the notation for today's chapters:__

- $V$: All words in the vocabulary
- $|V|$: Size of the vocabulary
- $w_i$: The i-th word in the vocabulary
- $w_t$: The t-th word in a sequence
- $c$: Size of the context
- $v$: Input representation of a word
- $v'$: Output representation of a word

## The idea of Word embeddings

__to embed__

_"The verb embed means to implant something or someone â€” 
like to embed a stone into a garden pathway or to embed a journalist in a military unit"._  

or

_"to embed a word into the continuous space of real numbers"_

__A note on the terminology:__

_Word embeddings_ are also referred to as _word vectors_, _word representations_ or _distributed representations_.

## The problem with One-hot encodings

__We are already able to do this, via one-hot encoding:__

\begin{align*}
football = [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]\\basketball = [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0]
\end{align*}

__Two major problems:__

- The dimensionality of these vectors?
- $similarity(football, basketball) =\; ?$  
  The vectors are orthogonal to each other, so $sim(w_i, w_j) = 0\; \forall\; i,j$
  
## Word embeddings -- Dense representations

__Problem of dimensionality:__

- Clearly, dimensionality $|V|$ ist way to high
- We want a densely filled vector of (relatively) low dimension
- Could look like this:
\begin{align*}
    football &= \begin{bmatrix}
            0.359 \\
           -0.174 \\
            0.701 \\
           \vdots \\
            0.445 \\
           -0.123 \\
            0,509 
         \end{bmatrix}
\end{align*}

## Word embeddings -- The distributional hypothesis

__Zellig S. Harris (1954):__

_\href{https://www.tandfonline.com/doi/pdf/10.1080/00437956.1954.11659520}{Distributional Structure}_

__J.R. Firth (1957):__

_"You shall know a word by the company it keeps."_

__Learn something about the meaning of _football_ by studying which context it appears in:__

\vspace{.5cm}

\centering
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{rcl}
.. the score of the & $football$ & game was 3:0 ..\\
.. he shot the & $football$ & directly at the goalkeeper ..\\
.. last night, I was watching & $football$ & on tv ..
\end{tabular}

## Word embeddings -- Desired properties

__Visualizing embeddings in 3D:__

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("linear-relationships.png")`}\\ 
\footnotesize{Source:} \href{https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space}{\footnotesize \it google}
\end{figure}

## Word embeddings -- Another visualization

__Visualizing embeddings in 3D:__

\begin{figure}
\centering
\includegraphics[width = 10cm]{`r ap("embeddings3d.png")`}\\ 
\footnotesize{Source:} \href{http://projector.tensorflow.org/}{\footnotesize \it http://projector.tensorflow.org/}
\end{figure}

## Word embeddings

__Where we have seen it before:__

\begin{figure}
\centering
\includegraphics[width = 8.5cm]{`r ap("bengio03_cut.png")`}\\ 
\footnotesize{Source:} \href{http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf}{\footnotesize Bengio et al. (2003)}
\end{figure}

## Word2Vec \href{https://arxiv.org/pdf/1301.3781.pdf}{\beamergotobutton{Mikolov et al. (2013a)}}

__Key contributions of this model:__

- Pick up the idea of Bengio et al. (2003)
- Drop expensive non-linearities
- Reduce computational cost of training
- _Keep the idea of the "look-up layer"_
- _Expand context for learning good representations_

__Proposal of two different architectures:__

- Skip-gram (SG) model
- Continuous Bag-of-words (CBOW) model

## Skip-gram model

__The "Fake Task":__

- _Training objective:_ Given a word, predict the neighbouring words
- _Generation of samples:_ Sliding fixed-size window over the text

\begin{tabular}{|c|c|c|cccccc|}
\hline
\cellcolor{blue!15}The & \cellcolor{blue!65}quick & \cellcolor{blue!65}brown & fox & jumps & over & the & lazy & dog \\
\hline
\end{tabular}
$\Rightarrow$ \qquad (the, quick); (the, brown)
\begin{tabular}{|c|c|c|c|ccccc|}
\hline
\cellcolor{blue!65}The & \cellcolor{blue!15}quick & \cellcolor{blue!65}brown & \cellcolor{blue!65}fox & jumps & over & the & lazy & dog \\
\hline
\end{tabular}
$\Rightarrow$ \qquad (quick, the); (quick, brown); (quick, fox)
\begin{tabular}{|c|c|c|c|c|cccc|}
\hline
\cellcolor{blue!65}The & \cellcolor{blue!65}quick & \cellcolor{blue!15}brown & \cellcolor{blue!65}fox & \cellcolor{blue!65}jumps & over & the & lazy & dog \\
\hline
\end{tabular}
$\Rightarrow$ \qquad (brown, the); (brown, quick), (brown, fox), (brown, jumps)

## Skip-gram model

__The architecture:__
\vspace{-1cm}
\begin{eqnarray*}
\underbrace{
\begin{bmatrix}
0 & 1 & 0 & .. & 0 \\
\end{bmatrix}
}_{\substack{\text{One-hot vector}\\\text{for the word}}} \pause \qquad \times \qquad
\underbrace{
\begin{bmatrix}
v_{1,1} & v_{1,2} & .. & v_{1,dim}\\
v_{2,1} & v_{2,2} & .. & v_{2,dim} \\
v_{3,1} & v_{3,2} & .. & v_{3,dim} \\
\vdots & \vdots & .. & \vdots \\
v_{|V|,1} & v_{|V|,2} & .. & v_{|V|,dim}
\end{bmatrix}
}_{\substack{\text{Projection layer weights}\\\text{for the words ($\hat{=}$ word vectors)}}} \\ \pause \times \qquad
\underbrace{
\begin{bmatrix}
v'_{1,1} & v'_{1,2} & v'_{1,3} & .. & v'_{1,|V|}\\
v'_{2,1} & v'_{2,2} & v'_{2,3} & .. & v'_{2,|V|} \\
v'_{3,1} & v'_{3,2} & v'_{3,3} & .. & v'_{3,|V|} \\
\vdots & \vdots & .. & \vdots \\
v'_{dim,1} & v'_{dim,2} & v'_{dim,3} & .. & v'_{dim,|V|}
\end{bmatrix}
}_{\substack{\text{Output layer weights}\\\text{for the words}}} \pause \qquad \Rightarrow \qquad
\underbrace{
\dfrac{e^x}{\sum e^x}
}_{\substack{\text{Softmax}\\\text{classifier}}}
\end{eqnarray*}

## Skip-gram model

__The architecture:__
\begin{eqnarray*}
\begin{bmatrix}
0 & 1 & 0 & .. & 0 \\
\end{bmatrix} \qquad \times \qquad
\begin{bmatrix}
\mathbf{v_{aal}}\\
\mathbf{v_{afrika}} \\
\mathbf{v_{astronaut}} \\
\vdots\\
\mathbf{v_{zebra}}
\end{bmatrix} \qquad \times \qquad
\begin{bmatrix}
\mathbf{v'_{aal}}\\
\mathbf{v'_{afrika}} \\
\mathbf{v'_{astronaut}} \\
\vdots\\
\mathbf{v'_{zebra}}
\end{bmatrix}^\intercal \\ 
\Rightarrow \qquad
\underbrace{
  \dfrac{e^x}{\sum e^x}
}_{\substack{\text{Softmax}\\\text{classifier}}} \hfill
\end{eqnarray*}

## Skip-gram model -- Broken down

__Input-Layer:__
\vspace{-.2cm}
\begin{align*}
& \underbrace{
\begin{bmatrix}
0 & \cellcolor{cyan}{1} & 0 & .. & 0 \\
\end{bmatrix}
}_{\substack{\text{One-hot vector}\\\text{for the word}}} \qquad \times \qquad
\underbrace{
\begin{bmatrix}
v_{1,1} & v_{1,2} & .. & v_{1,dim}\\
v_{2,1} & v_{2,2} & .. & v_{2,dim} \\
v_{3,1} & v_{3,2} & .. & v_{3,dim} \\
\vdots & \vdots & .. & \vdots \\
v_{|V|,1} & v_{|V|,2} & .. & v_{|V|,dim}
\end{bmatrix}
}_{\substack{\text{Projection layer weights}\\\text{for the words ($\hat{=}$ word vectors)}}} & \\ = & \qquad
\begin{bmatrix}
0 & 0 & .. & 0\\
\rowcolor{cyan}
v_{2,1} & v_{2,2} & .. & v_{2,dim} \\
0 & 0 & .. & 0\\
\vdots & \vdots & .. & \vdots \\
0 & 0 & .. & 0\\
\end{bmatrix} \qquad = \qquad
\begin{bmatrix}
\rowcolor{cyan}
v_{2,1} & v_{2,2} & .. & v_{2,dim} 
\end{bmatrix} &
\end{align*}

## Skip-gram model -- Broken down

__Hidden- & Ouput-Layer:__

\begin{align*}
 & \begin{bmatrix}
\rowcolor{cyan}
v_{2,1} & v_{2,2} & .. & v_{2,dim} 
\end{bmatrix} \qquad \times \qquad
\begin{bmatrix}
v'_{1,1} & v'_{1,2} & \cellcolor{red!80}{v'_{1,3}} & .. & v'_{1,|V|}\\
v'_{2,1} & v'_{2,2} & \cellcolor{red!80}{v'_{2,3}} & .. & v'_{2,|V|} \\
v'_{3,1} & v'_{3,2} & \cellcolor{red!80}{v'_{3,3}} & .. & v'_{3,|V|} \\
\vdots & \vdots & \cellcolor{red!80}{\vdots} & .. & \vdots \\
v'_{dim,1} & v'_{dim,2} & \cellcolor{red!80}{v'_{dim,3}} & .. & v'_{dim,|V|}
\end{bmatrix} & \\ & \text{results in:} & \\ && \\
& \begin{bmatrix}
\mathbf{v_{2}}^\intercal\mathbf{v'_{1}} & \mathbf{v_{2}}^\intercal\mathbf{v'_{2}} & \cellcolor{blue!20}{\mathbf{v_{2}}^\intercal\mathbf{v'_{3}}} & .. & \mathbf{v_{2}}^\intercal\mathbf{v'_{|V|}} 
\end{bmatrix} \qquad \Rightarrow \qquad
\underbrace{
\dfrac{e^x}{\sum e^x}
}_{\substack{\text{Softmax}\\\text{classifier}}} &
\end{align*}

## CBOW model

__The "Fake Task":__

- _Training objective:_ Given a context, predict the center word
- _Generation of samples:_ Sliding fixed-size window over the text

\begin{tabular}{|c|c|c|c|c|cccc|}
\hline
\cellcolor{blue!15}The & \cellcolor{blue!15}quick & \cellcolor{blue!65}brown & \cellcolor{blue!15}fox & \cellcolor{blue!15}jumps & over & the & lazy & dog \\
\hline
\end{tabular}
$\Rightarrow$ \qquad ([the, quick, fox, jumps], brown)
\begin{tabular}{|c|c|c|c|c|c|ccc|}
\hline
The & \cellcolor{blue!15}quick & \cellcolor{blue!15}brown & \cellcolor{blue!65}fox & \cellcolor{blue!15}jumps & \cellcolor{blue!15}over & the & lazy & dog \\
\hline
\end{tabular}
$\Rightarrow$ \qquad ([quick, brown, jumps, over], fox)
\begin{tabular}{|cc|c|c|c|c|c|cc|}
\hline
The & quick & \cellcolor{blue!15}brown & \cellcolor{blue!15}fox & \cellcolor{blue!65}jumps & \cellcolor{blue!15}over & \cellcolor{blue!15}the & lazy & dog \\
\hline
\end{tabular}
$\Rightarrow$ \qquad ([brown, fox, over, the], jumps)

## CBOW model -- Broken down

__Input-Layer:__

\begin{align*}
& avg \left(
\underbrace{
\begin{bmatrix}
0 & \cellcolor{cyan}{1} & \cellcolor{cyan}{1} & .. & \cellcolor{cyan}{1} & 0 \\
\end{bmatrix}
}_{\substack{\text{Indicator vector}\\\text{for the surrounding words}}} \qquad \times \qquad
\underbrace{
\begin{bmatrix}
v_{1,1} & v_{1,2} & .. & v_{1,dim}\\
v_{2,1} & v_{2,2} & .. & v_{2,dim} \\
v_{3,1} & v_{3,2} & .. & v_{3,dim} \\
\vdots & \vdots & .. & \vdots \\
v_{|V|,1} & v_{|V|,2} & .. & v_{|V|,dim}
\end{bmatrix}
}_{\substack{\text{Projection layer weights}\\\text{for the words ($\hat{=}$ word vectors)}}}
\right) & \\ & \text{taking the $elementwise$ average results in:} & \\
& \qquad\qquad 
\begin{bmatrix}
\rowcolor{cyan}
v_{avg,1} & v_{avg,2} & .. & v_{avg, dim}
\end{bmatrix} = \mathbf{v_{avg}}&
\end{align*}

## CBOW model -- Broken down

__Hidden- & Ouput-Layer:__

\begin{align*}
& \begin{bmatrix}
\rowcolor{cyan}
v_{avg,1} & v_{avg,2} & .. & v_{avg, dim}
\end{bmatrix} \times 
\begin{bmatrix}
v'_{1,1} & v'_{1,2} & \cellcolor{red!80}{v'_{1,3}} & .. & v'_{1,|V|}\\
v'_{2,1} & v'_{2,2} & \cellcolor{red!80}{v'_{2,3}} & .. & v'_{2,|V|} \\
v'_{3,1} & v'_{3,2} & \cellcolor{red!80}{v'_{3,3}} & .. & v'_{3,|V|} \\
\vdots & \vdots & \cellcolor{red!80}{\vdots} & .. & \vdots \\
v'_{dim,1} & v'_{dim,2} & \cellcolor{red!80}{v'_{dim,3}} & .. & v'_{dim,|V|}
\end{bmatrix} & \\ & \text{results in:} & \\ && \\
& \begin{bmatrix}
\mathbf{v_{avg}}^\intercal\mathbf{v'_{1}} & \mathbf{v_{avg}}^\intercal\mathbf{v'_{2}} & \cellcolor{blue!20}{\mathbf{v_{avg}}^\intercal\mathbf{v'_{3}}} & .. & \mathbf{v_{avg}}^\intercal\mathbf{v'_{3}} 
\end{bmatrix} \qquad \Rightarrow \qquad
\underbrace{
\dfrac{e^x}{\sum e^x}
}_{\substack{\text{Softmax}\\\text{classifier}}} &
\end{align*}

## Word2Vec training

- Trainable via Gradient Descent and Backpropagation
- Still, the computational cost of the softmax probabilities is way too high
- __Solution:__ 
    + _Negative Sampling_
    + _Subsampling of frequent words_

## Negative Sampling \href{https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}{\beamergotobutton{Mikolov et al. (2013b)}}

__Ordinary training objective (for SG):__  

For the sequence $w_1, w_2,\, \dots, w_T$, maximize the average log probability:
  \begin{align*}
\text{argmax}\; \frac{1}{T} \sum_{t = 1}^T \sum_{-c\leq j\leq c, j\neq 0} \log p(w_{t+j}|w_t)
  \end{align*}
  
__Softmax for calculating the probablities:__

  \begin{align*}
p(w_{out}|w_{in}) = \frac{exp(\mathbf{v'_{out}}^\intercal\mathbf{v_{in}})}{\sum_{w = 1}^{|V|} \exp(\mathbf{v'_{w}}^\intercal\mathbf{v_{in}})}
  \end{align*}
  
## Negative Sampling \href{https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}{\beamergotobutton{Mikolov et al. (2013b)}}

__Workaround:__  

- Sample "negative" examples from the corpus in order to avoid the expensive denominator
  
__Negative sampling objective:__

Define
\vspace{-.5cm}
  \begin{align*}
\log(\sigma(\mathbf{v'_{out}}^\intercal\mathbf{v_{in}})) + \sum_{i = 1}^{k} \mathds{E}_{w_i \sim P_n(w)} \left[\log(\sigma(\mathbf{-v'_{w_i}}^\intercal\mathbf{v_{in}}))\right]
  \end{align*}
  
in order to replace $p(w_{out}|w_{in})$ in the objective function.  

(Where $k$ is number of negative samples and $P_n(w)$ is the unigram distribution raised to the power of $3/4$)
  
## Subsampling of frequent words \href{https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}{\beamergotobutton{Mikolov et al. (2013b)}}

__Problem:__

- Most frequent words dominate the corpus with their occurences (_Zipf's law_)
- Model does not benefit much from seeing these words over and over again

__Subsampling approach:__

- Discard words during training with a probability inverse proportional to their frequency of occurence

  \begin{align*}
P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}
  \end{align*}

## A note one the literature

__Four papers out there:__

- About the model architectures:  
  \href{https://arxiv.org/pdf/1301.3781.pdf}{\beamergotobutton{Mikolov et al. (2013a)}}
- Computational subtleties:  
  \href{https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}{\beamergotobutton{Mikolov et al. (2013b)}}
- Linguistic Regularities:  
  \href{https://www.aclweb.org/anthology/N13-1090.pdf}{\beamergotobutton{Mikolov et al. (2013c)}}
- Use for Machine Translation:  
  \href{https://arxiv.org/pdf/1309.4168.pdf}{\beamergotobutton{Mikolov et al. (2013d)}}