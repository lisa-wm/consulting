## Word2Vec in Python
```{r, include=FALSE, cache=FALSE}
ap = adjust_path(paste0(getwd(), "/figure"))
```

__Train your own embeddings using the gensim module:__

- _Step 1:_ Set up the model parameters  
  (Model is still uninitialized)

- _Step 2:_ Feed your data (list of sentences) to the model  
  (Model is now initialized)
  
- _Step 3:_ Train the model

## Word2Vec in Python

__Set up the model parameters:__

```{python, include = T, eval = F}
from gensim.models import Word2Vec

w2v_model = Word2Vec(sg = 0,
                     cbow_mean = 1,
                     size = 100,
                     alpha = 0.025, 
                     min_alpha = 0.0001, 
                     window = 5,
                     min_count = 5,
                     sample = 0.001, 
                     negative = 5,
                     workers = 3,
                     ...)
```

- _\href{https://radimrehurek.com/gensim/models/word2vec.html}{Online documentation of Word2Vec}_

## Word2Vec in Python

__Structure of the corpus:__ 

```{python, include = T, eval = T}
from nltk import word_tokenize

my_sentences = ["we are eager to learn about nlp",
                "neural networks are fun",
                "python is my favourite language"]
                
my_corpus = [word_tokenize(doc) for doc in my_sentences]

my_corpus[0]
```

## Word2Vec in Python

__Initialize the model with data:__

```{python, include = T, eval = F}
w2v_model.build_vocab(sentences = my_corpus,
                      update = False,
                      progress_per = 10000)
```
  
__Train your embeddings:__

```{python, include = T, eval = F}
w2v_model.train(sentences = my_corpus,
                total_examples = w2v_model.corpus_count,
                epochs = 20)
```

## Word2Vec in Python

__Query the trained model for an embedding:__

```{python, include = T, eval = F}
w2v_model.wv["nlp"]

# array([-2.9408352 , -2.9448547, -0.0644585 , 2.5617635 , 0.11626738,
#        0.9916419 , -0.982047, -0.36746535,  2.282364  ,  4.807505  ,
#       -0.57372135, -0.1776367,  3.6928544, -0.07633026, -0.3486168 ,
#       -1.1801438 , -2.0244286,  3.9215038 ,  5.7360845 , 1.1472751 ,
#        ....
#        1.7839055 , -0.94101334,  5.75815  ,  3.9923134, -2.2030773 ,
#        1.2307607 , 0.41266924,  0.5325593 ,  0.6659662 , 1.2994248 ,
#        0.14457057, 1.6736019, -2.932189 , -1.2620611 , -0.7192462 ],
#      dtype=float32)
```

## Word2Vec in Python

__Using pre-trained word embeddings:__

- Download e.g. _\href{https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit}{GoogleNews-vectors}_

```{python, include = T, eval = F}
from gensim.models import KeyedVectors

w2v_model = KeyedVectors.load_word2vec_format("path/to/vectors", 
                                              binary = True)
```

- Or save your trained model and re-use it later:

```{python, include = T, eval = F}
trained_model.wv.save_word2vec_format("path/to/model.bin", 
                                      binary = True)

w2v = gensim.models.Word2Vec.load_word2vec_format("path/to/model.bin", 
                                                  binary = True)
```


## Measuring similarity

__Euclidean dot product formula:__
\vspace{-.35cm}
  \begin{align*}
\mathbf{A} \cdot \mathbf{B} = ||\mathbf{A}|| \cdot ||\mathbf{B}|| \cdot \cos(\theta)
  \end{align*}

__Cosine similarity:__
\vspace{-.35cm}
  \begin{align*}
similarity = \cos(\theta) &= \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \cdot ||\mathbf{B}||} \\
                          &= \frac{\sum_{i=1}^{n} A_iB_i}{\sqrt{\sum_{i=1}^{n} A_i^2}\sqrt{\sum_{i=1}^{n} B_i^2}}
  \end{align*}
  
__In Python:__

```{python, include = T, eval = F}
w2v_model.wv.similarity("France", "Paris")
w2v_model.wv.most_similar(positive = ["beautiful"])
```
  
## Additive Compositionality

__Most famous example:__

- $v_{king} - v_{man} + v_{woman} \approx v_{queen}$

__Other examples:__

- _Present/past tense:_  
  $v_{went} - v_{go} + v_{stand} \approx v_{stood}$
  
- _Country/Capital:_  
  $v_{Germany} - v_{Berlin} + v_{Paris} \approx v_{France}$
  
- _Something funny at the end:_
  $v_{Dortmund} - v_{losers} + v_{winners} \approx v_{MÃ¼nchen}$
  
__In Python:__

```{python, include = T, eval = F}
w2v_model.wv.most_similar(positive=["king","woman"],negative=["man"])
```

## Word2Vec and Bigrams

__Additive Compositionality__ sometimes doesn't work so well:

- E.g. $v_{new} + v_{york} \stackrel{?}{\approx} v_{new\_york}$
- Or: $v_{larry} + v_{page} \stackrel{?}{\approx} v_{larry\_page}$

__Learning phrases with ``gensim``:__

```{python, include = T, eval = F}
from gensim.models.phrases import Phrases, Phraser

phrases = Phrases(my_corpus, min_count = 5, threshold = 10.0)
bigrams = Phraser(phrases)

bigrams[my_corpus]
```

## Word2Vec and Bigrams

__Pointwise mutual information:__

$$pmi(x;y) = \log\left( \frac{p(x,y)}{p(x) \cdot p(y)} \right)$$

__Use of PMI:__

- Measure of association in information theory and statistics
- Higher values indicate a higher association between words
- (Co-)occurrence counts are used to approximate the probabilities
- Normalization (NPMI) to obtain a fixed range of values between -1 \& +1.