## What is happening at the moment?
```{r, include=FALSE, cache=FALSE}
ap = adjust_path(paste0(getwd(), "/figure"))
```

__The world's largest Tech companies are investing heavily:__

- fb ai research, google ai, microsoft research have own NLP groups
- Leading researchers like Geoffrey Hinton (Google) or Yann LeCun (Facebook) start working for the industry
- In Munich/Germany: Projects like the _\href{https://mcml.ai/}{MCML-Cluster}_ (Munich Center for Machine Learning)

## What is happening at the moment?

\begin{figure}
\centering
\includegraphics[width = 10cm]{`r ap("gpt2.png")`}\\ 
\footnotesize{Source:} \href{https://syncedreview.com}{\footnotesize Synced}
\end{figure}

## What is happening at the moment?

__Reasons why this works:__

- Computational power (GPUs, TPUs, ..)
- Lexical resources (from the internet)
- \textcolor{orange}{\bf Better parallelizable architectures}

## The workhorse -- Deep neural networks

__Once upon a time:__ _Recurrent neural networks_

\begin{figure}
\centering
\includegraphics[width = 10cm]{`r ap("lstm.png")`}\\ 
\footnotesize{Source:} \href{https://colah.github.io/posts/2015-08-Understanding-LSTMs/}{\footnotesize colah's blog}
\end{figure}

## The workhorse -- Deep neural networks

__The new kids in town:__ _Transformers_

\begin{figure}
\centering
\includegraphics[width = 10cm]{`r ap("attention.png")`}\\ 
\footnotesize{Source:} \href{http://jalammar.github.io/illustrated-transformer/}{\footnotesize jay alammar's blog}
\end{figure}

## End-to-end trainable models

__Naive approach:__

- Learn embeddings with a prefered algorithm
- Embeddings can be used for further tasks
- Plug them into a subsequent model for the actual task

\vspace{0.5cm}

__Better: Train models end-to-end__

- _Input:_ Sequences of words
- _Hidden:_ Deep neural networks
- _Output:_ E.g. a classification (positive/negative)

## Shortcomings of the naive approach

__1st Generation of neural embeddings are _"context-free"___

- Breakthrough paper by Mikolov et al, 2013 (Word2Vec)
- Followed by Pennington et al, 2014 (GloVe)
- Extension of Word2Vec by Bojanowski et al, 2016 (FastText)

__Why "Context-free"?__

- Models learn _one single_ embedding for each word
- Why could this possibly be problematic?
    + "The _default_ setting of the function is xyz."
    + "The probability of _default_ is rather high."

- Would be nice to have different embeddings for these two occurrences

## Contextual Embeddings

__"Contextual" embeddings?__

- Model makes further use of the context a word appears in
- Embeddings depend on the context around a word
- Requires us to process sequences $\rightarrow$ RNNs/LSTMs
- Distinguish between:
    + Unidirectional
    + Bidirectional

## Generative Pre-Training (GPT) \href{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}{\beamergotobutton{Radford et al. (2018)}}

\begin{figure}
\centering
\includegraphics[width = 3.5cm]{`r ap("gpt.png")`}\\ 
\footnotesize{Source:} \href{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}{\footnotesize Radford et al. (2018)}
\end{figure}

## Embeddings from Language Models (ELMo) \href{https://arxiv.org/pdf/1802.05365.pdf}{\beamergotobutton{Peters et al. (2018)}}

\begin{figure}
\centering
\includegraphics[width = 3cm]{`r ap("elmo.jpg")`}
\end{figure}

__Why "Bidirectional"?__

- Makes use of both sides of the context
- Leads to a more complex model:
    + So-called _bidirectional language model_ (biLM)
    + Consists of a forward LM & a backward LM (both LSTMs)
- Word embeddings as _intermediate layer representations_

__But:__ ELMo is sometimes referred to as _shallowly bidirectional_

## Bidirectional Encoder Representations from Transformers (BERT) \href{https://arxiv.org/pdf/1810.04805.pdf}{\beamergotobutton{Devlin et al. (2018)}}

\begin{figure}
\centering
\includegraphics[width = 3cm]{`r ap("bert.jpeg")`}
\end{figure}

__Improvement by BERT: Deep Bidirectionality__

- Multi-layer bidirectional Transformer encoder
- Transformers are Encoder-Decoder architectures using "Self-Attention"
- BERT consists of multiple Transformers connected bidirectionally

## BERT \href{https://arxiv.org/pdf/1810.04805.pdf}{\beamergotobutton{Devlin et al. (2018)}}

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("google-bert.png")`}\\ 
\footnotesize{Source:} \href{https://syncedreview.com/2019/10/25/milestone-bert-boosts-google-search/}{\footnotesize Synced}
\end{figure}

## GPT vs. ELMo vs. BERT

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("comparison-bert.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1810.04805.pdf}{\footnotesize Devlin et al. (2018)}
\end{figure}

## GPT2 \href{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}{\beamergotobutton{Radford et al. (2019)}}

- Successor of the GPT model
- Small architectural changes, but mainly just __bigger__
- Wasn't published at first due to concerns about malicious abuse
- Meanwhile the full model is published and available online

## Transfer learning

__Transfer learning in Image Analysis:__

- Use a huge pre-trained deep neural network (e.g. on ImageNet)
- Stack individual task-specific layers on top of this model
- Fine-tune the resulting model on your own data

__Developments in NLP:__

- Trend towards deeper pre-training models for word embeddings  
- Gives rise to transfer learning  
- One speaks of "NLP's ImageNet moment" (http://ruder.io/nlp-imagenet/)

## Transfer learning 

\begin{figure}
\centering
\includegraphics[width = 11cm]{`r ap("bert.png")`}\\ 
\footnotesize{Source:} \href{https://arxiv.org/pdf/1810.04805.pdf}{\footnotesize Devlin et al. (2018)}
\end{figure}

## GPT2 \href{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}{\beamergotobutton{Radford et al. (2019)}}

__Want to try out _how_ well they work?__

\vspace{1cm}

\centering

__*\href{https://transformer.huggingface.co/doc/gpt2-large}{\textcolor{orange}{Writing with Transformer architectures}}*__

\vspace{2cm}

## Something funny at the end

\begin{figure}
\centering
\includegraphics[width = 8cm]{`r ap("w2v-funny.jpg")`}\\ 
\footnotesize{Word2Vec gone wrong..}
\end{figure}