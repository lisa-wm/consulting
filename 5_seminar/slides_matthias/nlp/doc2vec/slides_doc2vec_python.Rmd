## Doc2Vec in Python
```{r, include=FALSE, cache=FALSE}
ap = adjust_path(paste0(getwd(), "/figure"))
```

__Train your own embeddings using the gensim module:__

- _Step 1:_ Set up the model parameters  
  (Model is still uninitialized)

- _Step 2:_ Feed your data (list of sentences) to the model  
  (Model is now initialized)
  
- _Step 3:_ Train the model

## Doc2Vec in Python

__Set up the model parameters:__

```{python, include = T, eval = F}
from gensim.models.doc2vec import Doc2Vec

d2v_model = Doc2Vec(dm = 1,
                    dm_concat = 0,
                    dm_mean = None,
                    dm_tag_count = 1
                    dbow_words = 0,
                    vector_size = 100,
                    ...)
```
    
- _\href{https://radimrehurek.com/gensim/models/doc2vec.html}{Online documentation of Doc2Vec}_

## Doc2Vec in Python

__Structure of the corpus:__ 

```{python, include = T, eval = F}
from gensim.models.doc2vec import TaggedDocument

my_corpus = [["we", "are", "eager", "to", "learn", "about", "nlp"],
             ["neural", "networks", "are", "fun"],
             ["python", "is", "my", "favourite", "language"]]
                
tagged_corpus = [TaggedDocument(words = d, tags = ["d_" + str(i)]) 
                  for i, d in enumerate(my_corpus)]
                  
tagged_corpus[1]

## TaggedDocument(words=['neural','networks','are','fun'],tags=['d_1'])
```

## Doc2Vec in Python

__Initialize the model with data:__ 

```{python, include = T, eval = F}
d2v_model.build_vocab(documents = tagged_corpus,
                      update = False,
                      progress_per = 10000)
```
  
__Train your embeddings:__

```{python, include = T, eval = F}
d2v_model.train(documents = tagged_corpus,
                total_examples = d2v_model.corpus_count,
                epochs = 20)
```

## Doc2Vec in Python

__Obtain similar documents:__ 

```{python, include = T, eval = F}
d2v_model.docvecs.most_similar(["d_3"], topn = 3)

# [('d_12', 0.8913683295249939),
#  ('d_18', 0.870488166809082),
#  ('d_1', 0.8682408332824707)]
```

__Calculate the similarity of two documents:__ 

```{python, include = T, eval = F}
d2v_model.docvecs.similarity("d_1", "d_2")

# 0.8704882
```

## Doc2Vec in Python

__Infer vectors for new documents:__ 

```{python, include = T, eval = F}
embedding = d2v_model.infer_vector(["my", "new", "document"])
```

\vspace{.5cm}

__What it actually does:__

- Adds a new dimension to the one-hot input vector
- Adds a corresponding new line to the projection layer
- Trains these newly added weights of the projection layer
- Weights of the output layer are kept constant

## Word embeddings from Doc2Vec?

__Distributed memory model:__

- Word embeddings are an essential part of the training procedure 
- Can be queried in the same way as in the Word2Vec models

\vspace{.5cm}

__Distributed Bag-of-words model:__

- No meaningful word embeddings are produced 
- __Unless:__ `dbow_words`-option is set to `1`
- Interleaved training of word vectors in skip-gram fashion
- Slows down training, but yields meaningful word embeddings
  
## Word embeddings from Doc2Vec?

__Functionality exactly alike to Word2Vec:__

- Plain embeddings:

```{python, include = T, eval = F}
d2v_model.wv[]
```

- Calculating similarity:

```{python, include = T, eval = F}
d2v_model.wv.similarity()
```

- Obtaining (most) similar embeddings:

```{python, include = T, eval = F}
d2v_model.wv.most_similar()
```
