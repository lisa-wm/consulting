---
title: "Basic Text Cleaning"
author: "Asmik & Lisa -- for Intro to NLP"
date: "30 April 2021"
output:
  html_document: default
  pdf_document: default
---

![](../../Logo_Consulting.JPG)

```{r setup_packages, message=FALSE, warning=FALSE}
# Load required packages

library("data.table")
library("quanteda")
library("stringi")
library("stringr")
```

### Disclaimer

Note that, in the following, we will work with `data.table` objects, an alternative to the more classic `data.frame` structure.

The syntax requires some getting used to, but then we notice that

* `data.table` is a lot faster, which helps with the many text mining operations we conduct.
* it is much more memory-friendly: rather than creating copies over and over, objects are modified in place -- a property all the more valuable for large data sets, and common in most programming languages other than R.
* the syntax comes natural to those with SQL background.
* it integrates seamlessly with `mlr3`, a whole universe for machine learning in R which we will encounter later on.

This [vignette](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) provides a nice introduction to `data.table`.

### Read data

Assuming we have concluded the initial step of scraping the data and storing them as a `.csv` file, we now **import** the data into R.

```{r import_data, message=FALSE, warning=FALSE}
# Specify path to data location (to be modified according to individual file structure)

path <- "5_seminar/twitter_data.csv"

# Read data

twitter_data <- data.table::fread(
  path,
  encoding = "UTF-8", # standard encoding
  sep = ";") # standard for German-locale CSV files

# Define an ID variable that acts as a unique identifier for each tweet

# NB: data.table modifies twitter_data in place without requiring re-assignment (the `:=` notation tells it to create/modify a variable)

# If a user should indeed have posted several tweets with the exact same time stamp, number these tweets

twitter_data[
  , rank_timestamp := seq_len(.N),
  by = .(username, created_at)]

# Create ID variable from username, timestamp converted to floating number, and timestamp rank

twitter_data[, doc_id := paste(
  username,
  as.character(as.numeric(as.POSIXct(created_at))),
  rank_timestamp,
  sep = ""),
  by = seq_len(nrow(twitter_data))]

# Delete auxiliary timestamp rank

twitter_data[, rank_timestamp := NULL]

# Check whether ID is really unique

stopifnot(nrow(twitter_data) == length(unique(twitter_data$doc_id)))

# Set ID as identifying key for the data.table

data.table::setkey(twitter_data, doc_id) 

# Inspect

head(twitter_data)
```

### Remove umlauts

First, we remove all **umlauts and ligature s**, which are not part of the standard A-Z characters and might therefore cause problems, from the text.

Unfortunately, R supports several encodings for umlauts (e.g., there is a unicode representation for the letter "ä" but also a composed form using "a" and a special character). 
We use `stringi` to convert all to a common format first.

```{r remove_umlauts}

# Transform all text to general Latin characters

twitter_data[, full_text := stringi::stri_trans_general(full_text, "Any-Latin")]

# Then, convert non-standard characters to standard ones

twitter_data[, full_text := stringr::str_replace_all(
  full_text,
  c("\u00c4" = "Ae",
    "\u00e4" = "ae",
    "\u00d6" = "Oe",
    "\u00f6" = "oe",
    "\u00dc" = "Ue",
    "\u00fc" = "ue",
    "\u00df" = "ss"))]

# Inspect

head(twitter_data, 10L)$full_text

```

### Remove unwanted symbols

Looking better already!
Now, let's remove all **symbols and special characters** that carry no particular meaning (e.g., URLs or leftovers from ampersand conversion).
Emojis and hashtags are not removed just yet; we will tackle these in a second.

```{r remove_symbols}
# Replace all "\n" from line breaks with a simple whitespace

twitter_data[, full_text := stringr::str_replace_all(
  full_text, 
  pattern = "\\n", # with additional backslash to escape the special character
  replacement = " ")]

# Create pattern for symbols we want to discard altogether (including various types of quotes, sequences 
# resulting from unicode conversion, hyperlinks, ...), concatenated with the logical OR operation

pattern <- stringr::str_c(c(
  "\U0022",
  "\U0027",
  "\U2018",
  "\U2019",
  "\U201C",
  "\U201D",
  "\U201E",
  "\U201F",
  "&amp;",
  "&lt;",
  "&gt;",
  "%",
  " http([^ ]*)",
  "http([^ ]*)",
  "\\\n"),
  collapse = "|")

twitter_data[, full_text := stringr::str_remove_all(full_text, pattern)]

# Remove additional whitespaces induced by removal

twitter_data[, full_text := stringr::str_squish(full_text)]

# Inspect

head(twitter_data, 10L)$full_text

```

### Extract Twitter-specific symbols

After handling standard text cleaning issues we remove the Twitter-specific sequences from the plain text, namely

* emojis (the corresponding unicode list of emojis is available [here](https://github.com/today-is-a-good-day/emojis/blob/master/emDict.csv)),
* hashtags, and
* tags.

This time, though, we keep the extracted parts and store them into separate variables since they might be useful for sentiment analysis.

First, specify patterns with regex:

```{r patterns}

# Specify emoji pattern (using external data)

path <- "5_seminar/emojis_unicode.csv"

emojis_unicode <- data.table::fread(
  path,
  encoding = "UTF-8",
  sep = ";")

pattern_emoji <- stringr::str_c(emojis_unicode$symbol, collapse = "|")

# Specify hashtag pattern: # symbol, followed by anything alphanumeric

pattern_hashtag <- "(#)[[:alnum:]]+"

# Specify tag pattern: @ symbol, followed by anything but a space

pattern_tag <- "@\\S+"

```

Extract all matching sequences and store them in separate columns:

```{r extraction}

# Extract

twitter_data[, emojis := stringr::str_extract_all(full_text, pattern_emoji)]
twitter_data[, hashtags := stringr::str_extract_all(full_text, pattern_hashtag)]
twitter_data[, tags := stringr::str_extract_all(full_text, pattern_tag)]

# Inspect

tail(twitter_data)

```

Seems to do the job. 
We will now deal with one more particularity that requires some regex wrangling: frequently, hashtags use camel case to concatenate several words (e.g., #CamelCase), a phenomenon we would like to trace back to the original words:

```{r camel_case}

# Define pattern for camel case hashtag (#, followed at least one character of any kind, a capital letter, and at least two lowercase letters)

pattern_camelcase_hashtag <- "#(.)+[:upper:][:lower:]{2,}"

# Define pattern to split along (lowercase, directly followed by capital letter)

pattern_split_camelcase <- "(?<=[:lower:])(?=[:upper:])"

# Example

camel <- stringr::str_extract("Don't use #CamelCase in R please", pattern_camelcase_hashtag)
stringr::str_split(camel, pattern_split_camelcase)

```


```{r camel_case_2}

# Find and split cases in Twitter data

twitter_data[, full_text := lapply(
  .I, # apply to each row
  function(i) {
    components <- unlist(stringr::str_split(full_text[i], " ")) # split text into single tokens separated by space
    case_numbers <- which(stringr::str_detect(
      components, pattern_camelcase_hashtag)) # find camel case
    cases <- components[case_numbers] # collect cases
    solved_cases <- sapply(
      stringr::str_split(cases, pattern_split_camelcase),
      function(j) paste0(c(j), collapse = " ")) # split and concatenate with spaces in between
    components[case_numbers] <- solved_cases # insert back
    paste0(c(components), collapse = " ")})] # unsplit text

# Inspect

head(twitter_data, 10L)$full_text

```

Et voilà, we seem to have caught and stored all special symbols, so we can eventually remove them from the text.

```{r remove}

# Collect all patterns to remove

patterns_to_remove <- stringr::str_c(
  c(pattern_emoji, "#", pattern_tag, "[^\001-\177]"),
  collapse = "|")

# Remove patterns and unwanted whitespace

twitter_data[, full_text := stringr::str_remove_all(full_text, patterns_to_remove)]
twitter_data[, full_text := stringr::str_squish(full_text)]

# Inspect

head(twitter_data, 10L)$full_text
```

### Create `corpus` object

In the end we **convert** our nice and clean data to a `corpus`, the most basic text object in `quanteda`.

A `corpus` carries documents identified by an ID variable and containing text, plus additional variables on document level if needed:

```{r corpus}

# Convert

twitter_corpus <- quanteda::corpus(
  twitter_data,
  docid_field = "doc_id",
  text_field = "full_text")

# Inspect

twitter_corpus
```

```{r corpus_2}

head(quanteda::docvars(twitter_corpus))
```

```{r corpus_3}

# Save corpus object for further analysis

saveRDS(twitter_corpus, "5_seminar/twitter_corpus.RDS")
```

**With this, we have solid base to work on.**