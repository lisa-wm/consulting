---
title: "Static Feature Extraction"
author: "Asmik & Lisa -- for Intro to NLP"
date: "30 April 2021"
output:
  html_document: default
  pdf_document: default
---

![](../../Logo_Consulting.JPG)

```{r setup_packages, message=FALSE, warning=FALSE}
# Load required packages

library("data.table")
library("quanteda")
library("stringi")
library("stringr")
```

### Include

* Polarity clues (other dictionary for exercise)
* Number of emojis (hashtags for exercise)
* Intensification (negation for exercise)
* POS tags

### Create `tokens` object

The following analyses all revolve around the presence/absence or number of specific text **tokens** across documents.
Therefore, we convert our `corpus` object from before to a `tokens`, where documents are represented by single tokens rather than a fluent text.

```{r read}

path <- "5_seminar/twitter_corpus.RDS"

twitter_corpus <- readRDS(path)

```

```{r tokens}

twitter_tokens <- quanteda::tokens(
  twitter_corpus,
  what = "word", # take tokens to be words
  remove_symbols = TRUE, # in case we forgot anything earlier
  remove_numbers = TRUE, # not relevant
  remove_separators = TRUE, # not relevant
  remove_punct = TRUE, # not relevant
  split_hyphens = TRUE, # sometimes useful with German language
  lowercase = TRUE, # omit cases
  include_docvars = TRUE) # keep additional variables

```

On the occasion, we perform **stemming** on our tokens to increase their congruency across documents (remember, we want to have our documents represented by as many common tokens as possible).
`quanteda` has a built-in functionality that in turn calls `SnowballC`'s `wordStem` function:

```{r stem}

twitter_tokens <- quanteda::tokens_wordstem(
  twitter_tokens, 
  language = "german")

twitter_tokens

```

We quickly spot some tokens that do not seem to be of much help (e.g., "zu").
This is where **stopwords** come in.
We define a list of such stopwords and have `quanteda` remove them from our `tokens`.

However, we need to account for the fact that we have done quite a bit of text manipulation so far -- in particular, we have removed umlauts from our tokens and reduced them to their word stem. 
In order to make sure our stopwords are matched correctly, we should give them the same treatment. 

In fact, we will do this cleaning-stemming transformation more than once, so we encapsulate it in a small function we can call again later:

```{r clean_stem}

clean_and_stem <- function(text) {
  
  # Convert to uniform encoding
  
  text <-  stringi::stri_trans_general(text, "Any-Latin")
  
  # Replace umlauts and litigate s
  
  text <- stringr::str_replace_all(
    text,
      c("\u00c4" = "Ae",
      "\u00e4" = "ae",
      "\u00d6" = "Oe",
      "\u00f6" = "oe",
      "\u00dc" = "Ue",
      "\u00fc" = "ue",
      "\u00df" = "ss"))
  
  # Stem
  
  text <- SnowballC::wordStem(text)
  
  text

}

clean_and_stem(c("DÃ¶ner", "Bereicherung", "kulinarischer", "Angebote"))

```

### Create `dfm` object

document-feature matrix

### Find polarity clues

The first static feature we will extract is the number of (positive or negative) **polarity clues** in the data.

Polarity clues are sentiment-bearing words like "grauenhaft".
There is a variety of open-source lists for these; we will use the **Global Polarity Clues** collection available [here](http://www.ulliwaltinger.de/sentiment/).

`quanteda` offers `dictionary` objects which allow for convenient look-up.

```{r get_dict}

path <- "5_seminar/twitter_corpus.RDS"

twitter_corpus <- readRDS(path)

```