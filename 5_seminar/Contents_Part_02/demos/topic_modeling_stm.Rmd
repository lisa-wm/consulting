---
title: "Topic Modeling -- STM"
author: "Asmik & Lisa -- for Intro to NLP"
date: "30 April 2021"
output:
  html_document: default
  pdf_document: default
---

![](../../Logo_Consulting.JPG)

## Modeling topics with an STM

This demo is about **topic modeling with a structural topic model (STM)**.

We show how to prepare the data for fitting an STM ([Roberts et al., 2016](https://scholar.princeton.edu/sites/default/files/bstewart/files/a_model_of_text_for_experimentation_in_the_social_sciences.pdf)), conduct the modeling process, and analyze the results.


```{r setup_packages, message=FALSE, warning=FALSE}
# Load required packages

library("data.table")
library("ggplot2")
library("quanteda")
library("stm")
```

### Create `dfm` object

As in the static feature extraction demo, we convert our data to a `dfm`. 
However, we focus on slightly different tokens this time as we are now interested in identifying topical relations.

First, we get the **data** (the `path` variable again requiring individual adaptation):

```{r path, include = FALSE}
path <- "5_seminar"
```


```{r read}
# path <- ... (individual file location)

twitter_corpus <- readRDS(sprintf("%s/twitter_corpus.RDS", path))

twitter_corpus

```

Now let's **tokenize** our corpus in a way appropriate for topic modeling. 

We assume that topical information is mostly carried by nouns and choose to remove all other words for document representation, attempting to eliminate noise that might distract our topic model.
This is, of course, a simplification, but that's life (of an analyst). 

Luckily, nouns are pretty easy to identify by their leading capital letters in German (implicitly relying on MPs' grammar skills and/or auto-correct, and accepting that this will also include non-nouns from the beginning of a sentence):

```{r tokenize}

# First, tokenize as usual

twitter_tokens <- quanteda::tokens(
  twitter_corpus,
  what = "word",
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_separators = TRUE,
  split_hyphens = TRUE,
  include_docvars = TRUE)

```

```{r stem}

# Stem

twitter_tokens <- quanteda::tokens_wordstem(
  twitter_tokens, 
  language = "german")

```

```{r keep}

# Keep only tokens starting with a capital letter

twitter_tokens <- quanteda::tokens_keep(
  twitter_tokens,
  pattern = c("[:upper:]([:lower:])+"),
  valuetype = "regex",
  case_insensitive = FALSE)

# Lowercase afterwards

twitter_tokens <- quanteda::tokens_tolower(twitter_tokens)

```

```{r clean_stem, include=FALSE}

clean_and_stem <- function(text) {
  
  # Convert to uniform encoding
  
  text <-  stringi::stri_trans_general(text, "Any-Latin")
  
  # Replace umlauts and litigate s
  
  text <- stringr::str_replace_all(
    text,
      c("\u00c4" = "Ae",
      "\u00e4" = "ae",
      "\u00d6" = "Oe",
      "\u00f6" = "oe",
      "\u00dc" = "Ue",
      "\u00fc" = "ue",
      "\u00df" = "ss"))
  
  # Stem
  
  text <- SnowballC::wordStem(text)
  
  text

}

clean_and_stem(c("DÃ¶ner", "Bereicherung", "kulinarischer", "Angebote"))

```

```{r stopwords}

# Remove stopwords using our cleaning function from earlier

stopwords <- clean_and_stem(quanteda::stopwords(language = "de"))
stopwords <- unique(stopwords)
stopwords <- stopwords[nchar(stopwords) > 0]

twitter_tokens <- quanteda::tokens_remove(
  twitter_tokens,
  pattern = stopwords)

# Inspect

twitter_tokens

```

Looking good, so convert to `dfm`:

```{r dfm}

twitter_dfm <- quanteda::dfm(twitter_tokens)

twitter_dfm

```

Unfortunately, as the above output tells us, our `dfm` is extremely **sparse** -- tokens are not at all common across documents, which will pose a challenge to any topic model.
This is actually not too surprising when we recall the extreme brevity of our Twitter documents.

Therefore, we make another simplifying concession: we **pool** documents to create larger entities that should share more common tokens.

In this demo, we pool very coarsely and concatenate all tweets by one user to a single document.
Of course, it is not terribly plausible to assume users sticking to exactly one topic, but our toy data here are somewhat limited.

In a real application you would try and find a more suitable way of pooling documents, e.g., taking into account the date of creation (and probably do it as little as possible).

Using the pooling variable, we can reshape the `dfm` by **grouping** it:

```{r group}

# Group dfm

twitter_dfm_grouped <- quanteda::dfm_group(
  twitter_dfm, 
  quanteda::docvars(twitter_dfm)$username)

# In addition, eliminate less common features

twitter_dfm_grouped <- quanteda::dfm_select(
  twitter_dfm_grouped,
  names(quanteda::topfeatures(twitter_dfm_grouped, n = 300)))

twitter_dfm_grouped

```

### Create prevalence formula

The special trait of the STM is its making use of s document-level **meta data**, so we need to specify which information to use here. In the `stm` function the corresponding argument is called `prevalence` and must be stated as a formula.

We will use the following meta variables (smooth effects defaulting to B-splines):

* `party` as a categorical variable
* `bundesland` as a categorical variable
* `unemployment_rate` as a smooth effect with 5 degrees of freedom
* `share_pop_migration` as a smooth effect with 5 degrees of freedom

The resulting formula, `~ party + bundesland + s(unemployment_rate, df = 5) + s(share_pop_migration, df = 5)`, is perfectly disputable -- this decision is up to domain knowledge and some trial-and-error, and we do not claim to use the ideal formula in this demo.

```{r prevalence}

prevalence_formula <- as.formula(paste(
  "", 
  "party + bundesland + s(unemployment_rate, df = 5) + s(share_pop_migration, df = 5)", 
  sep = "~"))

print(prevalence_formula)

```

Now we have one thing to pay attention to: the `stm` implementation cannot handle **missing data** in its prevalence variables.
We must therefore exclude non-complete observations (alternatively, we could impute the missing values if it made sense and we had a meaningful imputation mechanism):

```{r missings}

twitter_dfm_grouped <- quanteda::dfm_subset(
  twitter_dfm_grouped,
  !is.na(party) & !is.na(bundesland) & !is.na(unemployment_rate) & !is.na(share_pop_migration))

twitter_dfm_grouped

```

### Create `stm` object

We then convert our `dfm` to an `stm` object we can use in the structural topic model.
This is super simple with `quanteda`:

```{r stm}

twitter_stm <- quanteda::convert(twitter_dfm_grouped, to = "stm")

str(twitter_stm)

```

### Fit STM

Time to fit the actual **model**.

While models learn all necessary *parameters* during training, *hyperparameters* need to be specified upfront.
The most critical hyperparameter here is the number of topics, *K*. 
So how to find *K*?

Topic modeling being an unsupervised task (i.e., we do not have access to the ground truth), the quality of model output must ultimately be judged by human interpretability.
We can seek some help from the data: `stm` offers a function that searches for the optimal *K* (still requiring a range of potential values!).
Exactly what optimal means can be somewhat steered by the user.
We will use the *held-out likelihood* option which creates an artificial supervised task: we mask a certain part of some of the documents and ask the model to complete them (note that the ground truth is known now).
The more predictive power our model has, the higher the likelihood for the held-out words should be (for details see [Wallach et al., 2009](http://dirichlet.net/pdf/wallach09evaluation.pdf):

```{r searchk, results='hide'}

# Conduct hyperparameter search

hyperparameter_search <- stm::searchK(
  documents = twitter_stm$documents,
  vocab = twitter_stm$vocab,
  data = twitter_stm$meta,
  K = c(3:10),
  prevalence = prevalence_formula,
  heldout.seed = 1, # seed for reproducibility
  max.em.its = 5, # number of iterations
  init.type = "Spectral")

```

```{r findk}

# Fix optimal number of topics

(n_topics <- as.numeric(hyperparameter_search$results[
  which.max(hyperparameter_search$results[, "heldout"]), "K"]))

```

Seems 8 topics are a good choice, so let's fit the actual **model** now:
