---
title: "Static Feature Extraction"
author: "Asmik & Lisa -- for Intro to NLP"
date: "April/May 2021"
output:
  html_document: default
  pdf_document: default
---

![](../../Logo_Consulting.JPG)

## Extracting Static Features

This demo is about **static feature extraction**.

We show how to extract several features deemed useful for sentiment analysis. *Static* refers to these features being independent across documents (unlike, for instance, word embeddings).

```{r setup_packages, message=FALSE, warning=FALSE}
# Load required packages

library(data.table)
library(quanteda)
library(stringi)
library(stringr)
```

### Scope

We will extract a set of static features that have been shown to work well for sentiment analysis in general, including:

* Polarity clues
* Number of emojis
* Negations
* Character unigrams
* POS tags

### Create `tokens` object

The following analyses all revolve around the presence/absence or number of specific text **tokens** across documents.
Therefore, we convert our `corpus` object from before to a `tokens`, where documents are represented by single tokens rather than a fluent text, and along the way omit casing.

```{r path, include = FALSE}
path <- "../.."
```


```{r read}
# path <- ... (individual file location)

twitter_corpus <- readRDS(sprintf("%s/twitter_corpus.RDS", path))

```

```{r tokens}

twitter_tokens <- quanteda::tokens(
  twitter_corpus,
  what = "word", # take tokens to be words
  remove_symbols = TRUE, # in case we forgot anything earlier
  remove_numbers = TRUE, # not relevant
  remove_separators = TRUE, # not relevant
  remove_punct = TRUE, # not relevant
  split_hyphens = TRUE, # sometimes useful with German language
  include_docvars = TRUE) # keep additional variables

twitter_tokens <- quanteda::tokens_tolower(twitter_tokens)

```

On the occasion, we perform **stemming** on our tokens to increase their congruency across documents (remember, we want our documents to be represented by as many common tokens as possible).
`quanteda` has a built-in functionality that in turn calls `SnowballC`'s `wordStem` function:

```{r stem}

twitter_tokens <- quanteda::tokens_wordstem(
  twitter_tokens, 
  language = "german")

twitter_tokens

```

We quickly spot some tokens that do not seem to be of much help (e.g., "zu").
This is where **stopwords** come in.
We define a list of such stopwords and have `quanteda` remove them from our `tokens`.

However, we need to account for the fact that we have done quite a bit of text manipulation so far -- in particular, we have removed umlauts from our tokens and reduced them to their word stem. 
In order to make sure our stopwords are matched correctly, we should give them the same treatment. 

In fact, we will do this cleaning-stemming transformation more than once, so we encapsulate it in a small function we can call again later:

```{r clean_stem}

clean_and_stem <- function(text) {
  
  # Convert to uniform encoding
  
  text <-  stringi::stri_trans_general(text, "Any-Latin")
  
  # Replace umlauts and litigate s
  
  text <- stringr::str_replace_all(
    text,
      c("\u00c4" = "Ae",
      "\u00e4" = "ae",
      "\u00d6" = "Oe",
      "\u00f6" = "oe",
      "\u00dc" = "Ue",
      "\u00fc" = "ue",
      "\u00df" = "ss"))
  
  # Stem
  
  text <- SnowballC::wordStem(text)
  
  text

}

clean_and_stem(c("DÃ¶ner", "Bereicherung", "kulinarischer", "Angebote"))

```

Seems to do the job (but also note the limitations of stemming).

We create a clean stopwords list, which we again wrap in a function we can re-use:

```{r stopwords}

# Create stopwords list

stopwords <- clean_and_stem(quanteda::stopwords(language = "de"))

# Remove potential duplicates and empty instances induced by stemming

stopwords <- unique(stopwords)
stopwords <- stopwords[nchar(stopwords) > 0]

# Inspect

sort(stopwords)

```

Words indicating negation seem too important to discard for the sentiment analysis task. 
We cross them from the stopwords list and remove the remaining stopwords from our `tokens`:

```{r stopwords_2}

# Deselect negation words

stopwords <- stringr::str_remove_all(
  stopwords,
  "kein(.)*|nicht")

# Remove from tokens

twitter_tokens <- quanteda::tokens_remove(twitter_tokens, stopwords)

# Inspect

twitter_tokens

```

Apparently, some very short word stumps result from our cleaning operations, which we will also discard:

```{r stopwords_3}

twitter_tokens <- quanteda::tokens_select(twitter_tokens, min_nchar = 3)
twitter_tokens <- quanteda::tokens_remove(
  twitter_tokens, 
  c("der", "die", "das", "was", "wer", "wie", "ich", "sie", "wir", "ihr"))

twitter_tokens

```

There is still a variety of tokens we could probably remove (there are more extensive stopwords lists to be found online, the built-in one from `quanteda` is not too good tbh).
In a real application you should dedicate some time to this sub-task to make sure your tokens representation is not contaminated by such noise.

### Create `dfm` object

Now we have characterized our documents by tokens, we want to put some numbers on it: how often do the present tokens occur in each document? 

This is where the **document-feature matrix** (`dfm`) comes in:

```{r dfm}

twitter_dfm <- quanteda::dfm(twitter_tokens)

twitter_dfm

saveRDS(twitter_dfm, sprintf("%s/twitter_dfm.RDS", path))

```

### Find polarity clues

Time for some feature extraction at last.

The first static feature we will extract is the number of (positive or negative) **polarity clues** in the data.

Polarity clues are sentiment-bearing words like "grauenhaft".
There is a variety of open-source lists for these; we will use the *Global Polarity Clues* collection available [here](http://www.ulliwaltinger.de/sentiment/) (already somewhat processed to only include relevant columns).

```{r get_dict, include=FALSE}

data_german_polarity_clues <- c(
  "GermanPolarityClues-Positive-21042012.tsv", 
  "GermanPolarityClues-Negative-21042012.tsv")

dict_list <- lapply(
  seq_along(data_german_polarity_clues),
  function(i) {
    dt <- data.table::fread(
      sprintf(
        "../../../2_code/1_data/0_external_data/%s", 
        data_german_polarity_clues[i]),
      encoding = "UTF-8",
      drop = c(2:6),
      header = FALSE,
      col.names = c("term"),
      quote = "")
    dt[, term]})

names(dict_list) <- c("positive", "negative")

saveRDS(dict_list, sprintf("%s/global_polarity_clues.RDS", path))

```

```{r get_dict_2}

# Get polarities

list_gpc <- readRDS(sprintf("%s/global_polarity_clues.RDS", path))

# Clean

list_gpc <- lapply(list_gpc, clean_and_stem)

# Inspect

str(list_gpc)

```

`quanteda` offers dedicated `dictionary` objects which allow for convenient look-up of `dfm`'s, so we convert the GPC to such a format (note that this operation also takes care of lowercasing):


```{r make_dict}

dictionary_gpc <- quanteda::dictionary(
  list(
    positive = list_gpc$positive,
    negative = list_gpc$negative))

dictionary_gpc

```

Perform the look-up:

```{r polarities}

# Look up polarities

twitter_polarities <- quanteda::dfm_lookup(twitter_dfm, dictionary_gpc)

# Convert resulting dfm object to data.table (via data.frame as quanteda does not yet support conversion to data.table)

twitter_polarities <- quanteda::convert(twitter_polarities, to = "data.frame")

twitter_polarities <- data.table::as.data.table(
  twitter_polarities,
  key = "doc_id")

# Inspect

twitter_polarities[sample(twitter_polarities[, .I], 10)]

```

### Get number of emojis used

The next feature we are going to extract is the number of **emojis** (there are also online sources for associating them with polarities).

Good thing we did the emoji extraction before: 

```{r emojis}

# For this, we cannot operate on the texts but need to get the docvars where all non-text variables are stored

twitter_docvars <- data.table::as.data.table(
  cbind(
    doc_id = quanteda::docid(twitter_corpus),
    quanteda::docvars(twitter_corpus)), 
  key = "doc_id")

twitter_docvars[sample(twitter_docvars[, .I], 10)]

```

```{r emojis_2}

# Count emojis (stored as list objects, so we take the length of these lists)

twitter_emojis <- twitter_docvars[
  , .(doc_id, emojis)
  ][, n_emojis := lengths(emojis)
    ][, emojis := NULL]

list(
  twitter_emojis = twitter_emojis[sample(twitter_emojis[, .I], 10)],
  distribution = table(twitter_emojis$n_emojis))

```

### Detect negation

**Negation** handling with the bag-of-words assumption is difficult.
There are more sophisticated approaches, but for this demo we will simply note whether or not an expression of negation is present.

First, define negation indicators (and don't forget to clean):

```{r negation}

tokens_negation <- clean_and_stem(c(
    "nicht", 
    "nie", 
    "niemals", 
    "nein", 
    "niemand", 
    "nix", 
    "nirgends", 
    "kein"))

```

Then, again use a `dictionary` to count for each document how often one of the above tokens occurs:

```{r negation_2}

# Create dictionary

dictionary_negation <- quanteda::dictionary(list(negation = tokens_negation))

# Match tokens

twitter_negation <- quanteda::dfm_lookup(twitter_dfm, dictionary_negation)

# Convert resulting dfm object to data.table (via data.frame as quanteda does not yet support conversion to data.table)

twitter_negation <- quanteda::convert(twitter_negation, to = "data.frame")

twitter_negation <- data.table::as.data.table(
  twitter_negation,
  key = "doc_id")

# Inspect

twitter_negation[sample(twitter_negation[, .I], 10)]

```

### Get character unigrams

Due to the sheer variety of words that exist in a language a simple tokens representation will typically contain a huge number of features:

```{r unigrams}

quanteda::nfeat(twitter_dfm)

```

This is an overwhelmingly large number for any machine learning classifier (particularly so if we have more features than observations).

We will therefore streamline our tokens further. 
Most applications include some kind of **n-grams**. 
Interestingly, character unigrams seem to work quite well, i.e., rather than entire words or even concatenations, we simply count occurrences of single letters. 

Thereby, we effectively coerce our texts to a set of merely 26 features:

```{r unigrams_2}

# Create a new tokens object, this time consisting of single characters

twitter_tokens_char <- quanteda::tokens(
  twitter_corpus,
  what = "character",
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_separators = TRUE,
  split_hyphens = TRUE) 

# Convert to dfm

twitter_char_unigrams <-  quanteda::dfm(twitter_tokens_char)

# Convert resulting dfm object to data.table (via data.frame as quanteda does not yet support conversion to data.table)

twitter_char_unigrams <- quanteda::convert(twitter_char_unigrams, to = "data.frame")

twitter_char_unigrams <- data.table::as.data.table(
  twitter_char_unigrams,
  key = "doc_id")

data.table::setcolorder(twitter_char_unigrams, c("doc_id", letters))

# Inspect

twitter_char_unigrams[sample(twitter_char_unigrams[, .I], 10)]

```

### Get POS tags

The last set of features we are going to extract are **part-of-speech (POS) tags**.
Note that tagging must take place over the original documents, not some tokens representation agnostic to grammatical structures.

We use the `spacyR` package, a wrapper around Python's `spaCy` package that is nicely integrated with `quanteda` (well, they share the same author after all).
This [vignette](https://cran.r-project.org/web/packages/spacyr/vignettes/using_spacyr.html) provides a quick intro.

Note that setting this up can be a serious pain -- getting R and Python to work hand in hand on your local machine is hard sometimes.
When in doubt, resort to some clean environment such as Google Colab.

```{r pos, eval=FALSE}

# Install spacyr and get the relevant model for German language

spacyr::spacy_install()
spacyr::spacy_download_langmodel("de")

# Run POS tagger

spacyr::spacy_initialize(model = "de_core_news_sm")
  
twitter_pos_tags <- data.table::as.data.table(
  spacyr::spacy_parse(
    twitter_corpus,
    lemma = FALSE, # do not include lemmatized tokens
    entity = FALSE), # do not include named entities
  key = "doc_id")

```

```{r pos_2, include=FALSE}

load("../../../2_code/1_data/2_tmp_data/rdata_tweets_corpus_tagged.RData")

twitter_pos_tags <- tweets_corpus_tagged[doc_id %in% twitter_docvars[, doc_id]]

```

```{r pos_3}

head(twitter_pos_tags, 10)

```

Looking good already! 
For the POS tags to be admissible in sentiment analysis, however, we need to convert them from *long* to *wide* format:

```{r pos_4}

# Count occurrences per document and POS tag

twitter_pos_tags <- twitter_pos_tags[
  , .(doc_id, pos)
  ][, aux := 1
    ][, n_tags := sum(aux), by = list(doc_id, pos)
      ][, aux := NULL]

head(twitter_pos_tags)

``` 

```{r pos_5}

# Convert to wide format

twitter_pos_tags <- data.table::dcast(
  unique(twitter_pos_tags),
  doc_id ~ pos,
  value.var = "n_tags",
  fun.aggregate = sum)

head(twitter_pos_tags)

```

### Collect all static features

In a final step, we put everything together (starting the join operation with a list of all document IDs so we lose nothing):

```{r collect}

# Collect all features

twitter_features_static <- twitter_docvars[, .(doc_id, label)
  ][twitter_pos_tags,
    ][twitter_negation,
      ][twitter_char_unigrams,
        ][twitter_emojis,
          ][twitter_polarities, ]

saveRDS(twitter_features_static, sprintf("%s/twitter_features_static.RDS", path))

twitter_features_static[sample(twitter_features_static[, .I], 10)]

```

**This provides us with a solid foundation for sentiment analysis! We now represent each document by a total of `r toString(ncol(twitter_features_static) - 1)` variables. Of course we can think of many more features to include here -- we will see some more in the exercise; in general, be creative and see how classification can be improved by adding different explanatory variables.**

![](figures/screenshot_creative.svg.jpg)