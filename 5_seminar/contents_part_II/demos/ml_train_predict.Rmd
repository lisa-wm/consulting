---
title: "Sentiment Analysis -- Training & Prediction"
author: "Asmik & Lisa -- for Intro to NLP"
date: "April/May 2021"
output:
  html_document: default
  pdf_document: default
---

![](../../Logo_Consulting.JPG)

## Train Sentiment Analysis Classifier and Compute Prediction

This demo is about **training a classifier for sentiment analysis and predicting sentiment labels**.

We show how to perform sentiment analysis with machine learning methods. 
More specifically, we will use a logistic regression model to predict sentiment labels in a binary classification task.

The code is based on the `mlr3` universe, a unifying framework for a broad variety of tasks in supervised learning that subsumes many other packages and provides consistent syntax.
This [book](https://mlr3book.mlr-org.com/) provides a great introduction to `mlr3`. Further examples with applied use cases may be found [here](https://mlr3gallery.mlr-org.com/).

Note that it is built in `R6`. 
Covering object-oriented programming (OOP) is beyond the scope of this course but we will mention some basics.
The most important things to know here are: 

* All objects are of a specific *class* for which different *methods* exist.
* Methods are very similar to functions, but objects are modified *in place* rather than re-assigned (which is much more memory-friendly and natural to non-R programmers, we just need to pay attention that each operation might actually change our object even if we do not assign anything). 

```{r setup_packages, message=FALSE, warning=FALSE}
# Load required packages

library(data.table)
library(mlr3verse)
```

### Perform train-test split

```{r path, include = FALSE}

path <- "5_seminar"

```

As always, we start by **reading** the data.
Now there is one big BUT: we cannot simply take the data with topic embeddings as created in the last demo -- if we used these and splitted them into train and test sets, we would see information from the training observations leak into the test part.
That is why we called topic labels and embeddings *dynamic* features.

We want to avoid such bias and proceed as follows:

1. We split the data without topic labels / embeddings into training and test data.
2. We extract topics and compute topic-specific embeddings *separately* for both data sets.

```{r read}

# path <- ... (individual file location)

twitter_corpus <- readRDS(sprintf("%s/twitter_corpus.RDS", path))

```

```{r train-test}

# Create a binary variable indicating whether the document belongs to training or test data (setting a seed for reproducibility and choosing 70% of data for training)

set.seed(123)

twitter_corpus$is_train <- rbinom(
  n = quanteda::ndoc(twitter_corpus), 
  size = 1, 
  prob = 0.7)

twitter_corpus_train <- quanteda::corpus_subset(twitter_corpus, is_train == 1)
twitter_corpus_test <- quanteda::corpus_subset(twitter_corpus, is_train == 0)

quanteda::ndoc(twitter_corpus_train) + quanteda::ndoc(twitter_corpus_test) == 
  quanteda::ndoc(twitter_corpus)

```

Now we compute the **topic-specific embeddings**.
For the sake of clarity, we have collected all operations performed in the demos on topic modeling and word embeddings into a single function in the background (keeping the number of topics and word embeddings, respectively, unchanged).
We can now call this function on both training and test data:

```{r fun_tm, eval=FALSE}

make_topic_embeddings <- function(twitter_corpus) {
  
  ...
  
}

```


```{r fun_tm_2, include=FALSE}

make_topic_embeddings <- function(twitter_corpus) {
  
  # TOPIC MODELING
  
  quanteda::docvars(twitter_corpus)$text <- quanteda::texts(twitter_corpus)
  
  twitter_tokens <- quanteda::tokens(
    twitter_corpus,
    what = "word",
    remove_symbols = TRUE,
    remove_numbers = TRUE,
    remove_separators = TRUE,
    split_hyphens = TRUE,
    include_docvars = TRUE)
  
  twitter_tokens <- quanteda::tokens_wordstem(
    twitter_tokens, 
    language = "german")
  
  twitter_tokens <- quanteda::tokens_tolower(twitter_tokens)
  
  stopwords <- readRDS(sprintf("%s/stopwords.RDS", path))

  twitter_tokens <- quanteda::tokens_remove(
    twitter_tokens,
    pattern = stopwords)
  
  twitter_tokens <- quanteda::tokens_select(twitter_tokens, min_nchar = 3)
  
  twitter_dfm <- quanteda::dfm(twitter_tokens)
  
  twitter_dfm <- quanteda::dfm_select(
    twitter_dfm,
    names(quanteda::topfeatures(twitter_dfm, n = 300)))
  
  prevalence_formula <- as.formula(paste(
    "", 
    "party + bundesland", 
    sep = "~"))
  
  twitter_dfm <- quanteda::dfm_subset(
    twitter_dfm,
    !is.na(party) & !is.na(bundesland) & !is.na(unemployment_rate) &
      !is.na(share_pop_migration))
  
  twitter_stm <- quanteda::convert(twitter_dfm, to = "stm")
  
  topic_model <- stm::stm(
    documents = twitter_stm$documents,
    vocab = twitter_stm$vocab,
    data = twitter_stm$meta,
    K = 3,
    prevalence = prevalence_formula,
    gamma.prior = "L1",
    seed = 1,
    max.em.its = 15,
    init.type = "Spectral",
    verbose = FALSE)
  
  topic_probs <- stm::make.dt(topic_model)[
    , doc_id := names(twitter_stm$documents)
      ][, docnum := NULL]
  
  topic_cols <- names(topic_probs)[startsWith(names(topic_probs), "Topic")]

  topic_probs[
    , `:=` (
      max_topic_score = max(.SD, na.rm = TRUE),
      topic_label = which.max(.SD)),
    .SDcols = topic_cols,
    by = doc_id]
  
  twitter_docvars <- data.table::as.data.table(
    cbind(
      doc_id = quanteda::docid(twitter_corpus),
      quanteda::docvars(twitter_corpus)), 
    key = "doc_id")

  twitter_docvars <- topic_probs[
    twitter_docvars, on = "doc_id"
      ][, c(topic_cols) := NULL]
  
  twitter_corpus_with_topics <- twitter_corpus

  quanteda::docvars(twitter_corpus_with_topics) <- as.data.frame(twitter_docvars)
  
  twitter_corpus <- twitter_corpus_with_topics
  
  # EMBEDDINGS
  
  twitter_corpus$topic_label <- ifelse(
    is.na(twitter_corpus$topic_label),
    99, 
    twitter_corpus$topic_label)

  twitter_corpus_subsets <- lapply(
    unique(twitter_corpus$topic_label),
    function(i) quanteda::corpus_subset(twitter_corpus, topic_label == i))
  
  make_glove_embeddings <- function(text, stopwords, glove_args) {
  
    tokens <- tokenize(text, stopwords)
    vocabulary <- make_vocab(tokens, glove_args)
    glove <- make_glove_instance(vocabulary, glove_args)
    word_vectors <- get_word_vectors(tokens, glove, glove_args)
    doc_embeddings <- get_doc_embeddings(tokens, word_vectors)
    
    doc_embeddings
    
  }
  
  tokenize <- function(text, stopwords) {
  
    tokens <- quanteda::tokens(
      text,
      what = "word",
      remove_symbols = TRUE,
      remove_punct = TRUE,
      remove_numbers = TRUE,
      remove_separators = TRUE,
      split_hyphens = TRUE,
      include_docvars = TRUE)
    
    tokens <- quanteda::tokens_wordstem(tokens, language = "german")
    
    tokens <- quanteda::tokens_remove(
      quanteda::tokens_tolower(tokens),
      pattern = stopwords)
    
    tokens <- quanteda::tokens_select(tokens, min_nchar = 3)
    
    tokens
  
  }
  
  make_vocab <- function(tokens, term_count_min) {
    
    tokens <- as.list(tokens)
    itokens <- text2vec::itoken(tokens, progressbar = FALSE)
    vocab <- text2vec::create_vocabulary(itokens)
    vocab <- text2vec::prune_vocabulary(vocab, term_count_min = term_count_min)
    
    list(itokens = itokens, vocab = vocab)
    
  }
  
  make_glove_instance <- function(vocabulary, skip_grams_window, dimension, x_max) {
  
    vect <- text2vec::vocab_vectorizer(vocabulary$vocab)
    
    tcm <- text2vec::create_tcm(
      vocabulary$itokens, 
      vect, 
      skip_grams_window = skip_grams_window) 
    
    glove_instance <- text2vec::GlobalVectors$new(rank = dimension, x_max = x_max)
    
    list(tcm = tcm, glove_instance = glove_instance)
    
  }
  
  get_word_vectors <- function(tokens, glove, iterations, convergence_tol) {
  
    wv_main <- glove$glove_instance$fit_transform(
      glove$tcm, 
      n_iter = iterations, 
      convergence_tol = convergence_tol)  

    wv_context <- glove$glove_instance$components
    wv_main + t(wv_context)
  
  }
  
  get_doc_embeddings <- function(tokens, word_vectors) {
  
    dtm <- quanteda::dfm_match(
      quanteda::dfm(tokens),
      rownames(word_vectors))
    
    dtm <- text2vec::normalize(dtm, norm = "l1")
    
    as.matrix(dtm) %*% word_vectors
  
  }
  
  make_glove_embeddings <- function(text, 
                                  stopwords, 
                                  term_count_min,
                                  skip_grams_window, 
                                  dimension,
                                  x_max,
                                  iterations, 
                                  convergence_tol) {
  
    tokens <- tokenize(text, stopwords)
    vocabulary <- make_vocab(tokens, term_count_min)
    glove <- make_glove_instance(vocabulary, skip_grams_window, dimension, x_max)
    word_vectors <- get_word_vectors(tokens, glove, iterations, convergence_tol)
    doc_embeddings <- get_doc_embeddings(tokens, word_vectors)
    
    doc_embeddings
    
  }
  
  embeddings <- lapply(
    twitter_corpus_subsets,
    function(i) {make_glove_embeddings(
      quanteda::texts(i),
      stopwords = stopwords,
      term_count_min = 2,
      skip_grams_window = 5,
      dimension = 3,
      x_max = 10,
      iterations = 10,
      convergence_tol = 0.001)})
  
  embedding_matrix <- do.call(Matrix::bdiag, embeddings)
  
  embeddings_dt <- data.table::as.data.table(as.matrix(embedding_matrix))

  data.table::setnames(
    embeddings_dt,
    sprintf("embedding_%d", seq_along(embeddings_dt)))
  
  embeddings_dt$doc_id <- unlist(lapply(
    twitter_corpus_subsets, 
    function(i) quanteda::docvars(i)$doc_id))
  
  data.table::setkey(embeddings_dt, "doc_id")
  
  embeddings_dt
  
}

```

```{r tm_emb, warning=FALSE, results='hide'}

embeddings_train <- make_topic_embeddings(twitter_corpus_train)
embeddings_test <- make_topic_embeddings(twitter_corpus_test)

```

Next, we append the static features:

```{r read_sf}

# Read data

twitter_features_static <- readRDS(sprintf("%s/twitter_features_static.RDS", path))

# Append

data_train <- twitter_features_static[embeddings_train, on = "doc_id"]
data_test <- twitter_features_static[embeddings_test, on = "doc_id"]

# Inspect

list(train = head(data_train), test = head(data_test))

```

### Create classification task

Now we are all set for sentiment analysis.
The first step is to create a **task** object (note that we need to make sure the `doc_id` variable, which is not a feature but merely an identifier, is exempt from the features):

```{r tsk}

# Convert target variable to a factor

data_train$label <- as.factor(data_train$label)

# Create task object

task <- mlr3::TaskClassif$new("sentiment_analysis", data_train, target = "label")

# Set the role of doc_id to naming variable

task$set_col_roles("doc_id", "name")

# Inspect

task

```

### Create and train learner

The **learner** reflects our hypothesis about the feature-target relation. 

`mlr3` supports a variety of different learning algorithms (convenient look-up via `mlr_learners`) and even more are available in the `mlr3learners` extension package.
All are instantiated by the same call.

We choose **logistic regression** here, a generalized version of the linear regression model that is able to predict class probabilities (if the concept seems somewhat distant in your memory, have a look into [one of the standard statistics books](https://web.stanford.edu/~hastie/Papers/ESLII.pdf), or into [this one](https://christophm.github.io/interpretable-ml-book/logistic.html#example-1), or check out one of the abundant blogposts and YouTube sources).

The logistic regression model can be stated in different ways.
We will often see something of the form

\[\pi({x}_i) = \mathbb{P}(y_i = 1 \rvert 
{x}_i) = \frac{\exp(\beta_0 + \mathbf{\beta}^T \mathbf{x}_i)}{1 + \exp(\beta_0 +
\mathbf{\beta}^T \mathbf{x}_i)} = \frac{\exp(\beta_0 + \beta_1 x_{i, 1} + \dots + \beta_p x_{i, p})}{1 + \exp(\beta_0 + \beta_1 x_{i, 1} + \dots + \beta_p x_{i, p})},\]

specifying the probability of being in class 1 (here: *positive* class) for a given observation $\mathbf{x}_i$.

We can see how the so-called **log-odds** are related to the **linear predictor**:

\[\beta_0 + \mathbf{\beta}^T \mathbf{x}_i = \log 
\frac{\pi(\mathbf{x}_i)}{1 - \pi(\mathbf{x}_i)}.\]

So we do not directly model the conditional mean, as in the standard linear model, but a transformed version of it (hence *generalized* linear modeling).
Logistic regression does not require any hyperparameters, allowing us to skip the tuning part, and we can simply invoke a logistic regression learner by:

```{r lrn}

# Create learner

learner <- mlr3::lrn("classif.log_reg", predict_type = "prob")

learner

```

After instantiating the learner object, we do the actual **training** using our training task. 
Note how the learner is modified in-place:

```{r train, warning=FALSE}

# Train learner on training task

learner$train(task)

```

### Evaluate learner

Now is the time for our model to output some **predictions** on the test set so we can evaluate its performance.

```{r test, warning=FALSE}

# Compute predictions

data_test$label <- as.factor(data_test$label)
predictions <- learner$predict_newdata(data_test)

# Inspect confusion matrix

list(predictions = predictions, confusion = predictions$confusion)

```

Our learner looks like it's doing okay but not too accurate (it seems to struggle with the positive observations in particular). 
Let's compute some **performance metrics** (list all available metrics with `mlr_measures`) to get a better picture:

```{r eval}

eval_metrics <- list(
  mlr3::msr("classif.acc"), # accuracy
  mlr3::msr("classif.ppv"), # positive predictive value
  mlr3::msr("classif.fbeta") # f1 score
)

predictions$score(eval_metrics)

```

We can also plot the associated **ROC curve**:

```{r roc}

mlr3viz::autoplot(predictions, type = "roc") +
  ggplot2::ggtitle("ROC curve for Twitter sentiment analysis")

```

All in all, we seem to have found a fairly decent model.
It does not exactly achieve exciting performance, but the task at hand is actually a hard one (keep in mind that we try to predict the author's sentiment by such simple things as the amount of certain letters in the text).

In a real application model selection does not stop here; we would compare and benchmark multiple learners against each other to find the best one for our task.
For now, though, we stick with our logistic regression model.

### Fit final model

By splitting the data into train and test sets we deliberately forgo parts of the data for training (rule of thumb: more data, better model) so we had some spare observations for evaluation. 
This allowed us to obtain a (slightly pessimistic) performance estimation.

In the end we would typically train the learner on the entire data set and use the resulting, final model for predictions of future unseen data:

```{r final}

# Create task object

data_total <- rbind(data_train, data_test)
task_total <- mlr3::TaskClassif$new("sentiment_analysis_final", data_total, target = "label")
task_total$set_col_roles("doc_id", "name")

# Train learner

learner$train(task_total)

# Store model

final_model <- learner$model

# Inspect

summary(final_model)

```

```{r save, include=FALSE}

saveRDS(data_total, file = sprintf("%s/data_sentiment_analysis.RDS", path))

```

**And that's it for a first jump at sentiment analysis! We have performed a train-test split of our data, trained a logistic regression learner on the training task, used the resulting model to compute predictions for the test data to evaluate performance, and trained the final model we could now use for future sentiment analysis. **