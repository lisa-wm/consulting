\subsection{Data}
\label{data}

% ------------------------------------------------------------------------------

\begin{frame}{Data collection: web scraping}

\textbf{Idea:} collect tweets by members of the German parliament 
(\textit{Bundestag}) issued after the last federal election in September 2017

\vfill

\begin{enumerate}
  \setlength{\itemsep}{-0.3em}
  \setlength{\parskip}{0.3em}
  \item Gather MPs' names and basic information from the official Bundestag 
  website
  \item Find Twitter account names
  \item Acquire socioeconomic information for the time of the last federal 
  election on a per-district level
  \item Scrape actual tweets along with some additional variables
\end{enumerate}

\vfill

$\rightarrow ~ $\textbf{Manual labeling process}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data collection: web scraping}

\begin{figure}
  \raggedright
  \includegraphics[width = 0.3\textwidth]{figures/screenshot_amthor}
\caption{\raggedright url{https://www.bundestag.de/abgeordnete/}}
\end{figure}

\begin{figure}
  \raggedright
  \includegraphics[width = 0.6\textwidth]{figures/screenshot_andi}
\caption{\raggedright \url{https://www.twitter.com/}}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data labeling}

\begin{itemize}
  \item For each tweet: assign polarities \highlight{positive} or 
  \highlight{negative}, and also  \highlight{topic} descriptions required for 
  BERT's ABSA task
  \item Note: large number of tweets with no apparent sentiment, aspect 
  detection often difficult
  \item Class label distribution: \highlight{72\%} negative labels
\end{itemize}

\vfill

\tiny
  \begin{tabular}{l|l|l|l|r|r|l}
  \texttt{username} & \texttt{party} & \texttt{created\_at} & \texttt{text} & 
  \texttt{followers} & \texttt{unemployment\_rate} & \texttt{label}\\
  \hline
  karl\_lauterbach & spd & 2019-12-01 09:44:00 & "Die Wahl ..." & 337001 & 8.5 & 
  negative\\
  \hline
  Martin\_Hess\_AfD & afd & 2018-08-17 07:15:00 & "Vor den ..." & 6574 & 3.5 &
  negative\\
  \hline
  BriHasselmann & gruene & 2019-09-25 15:35:00 & "Ich finde ..." & 20299 & 8.6 
  & positive\\
  \hline
  danielakolbe & spd & 2020-05-12 06:05:00 & "Aber verpflichtend ..." & 8158 & 
  8.3 & negative\\
  \hline
  JuergenBraunAfD & afd & 2020-08-13 22:05:00 & "Panik-Latif + ..." & 3188 & 
  3.4 & negative\\
\end{tabular}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data: distribution over time}

\begin{figure}[H]
  \includegraphics[width = \textwidth]{figures/obs_over_time}
  \caption{\raggedright tweet issuance over time}
\end{figure}

\vfill

Periodical fluctuations in the number of tweets over time and a general 
upward-sloping trend

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data: distribution across parties}

\begin{figure}[H]
  \includegraphics[width = \textwidth]{figures/obs_per_party}
  \caption{\raggedright tweet issuance across parties}
\end{figure}

\vfill

Observations per party in labeled training data (left) and entire scraped data 
example (right), both depicted against seat distribution in current parliament

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data pre-processing}

\begin{itemize}
  \item \textbf{Basic text cleaning:} transcription of German umlauts and 
  ligature s into standard-Latin characters and removal of non-informative 
  symbols
  \item \textbf{Twitter-specific preparation:} identification, separate storage 
  and subsequent removal of special characters (i.e., hashtags, emojis and user 
  tags)
\end{itemize}

\vfill

\fbox{\begin{minipage}{0.9\textwidth}
Wir gedenken Willy Brandt, der heute vor 28 Jahren, am 8. Oktober 1992, 
verstarb. Mit seinen Reformen in der Sozial-, Bildungs- und Rechtspolitik hat 
er innenpolitisch neue \highlight{Massstaebe} gesetzt. 
\highlight{Kniefall Friedensnobelpreis 
mehrdemokratiewagen spd willybrandt}
\end{minipage}}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data challenges}

\begin{itemize}
  \item \textbf{Language-specific:} many approaches predominantly 
  tailored to English
  \begin{itemize}
    \item[$\rightarrow$] possible complications with regards to German grammar 
    and syntax
  \end{itemize}
  \item \textbf{Twitter-specific:} limit of 280 characters; no explicit 
  mentioning of the event or topical entity the author is referring to; 
  informal language style
  \item \textbf{Context-specific:} requirement of domain knowledge within 
  political context (specific vocabulary); sarcasm and irony
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\subsection{Standard machine learning solution}
\label{ml}

% ------------------------------------------------------------------------------

\begin{frame}{Analytical concept}

\begin{minipage}[t]{0.5\textwidth}
  Conceptualization as analytical \textbf{pipeline}
  \vspace{0.5cm}
  \begin{itemize}
    \item[$\rightarrow$] Exchangeability of components 
    \item[$\rightarrow$] Usability as integrated object
    \item[$\rightarrow$] Preserving train-test dichotomy
    \item[$\rightarrow$] Seamlessly integrated in \texttt{mlr3}
  \end{itemize}
\end{minipage}%
\begin{minipage}[t]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.45\textwidth}
  \begin{figure}[H]
    \includegraphics[width = \textwidth]{figures/supervised_learning_schema}
    \caption{\citet{mlr3book}}
  \end{figure}
\end{minipage}%

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Feature extraction}

\begin{minipage}[t]{0.55\textwidth}
  We discern two stages of feature extraction:
  \vspace{0.5cm}
  \begin{enumerate}
    \item \textbf{Static features:} all quantities that may be derived from a 
    single observation
    \item \textbf{Dynamic features:} quantities that are computed across a range 
    of observations
  \end{enumerate}
\end{minipage}%
\begin{minipage}[t]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.4\textwidth}
  \begin{figure}[H]
    \includegraphics[width = \textwidth]{figures/schema_feat_extr}
    \caption{\raggedright own representation, published on
    \url{https://lisa-wm.github.io/nlp-twitter-r-bert/}}
  \end{figure}
\end{minipage}%

\vfill 
$\rightarrow$ \textbf{Difference important in resampling processes}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Static features}

\begin{itemize}
  \item \textbf{Lexicon-based polarity:} counts of positive / negative 
  terms and emojis
  \item \textbf{Twitter variables:} hashtags, retweets, \dots
  \item \textbf{Syntactic features:} intensification, negation
  \item \textbf{Character unigrams:} number of respective character occurrences
  \item \textbf{Part-of-speech (POS) tags:} number of adjectives, nouns, \dots
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Dynamic features}

\begin{minipage}[c]{0.6\textwidth}
  \textbf{Idea:} capture topical context by computing a set of word embeddings
  for each topic cluster
\end{minipage}%
\begin{minipage}[c]{0.15\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.25\textwidth}
  \includegraphics[width = 0.8\textwidth]{figures/blockmatrix_tm}
\end{minipage}%

\vspace{1cm}

\begin{itemize}
  \item Topic modeling with \highlight{structural topic model}
  \citep{robertsetal2013}
  \begin{itemize}
    \item[$\rightarrow$] Additional consideration of document-level meta 
    variables
  \end{itemize} 
  \item Embeddings with \highlight{GloVe} \citep{penningtonetal2014}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Structural topic model (STM)}

\begin{itemize}
  \item Generative model based on latent Dirichlet allocation (LDA, 
  \citet{bleietal2003})
  \item Recall: characterization of topics by individual topic-word 
  distributions
  \item Two key enhancements:
  \begin{itemize}
    \item Allowing for inter-topic \textbf{correlations}
    \item Incorporating document-level \textbf{meta data}, either as 
    \highlight{topical prevalence} formula or as \textbf{topical content} 
    variables
  \end{itemize}  
\end{itemize}

\vfill

\texttt{. $\sim$ party + bundesland + s(unemployment) + s(pop\_migration)}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Structural topic model (STM)}

\begin{enumerate}

  \item Draw non-normalized topic proportions 
  $\bm{\eta_d} \sim$ \highlight{$\mathcal{N}_{K - 1}(
  \bm{\Gamma}^T\bm{x}_d^T, \bm{\Sigma})$}.
  
  \item Normalize $\bm{\eta_d}$ through a softmax operation, 
  yielding $\bm{\theta}_d$ with $\theta_{d, k} = \frac{\exp(\eta_{d, k})}{
  \sum_{j = 1}^K \exp(\eta_{d, j})} \in [0, 1]$, $k \in \setk$. 
  
  \item For each word position $n \in \{ 1, 2, \dots, N_d \}$:
  
  \begin{itemize}
    \item[1] Draw $\bm{z}_{d, n} \sim \mathit{Multinomial}(\bm{\theta}_d)$ to 
    assign the $n$-th position to a topic. 
    \item[2] Draw a word $w_{d, n}$ from the word distribution corresponding to the 
    assigned topic: $w_{d, n} \sim \mathit{Multinomial}(\bm{\beta}(d, n))$.
  \end{itemize}
  
\end{enumerate}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Word embeddings}

\begin{itemize}
  \item \textbf{Goal:} model semantic importance of words in dense numeric 
  representation
  \item Also achieved by bag-of-words (BOW) approach, but with high 
  dimensionality
  \item Dimensionality reduction by embedding observations into low-dimensional 
  latent space
  \begin{itemize}
    \setlength{\itemsep}{-0.3em}
    \setlength{\parskip}{0.3em}
    \item Characterize words by their surrounding context
    \item Find latent dimensions
    \item Similar meaning = similar representation in the vector space
  \end{itemize}  
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Word embeddings}

\textbf{GloVe:} Global Vectors

% \vspace{0.3cm}

\begin{minipage}[t]{0.55\textwidth}
  \begin{itemize}
    \setlength{\itemsep}{-0.2em}
    \item Based on word co-occurrence matrix
    \item Neighborhood relations between words
    \item Defined via window size %(symmetric or asymmetric)
    \item Underlying assumption: stronger relationship between close-lying
    words
  \end{itemize}
\end{minipage}%
\begin{minipage}[t]{0.1\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.35\textwidth}
  \begin{figure}
    \raggedright
    \includegraphics[width = 0.9\textwidth]{figures/embedding_example}
  \caption{\raggedright \url{
  https://towardsdatascience.com/the-magic-behind-embedding-models-c3af62f71fb}}
  \end{figure}
\end{minipage}%
  
\vfill

\footnotesize
The \fbox{quick \textbf{brown} \highlight{fox} \textbf{jumps} over} the lazy dog.

\end{frame}

% ------------------------------------------------------------------------------

\subsection{Deep learning solution}
\label{dl}
