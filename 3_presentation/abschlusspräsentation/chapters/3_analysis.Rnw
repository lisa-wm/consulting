\subsection{Data}
\label{data}

% ------------------------------------------------------------------------------

\begin{frame}{Data collection: web scraping}

\textbf{Idea:} collect tweets by members of the German parliament 
(\textit{Bundestag}) issued after the last federal election in September 2017

\vfill

\begin{enumerate}
  \setlength{\itemsep}{-0.3em}
  \setlength{\parskip}{0.3em}
  \item Gather MPs' names and basic information from the official Bundestag 
  website
  \item Find Twitter account names
  \item Acquire socioeconomic information for the time of the last federal 
  election on a per-district level
  \item Scrape actual tweets along with some additional variables
\end{enumerate}

\vfill

$\rightarrow ~ $\textbf{Manual labeling process}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data collection: web scraping}

\begin{figure}
  \raggedright
  \includegraphics[width = 0.3\textwidth]{figures/screenshot_amthor}
\caption{\raggedright Example MP landing page. \textit{Source:} 
\url{https://www.bundestag.de/abgeordnete/}.}
\end{figure}

\begin{figure}
  \raggedright
  \includegraphics[width = 0.6\textwidth]{figures/screenshot_andi}
\caption{\raggedright Example tweet. \textit{Source:} 
\url{https://www.twitter.com/}.}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data labeling}

\begin{itemize}
  \item For each tweet: assign polarities \highlight{positive} or 
  \highlight{negative}, and also  \highlight{topic} descriptions required for 
  BERT's ABSA task
  \item Note: large number of tweets with no apparent sentiment, aspect 
  detection often difficult
  \item Class label distribution: \highlight{72\%} negative labels
\end{itemize}

\vfill

\tiny
  \begin{tabular}{l|l|l|l|r|r|l}
  \texttt{username} & \texttt{party} & \texttt{created\_at} & \texttt{text} & 
  \texttt{followers} & \texttt{unemployment\_rate} & \texttt{label}\\
  \hline
  karl\_lauterbach & spd & 2019-12-01 09:44:00 & "Die Wahl ..." & 337001 & 8.5 & 
  negative\\
  \hline
  Martin\_Hess\_AfD & afd & 2018-08-17 07:15:00 & "Vor den ..." & 6574 & 3.5 &
  negative\\
  \hline
  BriHasselmann & gruene & 2019-09-25 15:35:00 & "Ich finde ..." & 20299 & 8.6 
  & positive\\
  \hline
  danielakolbe & spd & 2020-05-12 06:05:00 & "Aber verpflichtend ..." & 8158 & 
  8.3 & negative\\
  \hline
  JuergenBraunAfD & afd & 2020-08-13 22:05:00 & "Panik-Latif + ..." & 3188 & 
  3.4 & negative\\
\end{tabular}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data distribution over time}

\begin{figure}[H]
  \includegraphics[width = \textwidth]{figures/obs_over_time}
  \caption{\raggedright Tweet issuance over time.}
\end{figure}

\vfill

Periodical fluctuations in the number of tweets over time and a general 
upward-sloping trend

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data distribution across parties}

\begin{figure}[H]
  \includegraphics[width = \textwidth]{figures/obs_per_party}
  \caption{\raggedright Tweet issuance across parties.}
\end{figure}

\vfill

Observations per party in labeled training data (left) and entire scraped data 
example (right), both depicted against seat distribution in current parliament

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data pre-processing}

\begin{itemize}
  \item \textbf{Basic text cleaning:} transcription of German umlauts and 
  ligature s into standard-Latin characters and removal of non-informative 
  symbols
  \item \textbf{Twitter-specific preparation:} identification, separate storage 
  and subsequent removal of special characters (i.e., hashtags, emojis and user 
  tags)
\end{itemize}

\vfill

\fbox{\begin{minipage}{0.9\textwidth}
Wir gedenken Willy Brandt, der heute vor 28 Jahren, am 8. Oktober 1992, 
verstarb. Mit seinen Reformen in der Sozial-, Bildungs- und Rechtspolitik hat 
er innenpolitisch neue \highlight{Massstaebe} gesetzt. 
\highlight{Kniefall Friedensnobelpreis 
mehrdemokratiewagen spd willybrandt}
\end{minipage}}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Data challenges}

\begin{itemize}
  \item \textbf{Language-specific:} many approaches predominantly 
  tailored to English
  \begin{itemize}
    \item[$\rightarrow$] possible complications with regards to German grammar 
    and syntax
  \end{itemize}
  \item \textbf{Twitter-specific:} limit of 280 characters; no explicit 
  mentioning of the event or topical entity the author is referring to; 
  informal language style
  \item \textbf{Context-specific:} requirement of domain knowledge within 
  political context (specific vocabulary); sarcasm and irony
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\subsection{Standard machine learning solution}
\label{ml}

% ------------------------------------------------------------------------------

\begin{frame}{Analytical concept}

\begin{minipage}[t]{0.5\textwidth}
  Conceptualization as analytical \textbf{pipeline}
  \vspace{0.5cm}
  \begin{itemize}
    \item[$\rightarrow$] Exchangeability of components 
    \item[$\rightarrow$] Usability as integrated object
    \item[$\rightarrow$] Preserving train-test dichotomy
    \item[$\rightarrow$] Seamlessly integrated in \texttt{mlr3}
  \end{itemize}
\end{minipage}%
\begin{minipage}[t]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.45\textwidth}
  \begin{figure}[H]
    \includegraphics[width = \textwidth]{figures/supervised_learning_schema}
    \caption{Schematic process of supervised learning. \textit{Source:} 
    \citet{mlr3book}.}
  \end{figure}
\end{minipage}%

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Feature extraction}

\begin{minipage}[t]{0.55\textwidth}
  We discern two stages of feature extraction:
  \vspace{0.5cm}
  \begin{enumerate}
    \item \textbf{Static features:} all quantities that may be derived from a 
    single observation
    \item \textbf{Dynamic features:} quantities that are computed across a range 
    of observations
  \end{enumerate}
\end{minipage}%
\begin{minipage}[t]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.4\textwidth}
  \begin{figure}[H]
    \includegraphics[width = \textwidth]{figures/schema_feat_extr}
    \caption{\raggedright Feature extraction process. \textit{Source:} 
    own representation.}
  \end{figure}
\end{minipage}%

\vfill 
$\rightarrow$ \textbf{Difference important in resampling processes}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Static features}

\begin{itemize}
  \item \textbf{Lexicon-based polarity:} counts of positive / negative 
  terms and emojis
  \item \textbf{Twitter variables:} hashtags, retweets, \dots
  \item \textbf{Syntactic features:} intensification, negation
  \item \textbf{Character unigrams:} number of respective character occurrences
  \item \textbf{Part-of-speech (POS) tags:} number of adjectives, nouns, \dots
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Dynamic features}

\begin{minipage}[c]{0.6\textwidth}
  \textbf{Idea:} capture topical context by computing a set of word embeddings
  for each topic cluster
\end{minipage}%
\begin{minipage}[c]{0.15\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.25\textwidth}
  \includegraphics[width = 0.8\textwidth]{figures/blockmatrix_tm}
\end{minipage}%

\vspace{1cm}

\begin{itemize}
  \item Topic modeling with \highlight{structural topic model}
  \citep{robertsetal2013}
  \begin{itemize}
    \item[$\rightarrow$] Additional consideration of document-level meta 
    variables
    \item[$\rightarrow$] Drawback: pooling tweets into larger pseudo-documents
  \end{itemize} 
  \item Embeddings with \highlight{GloVe} \citep{penningtonetal2014}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Structural topic model (STM)}

\begin{itemize}
  \item Generative model based on latent Dirichlet allocation (LDA, 
  \citet{bleietal2003})
  \item Recall: characterization of topics by individual topic-word 
  distributions
  \item Two key enhancements:
  \begin{itemize}
    \item Allowing for inter-topic \textbf{correlations}
    \item Incorporating document-level \textbf{meta data}, either as 
    \highlight{topical prevalence} formula or as \textbf{topical content} 
    variables
  \end{itemize}  
\end{itemize}

\vfill

\texttt{. $\sim$ party + bundesland + s(unemployment) + s(pop\_migration)}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Structural topic model (STM)}

\begin{enumerate}

  \item Draw non-normalized topic proportions 
  $\bm{\eta_d} \sim$ \highlight{$\mathcal{N}_{K - 1}(
  \bm{\Gamma}^T\bm{x}_d^T, \bm{\Sigma})$}.
  
  \item Normalize $\bm{\eta_d}$ through a softmax operation, 
  yielding $\bm{\theta}_d$ with $\theta_{d, k} = \frac{\exp(\eta_{d, k})}{
  \sum_{j = 1}^K \exp(\eta_{d, j})} \in [0, 1]$, $k \in \setk$. 
  
  \item For each word position $n \in \{ 1, 2, \dots, N_d \}$:
  
  \begin{itemize}
    \item[1] Draw $\bm{z}_{d, n} \sim \mathit{Multinomial}(\bm{\theta}_d)$ to 
    assign the $n$-th position to a topic. 
    \item[2] Draw a word $w_{d, n}$ from the word distribution corresponding to the 
    assigned topic: $w_{d, n} \sim \mathit{Multinomial}(\bm{\beta}(d, n))$.
  \end{itemize}
  
\end{enumerate}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Word embeddings}

\begin{itemize}
  \item \textbf{Goal:} model semantic importance of words in dense numeric 
  representation
  \item Also achieved by bag-of-words (BOW) approach, but with high 
  dimensionality
  \item Dimensionality reduction by embedding observations into low-dimensional 
  latent space
  \begin{itemize}
    \setlength{\itemsep}{-0.3em}
    \setlength{\parskip}{0.3em}
    \item Characterize words by their surrounding context
    \item Find latent dimensions
    \item Similar meaning = similar representation in the vector space
  \end{itemize}  
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Word embeddings}

\textbf{GloVe:} Global Vectors

\begin{minipage}[t]{0.55\textwidth}
  \begin{itemize}
    \setlength{\itemsep}{-0.2em}
    \item Based on word co-occurrence matrix
    \item Neighborhood relations between words
    \item Defined via window size %(symmetric or asymmetric)
    \item Underlying assumption: stronger relationship between close-lying
    words
  \end{itemize}
\end{minipage}%
\begin{minipage}[t]{0.1\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.35\textwidth}
  \begin{figure}
    \raggedright
    \includegraphics[width = 0.9\textwidth]{figures/embedding_example}
  \caption{\raggedright Exemplary visualization of embedding space. 
  \textit{Source:} \url{
  https://towardsdatascience.com/the-magic-behind-embedding-models-c3af62f71fb}}
  \end{figure}
\end{minipage}%
  
\vfill

\footnotesize
The \fbox{quick \textbf{brown} \highlight{fox} \textbf{jumps} over} the lazy dog.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{AutoML pipeline}

Implementation as \texttt{mlr3} \textbf{graph learner}

\begin{minipage}[t]{0.65\textwidth}
  \begin{itemize}
    \setlength{\itemsep}{-0.2em}
    \item Input: text + static features
    \item Computation of topic-specific embeddings 
    \item Choice between random forest and logistic regression with elastic net 
    penalty
    \item Tuning over associated configuration spaces
  \end{itemize}
\end{minipage}%
\begin{minipage}[t]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.3\textwidth}
  \begin{figure}
    \raggedright
    \includegraphics[width = \textwidth]{figures/graph}
    \caption{Graph learner. \textit{Source:} own representation.}
  \end{figure}
\end{minipage}%

\vfill

$\rightarrow$ \textbf{Train, predict, resample, tune, benchmark}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Results}

\begin{table}[H]
  \footnotesize
  \begin{tabular}{l|r|r|r|}
    \cline{2-4}
    &
    \multicolumn{1}{l|}{\textbf{learner with topic modeling}} &
    \multicolumn{1}{l|}{\textbf{learner without topic modeling}} &
    \multicolumn{1}{l|}{\textbf{featureless learner}} \\ 
    \hline
    \multicolumn{1}{|l|}{accuracy} & 0.715 & \cellcolor{lightgray} 
    \textbf{0.859} & 0.724 \\
    \hline
    \multicolumn{1}{|l|}{F1 score} & 0.023 & \cellcolor{lightgray} 
    \textbf{0.706} & - \\
    \hline
    \multicolumn{1}{|l|}{TN} & 288.333 & \cellcolor{lightgray} 
    279.667 & \textbf{293.333} \\
    \hline
    \multicolumn{1}{|l|}{TP} & 1.333 & \cellcolor{lightgray} \textbf{68.333} 
    & 0.000 \\
    \hline
    \multicolumn{1}{|l|}{FN} & 110.333 & \cellcolor{lightgray} 
    \textbf{43.333} & 111.667 \\
    \hline
    \multicolumn{1}{|l|}{FP} & 5.000 & \cellcolor{lightgray} 13.667 & 
    \textbf{0.000} \\
    \hline
  \end{tabular}
  \caption{\raggedright Results for standard ML approach.}
  \label{tab_res_ml}
\end{table}

\end{frame}

% ------------------------------------------------------------------------------

\subsection{Deep learning solution}
\label{dl}

% ------------------------------------------------------------------------------

\begin{frame}{Deep transfer learning with BERT}

\highlight{B}i-directional \highlight{E}ncoder \highlight{R}epresentation 
from \highlight{T}ransformers \citep{devlinetal2018}

\vfill 

\textbf{Transfer learning}

\begin{itemize}
  \setlength{\itemsep}{-0.3em}
  \setlength{\parskip}{0.3em}
  \item Problem: generalization ability no longer reliable when train \& 
  prediction data do not follow the same distribution
  (few labels, domain shift)
  \item Idea: first train a model on an original task and domain, then 
  \textbf{transfer} knowledge to target task and domain 
  \item Allowing for use of \textbf{pre-trained} models, e.g, provided by 
  \texttt{huggingface} Transformers library, that are then \textbf{fine-tuned} 
  to a specific task
\end{itemize}

\vfill 

\textbf{Attention mechanism}

\begin{itemize}
  \setlength{\itemsep}{-0.3em}
  \setlength{\parskip}{0.3em}
  \item Avoid processing textual data sequentially 
  \item Allow for more parallelization and access to all hidden network states
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Input pre-processing}

\begin{figure}
  \raggedright
  \includegraphics[width = 0.7\textwidth, trim = 0 30 30 0, clip]{
  figures/bert_tokens}
  \caption{\raggedright BERT pre-processing. \textit{Source:} 
  \citet{devlinetal2018}.}
\end{figure}

\vfill

\begin{itemize}
  \setlength{\itemsep}{-0.3em}
  \setlength{\parskip}{0.3em}
  \item Token embeddings from model-specific tokenization
  \item Segment embeddings: 0 for A and 1 for B
  \item Position embeddings indicating the position of each token in the 
  sentence
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Pre-training}

\textbf{Idea:} self-supervised training on large corpora without need for labels

\vfill

Task 1: \textbf{masked language modeling (MLM)}
\\

$\rightarrow$ mask words and have BERT predict them without considering 
positioning

\vfill

\fbox{\begin{minipage}{0.9\textwidth}
\raggedright
[CLS] Die Ausgrenzung von \highlight{[MASK]} von der \#EssenerTafel 
ist inakzeptabel und \highlight{[MASK]}. [SEP]
Wir dürfen nicht zulassen, dass die \highlight{[MASK]} gegeneinander 
ausgespielt werden. [SEP]
\end{minipage}}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Pre-training}

Task 2: \textbf{next sentence prediction}
\\

$\rightarrow$ predict if the second sentence in a pair is the 
subsequent one in the original

\vfill

\fbox{\begin{minipage}{0.9\textwidth}
\raggedright
\textbf{Sentence A:} [CLS] Die Ausgrenzung von \highlight{[MASK]} von der 
\#EssenerTafel 
ist inakzeptabel und \highlight{[MASK]}. [SEP]
\\
\textbf{Sentence B:} Wir dürfen nicht zulassen, dass die \highlight{[MASK]} 
gegeneinander 
ausgespielt werden. [SEP]
\\
\textbf{Label:} IsNextSentence
\end{minipage}}

\vfill

\fbox{\begin{minipage}{0.9\textwidth}
\raggedright
\textbf{Sentence A:} [CLS] Die Ausgrenzung von \highlight{[MASK]} von der 
\#EssenerTafel 
ist inakzeptabel und \highlight{[MASK]}. [SEP]
\\
\textbf{Sentence B:} Freue mich sehr für ihn und auf die Zusammenarbeit. [SEP]
\\
\textbf{Label:} IsNextSentence
\end{minipage}}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Fine-tuning}

\textbf{Goal:} adapting BERT to task at hand

\vfill

\begin{minipage}[t]{0.55\textwidth}
  \begin{itemize}
    \setlength{\itemsep}{-0.2em}
    \item Initialization with pre-trained weights
    \item Replacing final layers from MLM \& NSP with classification layer
    \item Training with cross-entropy loss
    \item Task in this case:\\ \textbf{sequence classification}
  \end{itemize}
\end{minipage}%
\begin{minipage}[t]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.4\textwidth}
  \begin{figure}
    \raggedright
    \includegraphics[width = 0.9\textwidth, trim = 0 30 0 0, clip]{
    figures/bert_finetuning}
    \caption{\raggedright Example for BERT fine-tuning process. 
    \textit{Source:} \citet{devlinetal2018}.}
  \end{figure}
\end{minipage}%

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ABSA}

\begin{itemize}
  \item \textbf{Post-training}
  \begin{itemize}
    \item Further development of the basis-model
    \item \texttt{bert-base-german-cased}: pre-trained on German Wikipedia 
    texts, news articles and Open Legal Datasets of German court decisions and 
    citations
    \item Leverage both MLM and NSP with GermEval and pool of unlabeled tweets 
    in order to adapt the specific domain language to the model
  \end{itemize}  
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{ABSA}

\begin{itemize}
  \item \textbf{Aspect extraction}
  \begin{itemize}
    \item Idea: use supervised learning to label each token from a sequence
    with one of these three labels: \\
    \vspace{1em}
    \begin{tabular}{cl}
      \highlight{B} & beginning of an aspect\\
      \highlight{I} & inside of an aspect \\
      \highlight{O} & outside of an aspect
    \end{tabular}
    \item Requirement: exhaustive domain knowledge
  \end{itemize}  
  \item \textbf{Aspect sentiment classification:} classify polarity of given 
  text, taking into account the given aspects as an extra feature
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Results}

\begin{table}[H]
  \footnotesize
  \begin{tabular}{l|r|r|r|r|r|r|r|r|}
    \cline{2-9}
    & \multicolumn{4}{c|}{\textbf{ABSA}} & \multicolumn{4}{c|}{\textbf{SA}} 
    \\ \cline{2-9} 
    &
    \multicolumn{1}{r|}{\textbf{GC}} &
    \multicolumn{1}{r|}{\textbf{GC-G}} &
    \multicolumn{1}{r|}{\textbf{GC-T}} &
    \multicolumn{1}{r|}{\textbf{GCD}} &
    \multicolumn{1}{r|}{\textbf{GC}} &
    \multicolumn{1}{r|}{\textbf{GC-G*}} &
    \multicolumn{1}{r|}{\textbf{GC-T*}} &
    \multicolumn{1}{r|}{\textbf{GCD*}} \\ 
    \hline
    \multicolumn{1}{|l|}{accuracy} & 0.893 & 0.905 & \cellcolor{lightgray} 
    \textbf{0.918} & 0.889 
    & 0.889 & 0.901 & 0.905 & \cellcolor{lightgray} \textbf{0.926} \\ 
    \hline
    \multicolumn{1}{|l|}{F1 score} & 0.803 & 0.816 & \cellcolor{lightgray} 
    \textbf{0.851} & 0.791 
    & 0.794 & 0.821 & 0.827 & \cellcolor{lightgray} \textbf{0.864} \\ 
    \hline
    \multicolumn{1}{|l|}{TN} & 164.000 & \textbf{169.000} & 
    \cellcolor{lightgray} 166.000 & 165.000 & 
    164.000 & 164.000 & 165.000 & \cellcolor{lightgray} \textbf{168.000} \\ 
    \hline
    \multicolumn{1}{|l|}{TP} & 53.000 & 51.000 & \cellcolor{lightgray} 
    \textbf{57.000} & 51.000 & 
    52.000 & 55.000 & 55.000 & \cellcolor{lightgray} \textbf{57.000} \\ 
    \hline
    \multicolumn{1}{|l|}{FN} & 14.000 & 16.000 & \cellcolor{lightgray} 
    \textbf{10.000} & 16.000 & 
    15.000 & 12.000 & 12.000 & \cellcolor{lightgray} \textbf{10.000} \\ 
    \hline
    \multicolumn{1}{|l|}{FP} & 12.000 & \textbf{7.000} & \cellcolor{lightgray} 
    10.000 & 11.000 & 
    12.000 & 12.000 & 11.000 & \cellcolor{lightgray} \textbf{8.000} \\ 
    \hline
    \end{tabular}
  \caption{
  BERT results (asterisks indicate additional fine-tuning with GermEval data) \\
  \scriptsize  
  GC = \texttt{bert-base-german-cased}, \\
  GC-G = \texttt{bert-base-german-cased} post-trained with GermEval data \\
  GC-T = \texttt{bert-base-german-cased} post-trained with scraped but unlabeled 
  tweets, and 
  \\
  GCD = \texttt{bert-base-german-dbmdz-cased}.\\
  }
  \label{tab_res_bert}
\end{table}
\end{frame}