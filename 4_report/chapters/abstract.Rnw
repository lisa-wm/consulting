\textbf{Project outline}

\begin{itemize}
  \item \textbf{Problem description.} For Twitter data of German MPs, enable 
  automated textual analysis by topic-specific sentiment analysis
  \item \textbf{Key assumptions.} 
  \begin{itemize}
    \item Sentiment analysis is much stronger if topical context is taken into 
    account -- while some words, such as "excellent", carry a globally 
    applicable sentiment, others may convey quite different meanings in varying 
    context (see, for example, \citet{thelwallbuckley2013}).
    \item We will assume that one tweet refers to one (principal) topic and 
    carries a distinct sentiment towards this topic. This is induced by the 
    observation that politicians will mostly tweet ad-hoc in response to a 
    certain event, not as an overall statement of their stance toward some 
    issue.
  \end{itemize}
  \item \textbf{Relation to other problems of NLP.} We refer to the problem 
  treated herein as topic-specific sentiment analysis (TSSA). 
  TSSA overlaps with other well-known NLP problems:
  \begin{itemize}
    \item \textbf{Sentiment analysis (SA).} TSSA may be interpreted as a 
    methodological variant, taking into account topical context that is lost on 
    standard SA.
    \item \textbf{Topic modeling (TM).} TM retrieves topics without sentiment 
    classification and is thus subsumed under TSSA.
    \item \textbf{Aspect-based sentiment analysis (ABSA).} TSSA is closely 
    related to ABSA.
    In particular, what we call topics, exhibiting several aspects, could be 
    identified with entities from ABSA.
    However, we observe that problems and solutions framed in ABSA often take 
    a slightly different focus, and sometimes employ approaches that deviate 
    from the idea we pursue here.
    Therefore, we deliberately introduce this distinction to emphasize the 
    difference in intuitions we perceive as follows:
    \begin{itemize}
      \item ABSA is frequently discussed in the context of product reviews and 
      thus mostly concerned with a specific type of document: the topic treated 
      is quite clear (reviews are typically posted on a product detail page or 
      similar dedicated location), and the document always carries sentiment 
      (sometimes, even with an explicit rating attached).
      \item Whether or not such a prior sentiment information is available, ABSA 
      is interested in the particular aspects that constitute overall sentiment 
      (which all convey polarity, possibly of conflicting nature). 
      These aspects are typically assembled in higher-level entities. 
      \item A major task in ABSA is therefore the decomposition of the documents 
      into single-aspect fragments, which may be interpreted as a reshaping of 
      the corpus with a modified definition of the smallest-element level.
      \item Once this is achieved, SA is performed on each single-aspect 
      fragment.
      \item Our setting, on the other hand, deals with a different kind of 
      document. As tweets are publicly posted without explicit connection to a 
      specific target, we will usually not know what a document is about, 
      much less its polarity -- many documents will not even convey sentiment.
      \item Also, as mentioned above, tweets can be expected to have a narrow 
      focus on a single topical issue.
      \item We identify three potential sub-tasks of TSSA that relate to ABSA in 
      varying degree: 
      \begin{itemize}
        \item \textbf{Stand-alone TM.} We might be only interested in extracting 
        topics from a set of tweets, which is equivalent to the ABSA-sub-task 
        of aspect extraction (i.e., finding entity-aspect pairs), albeit on 
        document level only. If the goal is to label tweets according to 
        pre-specified topics, we are more in the realm of multi-label 
        classification.
        \item \textbf{Stand-alone SA.} We might also be purely interested in SA 
        without relation to single aspects (e.g., because the data were 
        collected in such a fashion that they are related to a very specific 
        topic from the beginning, or because we wish to analyze general polarity 
        of tweets by a certain source).
        Here, we emphasize in particular the topic orientation of TSSA: we still 
        want to leverage latent (sub-)topics because we believe it will aid 
        classification by providing contextual information.
        Rather than being an explicit target of sentiments, topics and aspects 
        are indicators of contextual clusters then.
        \item \textbf{TSSA.} This is indeed a form of ABSA with the specific 
        assumption that one document carries exactly one aspect and one 
        associated sentiment (topic is then synonym to entity).
        We underline that topics and aspects do not serve as mere
        document separators, but are viewed as indicative for different contexts 
        that need to be accounted for.
        By this, we differentiate with respect to approaches that only 
        reshape corpora according to aspects and then perform topic-agnostic 
        SA\footnote{
        E.g., \citet{sivakumarreddy2017}, \citet{zhaoetal2016}, 
        \citet{renhong2017} first assign aspects, but then apply 
        standard classifiers / global dictionaries.
        }.
      \end{itemize}
      \item Summing up the above, our approach incorporates all general ideas 
      present in ABSA, but does so with a slightly different focus, and special 
      emphasis on topical context.
    \end{itemize}
    
  \end{itemize}
  \item \textbf{Solutions.} We offer two different approaches to tackle this 
  problem:
  \begin{enumerate}
    \item Methods of standard machine learning, more precisely, a combination of 
    lexicon-based techniques and off-the-shelf classifiers \\
    $\rightarrow$ Accessible to researchers without strong data science 
    background and/or large computational resources
    \item Methods of deep learning, more precisely, a variant of BERT \\
    $\rightarrow$ Powerful tool for more advanced researchers
  \end{enumerate}
  % \end{itemize}
\end{itemize}

\textbf{Part I: proposed pipeline}

\begin{itemize}
  \item Deep learning methods, BERT in particular, are capable of actually 
  learning the intrinsic structures constituting language (grammatical patterns, 
  inter-term relations), and can draw from this knowledge to perform downstream 
  tasks such as sentiment analysis.
  \item By contrast, lexicon-based and machine learning approaches are confined 
  to learning from features derived from the bag-of-words assumption\footnote{
  Well, you could probably hand-craft grammatical features, but this would 
  become incredibly complicated.
  }. This makes the handling of phenomena like negations much harder.
  \item Nonetheless, both are widely applied in SA tasks.
  \item Lexicon-based approaches identify sentiment-bearing words and assign 
  them polarity scores, an aggregation of which on document level then yields 
  document polarity. In this, they are able to work fully unsupervised.
  \item ML approaches make use of a variety of features (e.g., unigrams), and 
  learn their association to the target from labeled training instances. 
  Typically, a combination of three types of features is used:
  \begin{itemize}
    \item Document-level lexical features (unigrams, number of exclamation 
    marks, ...)
    \item Lexicon-based features (number of positive-polarity words, ...)
    \item Word embeddings (learned from large, possibly corpus-external 
    feature-co-occurrence matrices)
  \end{itemize}
  \item For the incorporation of topical context, we choose to take a sequential 
  approach, where topics are assigned first, and then SA is performed. 
  \item While there exist successful implementations of joint topic-sentiment 
  modeling (and theoretical arguments are strong, particularly so if inference 
  is relevant), we decide against a simultaneous analysis for the following 
  reasons:
  \begin{itemize}
    \item This first part addresses researchers from other disciplines, so 
    methods should be fairly easy to implement and explain. 
    Joint models, however, are usually rather complicated, hierarchical Bayesian 
    architectures.
    \item As outlined above, we would like to be able to tackle sub-tasks 
    individually.
    \item A likely scenario entails TSSA for topics that are pre-specified based 
    on a specific research interest (given in form of keywords, for instance). 
    It is unclear how such a setting would be handled with joint models.
  \end{itemize}
  \item However, it is not clear whether we should use a single classifier 
  after topic labeling or one classifier per topic.
  \begin{itemize}
    \item Single classifier
    \begin{itemize}
      \item Pro: lean architecture; topic-agnostic features need to be learned 
      only once; single-best classifier is easily found and tuned
      \item Con: topic-specific features, such as word embeddings, are hard to 
      incorporate 
      \item Examples: single global lexicon with polarity as 
      topic-prevalence-weighted average across sentiment-bearing words 
      \citep{naskaretal2016}; aspect as single categorical variable in otherwise 
      global feature set (baseline of Sem-Eval 2016 ABSA task, 
      \citet{pontikietal2016})
    \end {itemize}
    \item Per-topic classifier
    \begin{itemize}
      \item Pro: easy handling of topic-specific features
      \item Con: many models in parallel; topic-agnostic features are learned 
      from smaller subsets; unclear which classifier is best 
      (single best, best for each sub-task?)
      \item Examples: domain-specific lexica and purely lexicon-based 
      \citep{thelwallbuckley2013}; domain-specific lexica and per-domain ML 
      classifiers \citep{choietal2009}; standard features and per-topic ML 
      classifiers \citep{ficamosliu2016}; per-topic word embeddings and LSTM 
      \citep{jebbaracimiano2016}
    \end{itemize}
  \end{itemize}
  \item Proposition: use single classifier with lexical features and embeddings 
  derived from topic-agnostic features, plus polarity counts from topic-specific 
  lexica (topic-agnostic meaning those that are selected into the topic lexica, 
  or at least those that are selected with different polarities)
\end{itemize}

\begin{center}
  \includegraphics[width = \textwidth]{figures/sketch-part-I-pipeline}
\end{center}