\subsection{Terminology}
\label{term}

In this section we briefly review general theoretical concepts; the actual
methods we employ are described in chapter \ref{analysis}.
Throughout the report we will make use of the following terminology:
\\

\textbf{Word.} Words $w$ are sequences of characters and represent the smallest 
unit of text we consider. 
% \\

\textbf{Vocabulary.} The aggregate of unique terms present in a collection of 
text constitutes a vocabulary of length $V \in \N$ from which a one-hot encoding 
for words can be derived: for the $v$-th instance of the vocabulary, 
$v \in \setv$, this is a length-$V$ vector with all but the $v$-th entry, which 
is one, equaling zero.
Note that pre-processing (discussed in chapter \ref{analysis}) might result in a
vocabulary that is smaller than the total number of distinct words occurring 
across all texts.
% \\

\textbf{Document.} Documents $d \in \setd$, $D \in \N$, are generally understood 
to be sequences of $N_d \in \N$ words, and, in our case, tweets.
% \\

\textbf{Corpus.} Lastly, the set of all $D$ documents considered makes up a 
corpus.

% ------------------------------------------------------------------------------

\subsection{Theoretical Concepts}
\label{concepts}

% ------------------------------------------------------------------------------

\subsubsection{Topic Modeling}
\label{tm}

\textbf{Idea.}
Recall that the ultimate goal is the classification of tweets into groups 
signaling a specific sentiment.
It is reasonable to assume that sentiment, and the way of expressing it, is 
susceptible to context, which suggests potential gains from clustering 
tweets prior to sentiment analysis (see, for example, \citet{ficamosliu2016},
\citet{bhatiap2018}, or \citet{jangetal2021}).

Grouping texts into semantic clusters, or topics, is generally referred to as 
\textit{topic modeling} and typically an unsupervised learning task.
The idea is to uncover latent structures in a corpus along which documents can 
be characterized.
Topic modeling is essentially a means of dimensionality reduction: text analysis
requires text to be cast to numerical representation, the 
simplest form of which is to represent documents by counts of vocabulary 
instances.
The dimension of the resulting document-term matrix increases exponentially in 
the number of documents and words contained in them, making an urgent case for 
compressing this dimensionality \citep{vayanskykumar2020}.

Topic modeling results in two types of output: one that links words with their 
propensity of occurring within a topic $k \in \setk$, $K \in \N$, and one 
stating the extent to which documents discuss each topic.
This projection of texts into a $K$-dimensional latent space ($K \ll V$) 
is a purely mathematical operation and the assessment of interpretability is up 
to human judgment.
Usually the resulting topics are then examined with respect to their most 
characteristic terms, according to an appropriate measure, in the attempt to 
find a meaningful description.
In particular, $K$ is a hyperparameter that must be specified a priori
\citep{aggarwal2018}.
\\

\textbf{Approaches.}
Topic modeling approaches roughly decompose into deterministic and 
probabilistic, or generative, approaches.
The former are based on factorizing the document-term matrix 
$M \in \R^{D \times V}$ (or a weighted version that takes into account prior 
probabilities of term occurrence) into two low-rank matrices, 
$U \in \R^{D \times K}$ and $W^T \in \R^{K \times V}$, whose product 
approximates $M$ loss-minimally.
Probably the most prominent methods from this category are \textit{latent 
semantic analysis}, which performs singular value decomposition and thus 
projects the data into a subspace spanned by $M$'s principal eigenvectors, and
\textit{non-negative matrix factorization}, a constrained version that often 
yields better interpretability \citep{aggarwal2018}.

Non-probabilistic models suffer from limitations in inference and 
out-of-sample extension, which is why generative approaches, addressing these 
issues, have become widely popular.
Generative models hail from the Bayesian paradigm.
Loosely speaking, they seek to reverse-engineer the imaginative process of 
document generation: first, for each document $d$ in a 
corpus we draw a length-$K$ vector of topic proportions from some distribution; 
then assign each word position in $\{1, 2, \dots, N_d\}$ to a topic 
with probabilities according to the sampled topic proportions, and then draw a 
word from the distribution associated with this topic \citep{vayanskykumar2020}.
\textit{Latent Dirichlet allocation (LDA)} by \citet{bleietal2003}, employing
Dirichlet and multinomial distributions, pioneered this approach to topic 
modeling.
We revisit LDA in section \ref{tssa_ml_method} as it also provides the 
foundation for the STM.

% ------------------------------------------------------------------------------

\subsubsection{Sentiment Analysis}
\label{sa}

\textbf{Idea.}
Sentiment analysis refers to the process of establishing the emotive nature 
of the opinion an author expresses in their text.
The types of sentiment that are of interest vary across applications and may be 
arbitrarily fine-grained.
We focus on the simplest form and attempt to determine 
whether tweets convey positive or negative sentiment, also called 
\textit{polarities}.
In this, sentiment analysis is a standard classification problem
\citep{medhatetal2014}.

Sentiment analysis occurs at different levels of a document.
Depending on its scope we can broadly discern document-level, sentence-level and 
aspect-level sentiment analysis.
The document and sentence level techniques are not fundamentally distinct; 
rather, document-level analysis follows after sentence-level analysis (it is 
also possible to drill down even further) and aggregates the sentiments 
expressed by sentences for the entire document.
Aspect-based sentiment analysis (ABSA) has a slightly different angle.
It studies sentiment regarding aspects of topical entities (for instance, 
\textit{carbon taxes} as an aspect of \textit{environmental policy}), meaning
sentiment targets are identified with respect to contents and not merely by 
sentence delimiters.
This requires solving the sub-tasks of aspect extraction and aspect sentiment 
classification \citep{aggarwal2018}.
ABSA therefore has connections to topic modeling.
We will discuss our view on the relation of topic modeling, sentiment analysis 
and ABSA in the subsequent section.
\\

\textbf{Approaches.}
There are two principal ways of approaching sentiment analysis.
The first is rule-based and avoids statistical modeling altogether, instead 
classifying documents by summing the number of terms associated with each 
sentiment and assigning the class with the highest count.
Prior polarities are typically taken from large dictionaries 
\citep{sidarenka2019}.
In our binary task this corresponds to identifying whether tweets contain more 
words with positive or negative connotation.
We do incorporate this type of analysis but use the respective numbers of 
positive- and negative-polarity terms as an intermediate input rather than 
the sole grounds for classification.

Instead, we perceive sentiment analysis as a classic instance of supervised ML.
Under the assumption that the data can be categorized into $g \in \N$ discrete 
classes we predict for each document its associated class from a set of 
features. 
More formally, we find a model $f: \mathcal{X} \rightarrow \R^g$, 
$\mathcal{X} \subseteq \R^p$ for $p \in \N$, that maps from the space of input 
features into $g$-dimensional Euclidean space.
Each observation is assigned a vector of continuous class scores or 
probabilities (depending on the classifier).
This mapping must be learned from a set of labeled training data, which marks a 
fundamental difference to the topic modeling task.
The actual class labels $y \in \mathcal{Y}$ are then found by 
thresholding or an $\mathit{argmax}$ operation on the score/probability vectors
\citep{bishop2006}.
In our case the set of labels, with $g = 2$, is typically encoded as 
$\mathcal{Y} = \{0, 1\}$ or $\mathcal{Y} = \{-1, 1\}$.

% As mentioned above, casting textual data into a design matrix of numeric 
% features must be handled with care.
Once the data are available in appropriate form, we can, in principle, use any 
type of learner suited to classification.
We specifically consider random forests and regularized logistic regression for 
the standard ML solution and turn BERT into a classifier by 
fine-tuning it to sentiment analysis; details are given in section 
\ref{analysis}.

% ------------------------------------------------------------------------------

\subsubsection{Topic-Specific Sentiment Analysis}
\label{tssa}

With the concepts of topic modeling and sentiment analysis we define a combined 
task we call \textit{topic-specific sentiment analysis}. 
There are various ways in which such a combination is conceivable and we 
ourselves pursue different approaches, which is why we use TSSA as an 
overarching term.

We see two types of solution to the TSSA problem: either perform topic 
extraction and sentiment analysis jointly, or regard it as a cascading task 
where sentiment analysis is subsequent to topic modeling.
Most publications to date share the second view; recent work, however, has
highlighted the advantages of joint modeling 
(e.g., \citet{nguyenetal2018}, \citet{tianetal2021}, \citet{wangetal2021}).
While theoretical arguments are strong, we decide against simultaneous 
methods mostly due to their complexity which is at odds with 
our goal of making text analysis accessible to a broader audience.
Also we would like to retain the option of performing both tasks in a 
stand-alone fashion.
The latter has become all the more important in the course of our study as we 
experience some difficulties in modeling topics for tweets.

We make the general assumption of each document pertaining to exactly one topic.
This seems plausible in the light of tweets' brevity (Twitter currently allows 
for 280 characters, up from 140 originally) and is supported by other research 
(\citet{zuoetal2016}, \citet{quiangetal2019}).
We then explore different options in our proposal.
For the standard ML classification we use topic modeling, 
somewhat implicitly, as a means of feature generation for sentiment analysis.
The clusters resulting from grouping tweets by topic serve as environments for 
computing topic-specific embeddings that are afterwards fed to the classifier.
In the DL solution we experiment with sentiments that directly refer 
to aspects by fine-tuning BERT to an ABSA task.
However, we observe in both approaches that the topical component does not aid 
the sentiment classification task and even tends to worsen results.
We will discuss this finding in section \ref{discussion}.
