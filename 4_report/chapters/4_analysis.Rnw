\subsection{Data}
\label{data}

% ------------------------------------------------------------------------------

\subsubsection{Data collection}
\label{data_collection}

The subject of our analysis are tweets by members of the German parliament
(\textit{Bundestag}) issued after the last federal election in September 
2017\footnote{
The 2017 Bundestag is comprised of 709 seats and seven political parties: the 
right-wing AfD, the Christian Democrats (CDU/CSU), the Liberals (FDP), the 
Greens, the Left Party, and the Social Democrats (SPD).
CDU/CSU and SPD as ruling parties co-exist in a grand coalition.
}.
Twitter makes these publicly accessible via its official API and the number of 
retrievable tweets per user can be exploited generously, so data supply is 
almost unrestricted.
However, with sentiment classification as ultimate goal of analysis, we face a 
major bottleneck in the need for labeled data.
Lacking the resources for large-scale annotation we did the labeling by hand.
The resulting data set, from a vast amount of available data, consists of 1,215 
observations, imposing some practical limits on the analytical scope.
\\

\textbf{Web scraping.}
For data collection from the Web we rely on the scraping procedure developed in 
the predecessor project, with minor modifications.
The process entails four steps: first, gather MPs' names and basic information 
(such as party affiliation and electoral district) from the official Bundestag 
website; second, find Twitter account names (using individual party websites as 
additional sources); 
third, acquire socioeconomic information for the time of the last 
federal election on a per-district level (available at the official website of 
the federal returning officer); and, lastly, scrape actual tweets along with 
some additional variables like the number of retweets.
We use a \texttt{Python} code base and mainly employ selenium webdrivers as well
as the \texttt{BeautifulSoup} library \citep{richardson2007} for parsing HTML
content and the \texttt{tweepy} library \citep{roesslein2020} for accessing the
official Twitter API.
For more details on the procedure and the large data base assembled in the 
predecessor project please refer to \citet{schulzewiegrebe2020}; the code is 
fully submitted in our electronic appendix and a somewhat more compact 
demonstration may be found among the teaching material.
\\

\textbf{Data labeling.}
In the data annotation phase we extracted a set with some tens of thousands 
of observations according to the above process and manually selected what we 
deem informative examples.
For these we assigned polarities, i.e., predicates \textit{positive} or
\textit{negative}, and also topic descriptions required for BERT's ABSA task.
We noted in the process that a large number of tweets do not appear to carry 
sentiment at all.
The resulting 1,215 training observations, originated by a total of 256 MPs, 
date from the period of October 2017 to January 2021.
In figure \ref{fig_obs_time} we detect both periodical fluctuations in the 
number of tweets over time and a general upward-sloping trend.

\begin{figure}[H]
  \includegraphics[width = \textwidth]{figures/obs_over_time}
\caption[Training data: observations over time]
{\raggedright Observations over time.}
\label{fig_obs_time}
\end{figure}

An exemplary extract from our training data with some of the most important 
variables is shown in table \ref{tab_extract}.
Exactly which features enter sentiment classification is documented in the 
subsequent chapters.

\begin{table}[H]
  \scriptsize
  \begin{tabular}{l|l|l|l|r|r|l}
  \texttt{username} & \texttt{party} & \texttt{created\_at} & \texttt{text} & 
  \texttt{followers} & \texttt{unemployment\_rate} & \texttt{label}\\
  \hline
  karl\_lauterbach & spd & 2019-12-01 09:44:00 & "Die Wahl ..." & 337001 & 8.5 & 
  negative\\
  \hline
  Martin\_Hess\_AfD & afd & 2018-08-17 07:15:00 & "Vor den ..." & 6574 & 3.5 &
  negative\\
  \hline
  BriHasselmann & gruene & 2019-09-25 15:35:00 & "Ich finde ..." & 20299 & 8.6 
  & positive\\
  \hline
  danielakolbe & spd & 2020-05-12 06:05:00 & "Aber verpflichtend ..." & 8158 & 
  8.3 & negative\\
  \hline
  JuergenBraunAfD & afd & 2020-08-13 22:05:00 & "Panik-Latif + ..." & 3188 & 
  3.4 & negative\\
  \end{tabular}
  \caption[Training data: extract]
  {\raggedright Training data extract for selected variables.}
  \label{tab_extract}
\end{table}

Figure \ref{fig_obs_party} depicts the number of observations per party both 
for our labeled training data and the larger sample from which the training data 
have been selected (containing just over 31,000 tweets).
We notice two things.
First, in either case, the share of tweets (blue) does 
not mirror the share of seats in the Bundestag (gray); most notably, the 
Christian Democrats tweet rather little, whereas the Greens are 
disproportionately active on Twitter.
Second, the right-wing AfD and the Greens are over-represented in our training 
data at the expense of the other groups.
This is simply because these two parties, in our personal experience from the
annotation process, more often issue tweets that are strongly opinionated.

\begin{figure}[H]
  \includegraphics[width = \textwidth]{figures/obs_per_party}
\caption[Training data: observations per party]
{Observations per party in labeled training data (\textit{left}) and entire 
scraped data example (\textit{right}), both depicted against seat distribution 
in current parliament.}
\label{fig_obs_party}
\end{figure}

Inspecting the class label distribution in the training data,
an imbalance favoring the negative class becomes immediately visible: some 
72\% of tweets have been marked as negative.
This reflects our general impression that most tweets which do carry sentiment 
express negative opinions.

% ------------------------------------------------------------------------------

\subsubsection{Data pre-processing}
\label{data_preproc}

The standard ML approach requires a lot more feature engineering 
than BERT does, but some general pre-processing steps are applied for both.
In an initial step all tweets in non-German language are excluded from the data.
We proceed with basic text cleaning, namely transcription of 
German umlauts and ligature s into standard-Latin characters and removal of 
non-informative symbols (such as those induced by ampersand conversion). 
The next block of operations is specific to Twitter data and includes the 
identification, separate storage and subsequent removal of special characters 
such as hashtags, emojis and user tags.
By this we ensure the data are available for explicit analysis but do not 
introduce noise in the text.
We finish the pre-processing procedure by assigning a unique identifier to 
each tweet.

% ------------------------------------------------------------------------------

\subsubsection{Challenges}
\label{challenges}

Text data come with many idiosyncrasies to begin with: language is highly 
diverse, irregular, and subject to constant change.
Contextual dependencies and complex constructs such as colloquialisms or sarcasm 
pose serious obstacles in NLP, besides which profanities like spelling or 
translation mistakes must be handled \citep{mohammad2017}.
Some particular properties of the data at hand add to these challenges.
\\

\textbf{Language-specific.}
Not surprisingly, most work in NLP is concerned with the analysis of 
English documents.
Although German is not a low-resource language and attracts its own share of 
research, many analyses and tools are predominantly tailored to English.
German grammar is another aspect that needs to be considered.
Syntax is heterogeneous, and inflections due to cases and genera 
result in many variations of lexical lemmata \citep{rauh2018}.
\\

\textbf{Twitter-specific.}
Tweets' brevity is arguably the most critical issue for analysis.
The limit of 280 characters means that words rarely appear more than once and 
we cannot expect many indicators of topics or sentiments in each document.
It also prompts the use of abbreviations.
On a similar note, we observe that Twitter posts often refer to certain events 
or topical entities without explicitly mentioning them, which is probably both 
due to the character limit and the real-time character of publications.
The message may be clear for a well-informed human annotator then but will be 
hard to grasp for machines.
Furthermore, tweets tend to be of rather informal style and use language that 
appears almost exclusively in social media, enlarging the vocabulary the 
classifier must understand.
\\

\textbf{Context-specific.}
The context of our data lessens the degree of informality somewhat; the issued 
documents are mostly political statements and as such more akin to written 
texts from other sources.
Still, the political domain introduces new vocabulary yet again and makes the 
transfer of knowledge from other contexts harder.
We also note that sarcasm and irony are frequently applied. 
Lastly, as mentioned before, we find many tweets to be solely informative and 
detect an imbalance toward negative sentiment in those that do convey opinion.

% ------------------------------------------------------------------------------

\subsection{Standard machine learning solution}
\label{tssa_ml}

% ------------------------------------------------------------------------------

\subsubsection{Concept}
\label{tssa_ml_concept}

A number of components make up a typical workflow in supervised ML that adheres 
to the principle of \textit{empirical risk minimization}
(figure \ref{fig_ml_workflow}).

\vspace{0.3cm}

\begin{minipage}[b]{0.45\textwidth}
  Starting from a \textit{task} which consists of (initial) features and one or 
  multiple target values, we train a \textit{learner} on parts of the data.
  The learner encapsulates our beliefs about the feature-target relationship 
  (\textit{hypothesis}) and uses some \textit{optimization} method to produce a 
  model that incurs minimum training loss, or \textit{risk}, according to some 
  metric.
  Applying this usually parameterized model to the observations set aside for 
  testing yields predictions we can compare to the ground truth for 
  \textit{performance evaluation}, again with respect to a given metric. 
  With this, we estimate the true \textit{generalization ability} of our model 
toward unseen data from the same data-generating process.
\end{minipage}%
\begin{minipage}[b]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[b]{0.5\textwidth}
  \begin{figure}[H]
    \begin{center}
      \includegraphics[width = \textwidth]{figures/supervised_learning_schema}
    \end{center}
  \caption[Typical ML workflow]{Typical ML workflow \citep{mlr3book}}
  \label{fig_ml_workflow}
  \end{figure}
\end{minipage}%

\vspace{0.3cm}

As performance might depend rather strongly on the given training data, 
especially if the number of observations is small, we will typically 
conduct multiple train-test splits (\textit{resampling}).
Various resampling strategies are available, such as bootstrapping or 
cross-validation.
When the training process requires internal validation steps itself, e.g., for 
hyperparameter tuning, we need a \textit{nested resampling} procedure to ensure 
that no information from training leaks into the test predictions 
\citep{japkowiczshah2011}. 
\\

As figure \ref{fig_ml_workflow} suggests, the workflow is of quite modular 
nature.
We therefore build an automated ML (AutoML) pipeline for sentiment 
classification that allows to repeat training procedures and exchange components 
easily.
Following a pre-training feature extraction phase (referred to as 
\textit{static features}), the pipeline roughly serves 
the purpose of computing topic-specific embeddings (\textit{dynamic features}), 
selecting an appropriate learning algorithm with included tuning step, and 
returning a trained model.
Designing the process this way is beneficial for several reasons:
\newpage

\begin{enumerate}
  \item \textbf{Dichotomy between train and test sphere.} Topic-specific 
  embeddings are computed globally across several observations.
  In order to actually enable predictions for unseen data, these dynamic 
  calculations must be based solely on the training data.
  Since unbiased learning requires strict separation between train and test
  sphere, we must be careful to wall off predictions at test time from these 
  computations exacted during training.
  The pipeline approach ensures this for each sequence of training and 
  prediction.
  \item \textbf{Automatization.} Automating (parts of) algorithm selection and 
  hyperparameter tuning helps with crucial design decisions by attaching them to 
  the given data.
  \item \textbf{Transferability.} The possibility to plug in alternative 
  components makes the 
  approach instructive also for different applications.
\end{enumerate}

In the following, we explain in more detail the two different stages of feature 
extraction, introduce the learners we propose to include, and show how the 
final pipeline is assembled.

% ------------------------------------------------------------------------------

\subsubsection{Feature extraction}
\label{tssa_ml_feat}

\textit{\textbf{Static features}}
\\

Static feature extraction forms the first block of the standard ML procedure and 
provides tabular data which then enter the AutoML pipeline.
We refer to these features as static because they can be calculated prior to any 
training process, depending solely on single observations.
They are based on the so-called \textit{bag-of-words 
(BOW)} assumption that leads to texts being treated as arbitrary collections of 
vocabulary instances.
In particular, information about grammar and word order is discarded in BOW 
approaches.
This is obviously a strong simplification but hard to avoid entirely with 
standard classifiers \citep{cambriaetal2017}.

The static feature extraction steps rely heavily on \texttt{R}'s 
\texttt{quanteda} package \citep{pkgquanteda} for organizing documents in corpus 
objects, tokenizing texts and performing look-ups with dictionaries.
We use lists of stopwords in multiple places to exclude uninformative 
tokens such as determiners or auxiliary verbs.
These are compilations from various sources, including \texttt{quanteda}'s 
built-in list, open-source data available on 
\href{https://github.com/stopwords-iso/stopwords-de}{GitHub} and some manually 
appended, domain-specific terms. 
In addition, we repeatedly apply \textit{stemming} in order to reduce words to 
their root form (for instance, cutting back both \textit{"works"} and 
\textit{"working"} to \textit{"work"}).

All of these operations are aimed at compressing document representation from 
the total of unique original terms to more generalized tokens that co-occur 
across texts
This way we create features that are actually shared by multiple observations 
and thus help classifiers infer feature-target relations with the ability to 
generalize.
For the static part we exploit insights from other studies 
(e.g., \citet{balyetal2017}, \citet{correaetal2017}, \citet{jabreelmoreno2017}, 
\citet{sidarenka2019})
and include the following features: 

\begin{enumerate}

  \item \textbf{Lexicon-based polarity counts.} These comprise two sources of 
  prior polarity, namely words and emojis. 
  We use the aggregate of three large German polarity-term dictionaries, 
  \textit{GlobalPolarityClues} \citep{waltinger2010}, \textit{SentiWS} 
  made available \href{https://wortschatz.uni-leipzig.de/en/download}{online}
  by Leipzig university and a collection by \citet{rauh2018}, for word 
  polarities.
  The resulting lexicon distinguishes weak and strong sentiment.
  In each tweet, if a word is part of the dictionary and judged to have 
  either positive or negative connotation (weak or strong), it raises the 
  respective polarity count by one.
  Emojis are treated analogously, though without the discrimination between 
  degrees of sentiment, using an emoji polarity list accumulated by 
  \citet{kraljetal2015}.
  \newpage
  
  \item \textbf{Twitter variables.} We make use of the additional information 
  provided by the Twitter API and note for each tweet the number of likes and 
  retweets as well as the number of users tagged.
  For hashtags we resort to the same naive measure since we find them to be so 
  heterogeneous across documents in our training set that no meaningful 
  information can be extracted. 
  This certainly marks an opportunity for future improvement.
  
  \item \textbf{Syntactic features.} This group attempts to mitigate the 
  simplification introduced by the BOW assumption to some extent.
  We track the presence of modifiers that indicate sentiment intensification 
  as well as punctuation via the respective number of question and exclamation 
  marks.
  Negation is a delicate problem we currently treat with the brute-force 
  approach of noting whether or not negating tokens are present.
  German language complicates negation handling by the fact that modifier and 
  modification target often appear in quite separate sentence positions, making 
  it hard to identify the actually negated construct in an automated manner.
  
  \item \textbf{Character unigrams.} Unigrams refer to single tokens (whereas 
  $n$-grams mean the concatenation of $n$ tokens) and we consider the special 
  case of character tokens, meaning we count the occurrence of single letters.
  While this may not seem too helpful at first glance, character unigrams 
  lead to a very dense document representation as opposed to the sparsity 
  induced by word unigrams or even bi- and trigrams.
  
  \item \textbf{Part-of-speech (POS) tags.} The last type of features captures 
  grammatical structure.
  POS tags are used to categorize sentence parts according to their syntactic 
  role.
  Including these stems from the idea that the number of certain tags, for 
  example adjectives and adverbs, might indicate sentiment.
  % We use the \texttt{spacyR} package \citep{pkgspacyr}, a wrapper around 
  % \texttt{Python}'s \texttt{spaCy} library closely integrated with 
  % \texttt{quanteda}, for POS tagging.
  
\end{enumerate}

\vspace{0.3cm}

\textit{\textbf{Dynamic features}}
\\

The static features are not in any sense topic-specific.
In order to allow for sentiments to vary across topics, we include 
topic-specific word embeddings.
This entails a two-step procedure: first, we group documents with an STM, 
and then compute word embeddings for each topical cluster. 
\\

\textbf{Topic modeling.}
The STM \citep{robertsetal2016} is a probabilistic topic model and attempts to 
recreate the very process that generated the observed documents.
As sketched in section \ref{tm}, this is achieved by sampling topic proportions 
for each document from some distribution, assigning word positions to topics 
with the resulting probabilities, and drawing vocabulary instances from the 
distribution associated with the respective topic.
As opposed to the original LDA approach that assumes a single global prior 
distribution, the STM offers the possibility to incorporate document-level meta 
data. 
These may affect the sampling process in two places: first, by influencing the 
topic proportions for a given document (\textit{topical prevalence}), and 
second, by altering the distribution the vocabulary assumes over each topic 
(\textit{topical content}).
Our case lends itself more to using topical prevalence variables, presuming that 
certain meta features on electoral district level, which can be specified in 
terms of an additive formula, have an impact on which topics are discussed.
Defining a topical content variable (a single, categorical one is supported in 
the STM implementation \citep{pkgstm}) allows for topic-word distributions to 
vary for its different levels.
Such an approach is also conceivable but not further pursued here.
The STM can be expressed by the following generative process 
\citep{robertsetal2016}.

For each document $d \in \setd$:
\newpage

\begin{enumerate}

  \item Draw non-normalized topic proportions $\bm{\eta_d}$ from a 
  $(K - 1)$-dimensional Gaussian: \\
  $\bm{\eta_d} \sim \mathcal{N}_{K - 1}(\bm{\Gamma}^T\bm{x}_d^T, \bm{\Sigma})$, 
  where the $K$-th component is set to zero to keep the model identifiable.
  $\bm{x}_d$ is the vector of $P \in \N$ prevalence covariates associated with 
  document $d$, and $\bm{\Gamma} \in \R^{P \times K}$ is a matrix of topic 
  proportion coefficients obtained by means of Bayesian linear regression with 
  Gaussian priors and first-order penalty term.
  The covariance matrix $\bm{\Sigma}$ can have arbitrary structure and thus 
  incorporate inter-topic correlations.
  
  \item Normalize $\bm{\eta_d}$ through a softmax operation, 
  yielding $\bm{\theta}_d$ with $\theta_{d, k} = \frac{\exp(\eta_{d, k})}{
  \sum_{j = 1}^K \exp(\eta_{d, j})}$, \\$k \in \setk$. 
  The $\bm{\theta}_d$ then effectively follow a logistic normal distribution. 
  
  \item For each word position $n \in \{ 1, 2, \dots, N_d \}$:
  
  \begin{enumerate}
    \item Draw $\bm{z}_{d, n} \sim \mathit{Multinomial}(\bm{\theta}_d)$ to 
    assign the $n$-th position to a topic. 
    \item Draw a word $w_{d, n}$ from the word distribution corresponding to the 
    assigned topic: $w_{d, n} \sim \mathit{Multinomial}(\bm{\beta}(d, n))$.
  \end{enumerate}
  
\end{enumerate}

For our analysis we include a total of four prevalence covariates, namely the
MP's party affiliation and federal district (\textit{Bundesland}) as well as 
the share of migrant population and overall employment rate.
The latter two are modeled as smooth effects with five degrees of freedom each.
While we find this formula to work well with our particular training data, the 
specification can be easily modified.

However, it must be noted that the STM requires a major simplification of the 
topic modeling problem.
Due to the extreme brevity of our texts, tweets must be pooled into larger 
pseudo-documents to prevent the topic extraction from failing.
We therefore resort to aggregating tweets on a per-user-per-month level.
The underlying assumption of authors discussing the same topics over the course 
of several weeks is problematic; yet, other generative models would be 
similarly affected.
Topic models specifically designed for short texts (see, for example, 
\citet{quiangetal2019}) are not yet widely available but should certainly be 
considered for future work in this area.
Alternatively, finding a smarter pooling criterion might help to alleviate 
the oversimplification.
\\

\textbf{Word embeddings.}
We now use the information from topic assignment to compute topic-specific word 
embeddings.
Recall that the BOW assumption results in very high-dimensional document 
representations disregarding inter-term relationships.
Word embeddings, by contrast, seek to capture semantic structures in these 
representations: 
words are characterized by their surrounding context and projected into a latent 
space in which similar words ought to end up in nearby locations.
At the same time, embeddings perform dimensionality reduction as the latent 
space is typically chosen to be of much lower dimension than the 
observation space.

We use the \textit{Global Vectors (GloVe)} model proposed by 
\citet{penningtonetal2014} and implemented in \texttt{R} by \citet{pkgtext2vec}.
Its computation is based on the symmetric \textit{co-occurrence matrix} 
$C \in \R^{M \times M}$, where $M \in \N$ denotes the number of tokens remaining 
after such pre-processing steps as stopwords removal.
Entry $c_{i, j}$ states how often token $j$ occurs in the context of token $i$.
Context is a local notion controlled via \textit{window size} $\ell \in \N$, 
where $\ell$ specifies the number of positions to each side of a token
considered to be in its vicinity, the underlying (simplifying) assumption being 
that the strength of semantic relation decays with increasing distance.
The entries of the co-occurrence matrix are weighted by the inverse 
distance to the target token.
We can therefore express probabilities of co-occurrence as fractions of row (or 
column) sums: the probability of the $i$-th token occurring in the context of 
the $j$-th token is given by $p_{ij} = \tfrac{c_{ij}}{c_{i.}}$ with $c_{i.}$ 
as the $i$-th row sum.
The co-occurrence probabilities in turn give rise to odds 
$\frac{p_{ik}}{p_{jk}}$, for which \citet{penningtonetal2014} posit that they 
should be reflected by embedding vector distances.
This is formalized as 

\begin{equation*}
  \exp((q_i - q_j)^T \tilde q_k) = \exp(q_i^T \tilde q_k - q_j^T \tilde q_k) = 
  \frac{p_{ik}}{p_{jk}},
\end{equation*}

with $q_i \in \R^m$, $m \in \N$, as the $m$-dimensional embedding vector 
representation for word $i$.
$\tilde q_k$ is structurally equivalent to $q_k$ for symmetric $C$ and 
discrepancies arise solely from different random initialization of the model.
The authors do not provide a sound theoretical foundation for this double 
computation but point out that such stochastic variation has been shown to 
benefit overparameterized models.
In the end GloVe actually uses the sum of the vectors as embeddings.
We arrive at $q_i^T \tilde q_j + b_i + \tilde b_j \approx \log(c_{ij})$,
with bias $b_i$ approximately equal to $\log(c_i)$, and can now express the 
embedding error minimization problem by the following weighted least-squares
objective:

\begin{equation*}
  \min_{\bm{v}, \bm{b}} \sum_{i = 1}^M \sum_{j = 1}^M g(c_{ij}) \left(
  q_i^T \tilde q_j + b_i + \tilde b_j - \log(c_{ij}) \right)^2.
\end{equation*}

$g: \R \rightarrow \R$ is a weighting function for which the authors propose to 
use

\begin{equation*}
  g(x) = \begin{cases} \left( \frac{c_{ij}}{c_{\mathit{max}}} \right)^{\delta} & 
  c_{ij} < c_{\mathit{max}}, \\
  1 & \text{otherwise}.
  \end{cases}
\end{equation*}

$c_{\mathit{max}} > 0$ and $\delta > 0$ are hyperparameters to the GloVe 
algorithm, as are the embedding dimensionality $m$ and the size $\ell$ of 
the context window \citep{penningtonetal2014}.
\\

GloVe returns an embedding vector for each word.
Since we are interested in a representation on document level, the word 
embeddings are aggregated in a final step.
This leaves us with a block-like structure of embedding vectors: for $K$ topics 
and $m$ embedding dimensions we obtain $K \cdot m$ feature vectors, where each 
document has non-zero entries only for the columns corresponding to its 
associated topic.
At the end of the entire feature-generating process we can thus describe our 
documents by a number of static covariates plus topic-specific embedding values.
This tabular representation of our Twitter corpus eventually forms our 
classification task.

% ------------------------------------------------------------------------------

\subsubsection{Sentiment classification}
\label{classifiers}

We propose two different classification algorithms, a \textit{random forest} and 
a logistic regression learner with elastic net penalty (\textit{glmnet}).
Both ship with built-in feature selection, which we consider rather helpful in 
the potentially high-dimensional feature space of text classification, and are 
fairly easy to tune (\citet{japkowiczshah2011}, \citet{probstetal2019}).
However, the pipeline design would allow to plug in any other suitable learner.
\\

\textbf{Random forests.}
Random forests are ensembling methods with tree  base learners 
\citep{louppe2014}.
Classification trees create subsequent rectangular partitions of the feature 
space via binary splits along one input dimension at a time.
During training, observations are passed along the resulting tree structure 
until they end up in a particular leaf node.
Predictions are made by assigning the prevailing class label in this node.
The associated hypotheses can be expressed in the form 
$f(\bm{x}) = \sum_{t = 1}^T r_t \mathbb{I}[\bm{x} \in R_t]$, where $T \in \N$ 
denotes the number of terminal regions $R_1, R_2, \dots, R_T$ and 
$r_t \in \{0, 1\}$ is the class label predicted for observations in leaf node 
$Q_t$.
Each split is loss-minimal across all child nodes by design.
Trees thus perform a greedy search that, unless regularized, requires exhaustive 
evaluation of all possible split-threshold combinations \citep{breimanetal1984}.

When allowed full growth, i.e., until each observation has its own leaf node, 
classification trees will perfectly fit every pattern in the training data, 
making their structure extremely sensitive to data alterations.
This low-bias, high-variance character is exploited in random forests, where 
$B \in \N$ tree base learners are assembled to predict class labels via 
majority voting.
The procedure is typically randomized in two aspects: first, each base learner 
receives a bootstrap sample of the data for training, and second, it may use 
only a random subset of the features for splitting.
As a result we obtain decorrelated and relatively stable ensembles. 
% \newpage

A major advantage of random forests is their natural handling of all feature 
types, including interactions of arbitrary order.
This property, along with built-in variable selection, makes them suitable 
candidates for tasks with many, possibly correlated, features 
\citep{louppe2014}.
\\

\textbf{Regularized logistic regression.}
Logistic regression employs a predictor that is linear in its coefficients but 
subject to a non-linear transformation with a logit link function, which enables 
the prediction of a conditionally Bernoulli-distributed quantity. 
Consider the feature matrix $\bm{X} = [\bm{x}_d^T]_{d = 1, 2, \dots, D} 
\in \mathcal{X} \subset \R^{D \times p}$ and let $\bm{y} \in \{0, 1\}^D$ denote 
the binary sentiment target.
Logistic regression models the probability of $y_d = 1$ for a 
given observation $\bm{x}_d$ as \\$\pi(\bm{x}_d) = \mathbb{P}(y_d = 1 ~ \rvert ~
\bm{x}_d) = \tfrac{\exp(\gamma_0 + \bm{\gamma}^T\bm{x}_d)}{1 + \exp(\gamma_0 +
\bm{\gamma}^T\bm{x}_d)}$, with regression coefficients $(\gamma_0, \bm{\gamma}) 
\in \R^{p + 1}$ \citep{lindsey1997}.
The output is a probabilistic score in the unit interval.
By thresholding these posterior probabilities, often simply at 50\%, we obtain 
actual class label predictions.
As our task entails a large number of possibly correlated features, we opt for 
regularized logistic regression, where a penalized version of the negative 
log-likelihood is used to estimate $(\gamma_0, \bm{\gamma})$.
More specifically, we pose an additive elastic penalty, which yields a hybrid 
of first-order (LASSO) and second-order (Ridge) regression and thus promotes 
shrinkage in the regression coefficients (typically only in the non-intercept 
terms).
$\lambda \geq 0$ controls overall penalization and the two 
regularizers are balanced by a parameter $\alpha \in [0, 1]$.
We then arrive at the following optimization problem
\citep{hastieetal2021}:

\begin{equation*}
    \min_{(\gamma_0, \bm{\gamma}) \in \R^{p + 1}} - \frac{1}{D} \sum_{d = 1}^D
    y_d (\gamma_0 + \bm{\gamma}^T\bm{x}_d) - \log(1 + 
    \exp(\gamma_0 + \bm{\gamma}^T\bm{x}_d)) +
    \lambda \left( (1 - \alpha) \| \bm{\gamma} \|_2^2 \cdot \tfrac{1}{2} + 
    \alpha \| \bm{\gamma} \|_1 \right ).
    \label{eq_glmnet}
\end{equation*}

% ------------------------------------------------------------------------------

\subsubsection{Automated machine learning pipeline}
\label{pipeline}

We assemble all components presented above in an AutoML pipeline.
The pipeline takes care of

\begin{tight_enumerate}
  \item computing the dynamic features, i.e., topic-specific embeddings,
  \item selecting and training the best learner, including hyperparameter 
  optimization, and
  \item predicting sentiment labels for new observations.
\end{tight_enumerate}

We implement the algorithm as a graph learner in the \texttt{mrl3} ecosystem
\citep{pkgmlr3}.
Graph learners are composed of nodes that perform operations on the data.
Multiple options for branching and re-uniting enable arbitrarily 
complex constructions.
The design allows to use the entire pipeline just like any other learner object 
in standard ML procedures such as training, prediction, tuning or benchmarking.
\texttt{mrl3}'s internal structure ensures the dichotomy between train and test 
sphere in every step and makes our implementation compatible with other
routines.
\\

\textbf{Pipeline components.}
The proposed pipeline takes a static feature representation of our annotated 
training data and starts by assigning topic labels, afterwards computing 
topic-specific embeddings.
The next block conducts the entire tuning process, where the 
selection of a learner is directly included, such that the optimal classifier is 
found in joint optimization of algorithm and associated hyperparameters.
We rely on existing functionalities for much of the required steps.
However, we contribute custom implementations for the STM and the subsequent 
embedding computation with GloVe.
Embeddings can also be calculated without a preceding topic modeling step.
Furthermore, an alternative topic modeling approach is available. 
It is a non-stochastic procedure that accepts manually defined keywords and 
finds all documents associated with the terms or derivatives thereof.
If necessary, this can be combined with a stratification step that approximately 
retains the ratio of documents associated with a keyword in each resampling 
step.
All these novel operations are constructed as so-called \textit{pipe operators}
in \texttt{mlr3}.
Eventually, the pipeline assumes the structure depicted in figure 
\ref{fig_graph}. 
\vspace{0.5cm}

\begin{minipage}[c]{0.5\textwidth}
  \textbf{Settings.}
  Our design choices are partly inspired by \citet{probstetal2019} and otherwise 
  reflect what we find to work best.
  The decision to compute topic-specific embeddings and the selection 
  of learners are such choices already.
  We further fix some hyperparameters in advance to save on computation time. 
  This includes capping the maximum number of iterations in regularized 
  regression at 1,000 and limiting tree growth in the random forest (leaf nodes 
  must contain at least 5\% of observations) as well as allowing for parallel 
  computations. 
  Other hyperparameters are left to tuning.
  The first two are the number of topics with $3\leq K \leq 6$ and the
  dimensionality of embeddings $10\leq m \leq 50$,
  where we exploit the fact that these inherently unsupervised problems are input
  to a supervised learning task: we can simply have the algorithm choose the
  configuration with the best predictive performance.
\end{minipage}
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
  \begin{figure}[H]
    \includegraphics[width = \textwidth]{figures/graph}
    \caption[Proposed AutoML pipeline]{Schematic representation of proposed\\ 
    AutoML pipeline}
    \label{fig_graph}
  \end{figure}
\end{minipage}

\vspace{0.5cm}

For the actual classification part we allow for optimization of the penalty 
coefficients in logistic regression ($0 \leq \alpha \leq 1$ and $0 \leq \lambda
\leq 0.2$) and the number of candidate features for split computation in the 
random forest (limited to 50\% of features at most), as well as the fraction of 
observations to be sampled for each base learner (between 0.25 and 1).
Note that the search space remains rather small; more tuning parameters, larger 
ranges and/or more configurations could be explored with enough computational 
resources.

We use three-fold cross validation (3-CV) as resampling strategy in both the 
outer (performance evaluation) and inner (hyperparameter tuning) training loop.
3-CV divides the training data into three roughly equal-sized partitions, each 
of which serves as test set once while the other two provide the training data 
\citep{japkowiczshah2011}.
Tuning is conducted via random search with a total budget of 50 evaluations 
(again, this could be extended if affordable).
Random search is based on the simple idea of drawing from the space of possible 
configurations uniformly at random.
While implementation is easy and readily parallelizable, the number of 
evaluations required to sample the space adequately rises exponentially with the
number of dimensions, with no possibility to learn from prior draws 
\citep{feurerhutter2019}.
More efficient strategies might improve the tuning process here.
Lastly, in the inner tuning loop, we evaluate performance by means of accuracy 
(i.e., the share of correctly classified instances, see section 
\ref{tssa_ml_results}), as we identify no need to 
emphasize one side of misclassification or the other in any particular way.
\newpage

% ------------------------------------------------------------------------------

\subsubsection{Results}
\label{tssa_ml_results}

We evaluate the performance of two variants of our pipeline, a full version 
including topic-specific embeddings and a topic-agnostic alternative that 
omits the STM node.
Performance metrics include, besides the elements of the confusion matrix, 
the following quantities (e.g., \citet{japkowiczshah2011}):

\begin{enumerate}
  \item Accuracy: $\rho_{\text{accuracy}} =  \frac{\text{TN + TP}}{
  \text{TN + TP + FN + FP}}$, and
  \item F1 score: $\rho_{\text{F1}} =  2 \cdot
  \dfrac{\rho_{\text{precision}} \cdot \rho_{\text{recall}}}{
  \rho_{\text{precision}} +
  \rho_{\text{recall}}}$, composed of
  \begin{itemize}
    \item[a)] $\rho_{\text{precision}} = \frac{\text{TP}}{\text{TP + FP}}$ and
    \item[b)] $\rho_{\text{recall}} = \frac{\text{TP}}{\text{TP + FN}}$, 
  \end{itemize}
\end{enumerate}

where TN = true negatives, TP = true positives, FN = false negatives and 
FP = false positives.

The results in table \ref{tab_res_ml} show a striking difference in performance.
We see that the learner with topic modeling node (first column) performs 
substantially worse than the topic-free variant (second column) with a gap in 
accuracy of more than 14 percentage points.
In fact it is even outperformed by the featureless classifier (third column) 
that always predicts the most frequent class, in this case the negative one, 
and thus on average achieves an accuracy equivalent to the share of negative
observations:

\begin{table}[H]
  \small
  \begin{tabular}{l|r|r|r|}
    \cline{2-4}
    &
    \multicolumn{1}{l|}{\textbf{learner with topic modeling}} &
    \multicolumn{1}{l|}{\textbf{learner without topic modeling}} &
    \multicolumn{1}{l|}{\textbf{featureless learner}} \\ 
    \hline
    \multicolumn{1}{|l|}{accuracy} & 0.715 & \cellcolor{lightgray} 
    \textbf{0.859} & 0.724 \\
    \hline
    \multicolumn{1}{|l|}{F1 score} & 0.023 & \cellcolor{lightgray} 
    \textbf{0.706} & - \\
    \hline
    \multicolumn{1}{|l|}{TN} & 288.333 & \cellcolor{lightgray} 
    279.667 & \textbf{293.333} \\
    \hline
    \multicolumn{1}{|l|}{TP} & 1.333 & \cellcolor{lightgray} \textbf{68.333} 
    & 0.000 \\
    \hline
    \multicolumn{1}{|l|}{FN} & 110.333 & \cellcolor{lightgray} 
    \textbf{43.333} & 111.667 \\
    \hline
    \multicolumn{1}{|l|}{FP} & 5.000 & \cellcolor{lightgray} 13.667 & 
    \textbf{0.000} \\
    \hline
  \end{tabular}
  \caption[Results for standard ML approach]{Results for standard ML approach 
  (non-integer values in TN, TP, FN and FP are due to aggregation over cross 
  validation folds). Best performance achievements are marked bold; overall 
  best learner is marked gray.}
  \label{tab_res_ml}
\end{table}

Diving deeper into the resampling results, the reason for this behavior becomes 
evident. 
When the embeddings are computed in a topic-specific manner, a much larger 
number of embedding vectors is created (factor 6 for the upper end of the 
search range for $K$), spanning a quite high-dimensional feature space for 
relatively few observations.
In two out of three folds the pipeline selects the logistic regression 
classifier and both times fits a very sparse model that, in effect, differs not 
so much from the featureless classifier, and thus yields similar performance.
Interpreting the variable selection process of the random forest classifier is 
not as straightforward due to the heterogeneity of the 500 base learners and
the randomness induced by sampling features as well as tuning the number of 
split variable candidates via random search.
In any case, topic-specific embeddings seem to increase the number of features 
to a level that the classifiers cannot handle well.
The topic-agnostic learner, by contrast, achieves 85.9\% accuracy and 70.6\% F1 
score\footnote{
For all learners, the gap between accuracy and F1 score may be attributed to the 
class imbalance in the predictions. 
This effect is particularly strong for the topic-agnostic and the featureless 
classifier (which, as discussed above, are quite alike). 
Both achieve very low recall, i.e., the share of positives that 
have been detected as such, because the positive class is rarely predicted 
(indeed not at all in the case of the featureless classifier, which is why its 
recall is actually zero and no F1 score can be calculated due to TP = 0).
}, thereby 
clearly dominating the naive baseline.
Here, the logistic classifier emerges as the best learner in all three folds.
We will see how the BERT solution exhibits similar behavior, leading to the 
conclusion that topic modeling for this kind of data seems to complicate the 
task rather than aiding it.

% ------------------------------------------------------------------------------

\subsection{Deep learning solution}
\label{tssa_dl}

% ------------------------------------------------------------------------------

\subsubsection{Deep transfer learning with BERT}
\label{bert}

As an alternative to casting TSSA as a standard ML classification task we now 
consider a solution rooted in deep learning.
This second part of our work is based on the Bidirectional Encoder 
Representations from Transformers (BERT) network \citep{devlinetal2018} and its 
variations. 
The name draws from the core architecture as a multi-layer bidirectional 
Transformer encoder. 
BERT models are extremely large, their parameters numbering in the millions, and 
considered state of the art for many NLP tasks today.

A thorough introduction to deep learning is beyond the scope of this report and 
the BERT models' architectures are particularly complex at that.
We will therefore only point out two distinctive properties of Transformer 
models and refer to, e.g., \citet{goodfellowetal2016} for further reading on 
deep learning in general and to \citet{vaswanietal2017} for the detailed 
Transformer workflow.
Transformer-based models, which have gained much traction in recent 
research, rely on two key principles, namely \textit{transfer learning} and
\textit{attention}.
\\

\textbf{Transfer learning.} In the standard supervised learning approach, where 
we ideally have a large number of labeled training instances, we assume the 
test data to follow the same underlying distribution as the training data, a 
belief we exploit in evaluating the generalization ability of our learner.
When this can no longer be plausibly claimed and domain shift occurs, for 
example in applying our model to documents with different topical context, we 
cannot expect our model to generalize as the test performance suggests.
Still, the tasks might not be so fundamentally different, and some of the 
gathered information on the feature-target relationship will be of use 
nonetheless.
Much like in human learning processes we can presume a certain amount of 
abstraction.
It is precisely this knowledge migration that is at the heart of 
transfer learning \citep{Murphy2021}.

As discussed before, the abundance of data is in stark contrast to the scarcity
of labels in many applications, such as in ours.
The large (manual) annotation effort this entails poses severe constraints on 
the practicability of supervised learning.
Therefore, it is reasonable to accumulate as much existing knowledge as possible 
to aid the given task.
Transfer learning represents the approach of statistical learning in which we 
first train a model on an original task and domain and then transfer the 
acquired knowledge to the target task and domain \citep{ruder2019}.
This is mainly achieved in the pre-training task (see below) of a BERT model on 
a large corpus in the relevant language with the help of neural networks. 
In the 
fine-tuning step, this knowledge (i.e., the parameter values learned during 
training the source model) is then applied to a new, purpose-specific context to 
learn a certain task and vocabulary \citep{devlin2019}.
\\

\textbf{Attention mechanism.} Moreover, Transformer-based architectures avoid 
processing text data sequentially, as a \textit{recurrent neural network (RNN)}
would.
Obviously, text is inherently sequential and RNNs have been designed to handle 
precisely this property (as opposed to, for example, the BOW approach).
Disadvantages of recurrent modeling, however, include a lack of 
parallelizability and the tendency to favor recent input.
The attention mechanism allows the network to access any past modeling state by
allocating dedicated attention weights and therefore achieves much better 
parallelization and reduced training times \citep{vaswanietal2017}.

% ------------------------------------------------------------------------------

\subsubsection{Input pre-processing}
\label{bert_preproc}

Very roughly speaking, a Transformer architecture consists of an encoder and a 
decoder component: the former processes inputs and creates an intermediate 
representation the latter then maps to predictions \citep{zhang2020}.
In order to apply BERT, the textual input to its encoder must be pre-processed 
in a specific way.
To this end, we split word into tokens or word fragments based on a given 
vocabulary which is determined a priori by the pre-trained language model.
Each sequence of tokens is then converted into a numerical vector, with some 
additional tokens: the beginning of the first sequence is signed by a “[CLS]” 
token, the end by a “[SEP]” token. 

Consider the following example to illustrate this procedure, where the first 
statement is the original text and the second its pre-processed form:

\begin{center}
\fbox{\begin{minipage}{0.8\textwidth}
\raggedright
\textit{Die Ausgrenzung von MigrantInnen von der \#EssenerTafel ist inakzeptabel 
und rassistisch. Wir dürfen nicht zulassen, dass die Ärmsten gegeneinander
ausgespielt werden.
}
\end{minipage}}
\end{center}

\begin{center}
\fbox{\begin{minipage}{0.8\textwidth}
\raggedright
\textit{[CLS] Die Ausgrenzung von MigrantInnen von der \#EssenerTafel ist 
inakzeptabel und rassistisch. Wir dürfen nicht zulassen, dass die Ärmsten 
gegeneinander ausgespielt werden. [SEP]
}
\end{minipage}}
\end{center}

A marker assigning it to sentence "A" or "B" helps to indicate for each token 
in a sequence which sentence it belongs to.
A positional embedding further stands for the position of each token in the 
sentence.
In short, the document contents and structure are translated into a numerical 
representation.
The inputs have to be of identical length, where the maximum length of an input 
sequence can be 512 tokens. 
Shorter inputs are filled with padding tokens and longer ones are truncated 
\citep{devlin2019}.
% \newpage

% ------------------------------------------------------------------------------

\subsubsection{Pre-training}
\label{bert_pretrain}

In its originally proposed form, BERT was pre-trained on a huge corpus of 
Wikipedia entries for two types of task: \textit{masked language modeling (MLM)} 
and \textit{next sentence prediction (NSP)}.
These basic strategies shall enable the network to learn the fundamental 
structure of a language from a vast amount of training data in a self-supervised 
manner, without any need for labels.
\\

\textbf{Masked language modeling (MLM).}
By masking a random word, BERT tries to predict it without considering its 
positioning in the sequence. 
As BERT is able to work in a bidirectional way, 
that is, to condition the predictions for the masked word on the co-occurring 
words on both sides, it is able to capture varied and flexible 
information about the context of a certain word. 
After randomly choosing  15\% of the token embeddings, 80\% of these will be 
replaced by the “[MASK]” token, whereas a further 10\% are swapped for a random 
token, and in the 10\% remaining cases, the masked tokens are left unchanged. 
Afterwards, these sequences are fed into the BERT model \citep{devlin2019}.
Revisiting our example, the masking process could result in something like: 

\begin{center}
\fbox{\begin{minipage}{0.8\textwidth}
\raggedright
\textit{[CLS] Die Ausgrenzung von [MASK] von der \#EssenerTafel ist inakzeptabel 
und [MASK]. Wir dürfen nicht zulassen, dass die [MASK] gegeneinander ausgespielt
werden. [SEP]
}
\end{minipage}}
\end{center}

\textbf{Next sentence prediction (NSP).}
Furthermore, to capture the relationship between two sentences, BERT learns to 
predict whether or not the second sentence in a pair is the subsequent one in 
the original document. 
During training, half of the inputs are comprised of original pairs, while the 
other half has a random sentence from the corpus as second sentence. 
The goal is to have BERT distinguish between real and fake pairs in a reliable 
manner.

\begin{center}
\fbox{\begin{minipage}{0.8\textwidth}
\raggedright
\textit{\textbf{Sentence A:} [CLS] Die Ausgrenzung von [MASK] von der 
\\\#EssenerTafel 
ist inakzeptabel und [MASK]. [SEP]
\\
\textbf{Sentence B:} Wir dürfen nicht zulassen, dass die [MASK] gegeneinander 
ausgespielt werden. [SEP]
\\
\textbf{Label:} IsNextSentence
}
\end{minipage}}
\end{center}

\begin{center}
\fbox{\begin{minipage}{0.8\textwidth}
\raggedright
\textit{\textbf{Sentence A:} [CLS] Die Ausgrenzung von [MASK] von der 
\\\#EssenerTafel 
ist inakzeptabel und [MASK]. [SEP]
\\
\textbf{Sentence B:} Freue mich sehr für ihn und auf die Zusammenarbeit. [SEP]
\\
\textbf{Label:} NotNextSentence
}
\end{minipage}}
\end{center}

These two strategies are applied jointly in order to minimize an additive loss 
function.
% \newpage

% ------------------------------------------------------------------------------

\subsubsection{Fine-tuning}
\label{bert_fine}

BERT can be applied to various language tasks, e.g., \textit{question 
answering} or \textit{named entity recognition}, and sentiment classification 
in particular.
In order to have the pre-trained network solve a specific problem, we must 
fine-tune it on the target task.
Crucially, at this point, the need for labeled data comes in.
For fine-tuning we simply alter the output layer that adapts to the 
target task.
In our analysis, we use the \texttt{huggingface PyTorch} implementation 
\texttt{BertForSequenceClassification}\footnote{
\url{https://huggingface.co/transformers/model\_doc/bert.html\#tfbertforsequenceclassification}
},
which adds a single linear network layer on top of the existing network for 
classification.
During fine-tuning, parameters in all layers are updated \citep{devlin2019}.
We set the following parameters recommended to use for fine-tuning by the 
authors: a minibatch size of 16 sequences, a global Adam learning rate of 2e-5, 
and 4 as number of epochs. 

% ------------------------------------------------------------------------------

\subsubsection{Aspect-based sentiment analysis}
\label{bert_absa}

The ABSA approach is a more sophisticated task than standard text-level 
sentiment analysis. 
Apart from classifying a given text, in this case a tweet, it focuses on 
extracting aspects mentioned in a given text together with the tokens contained 
in the text in the hope of extracting the most relevant information from the 
data.
For our data situation, the most appropriate approach is therefore the 
methodology described by \citet{xuetal2019} in fine-tuning BERT to ABSA.
\\

\textbf{Post-training.}
The basic pre-trained BERT model used in our work is the 
\texttt{bert-base-german-cased} model as we deal with tweets in German language 
and may expect to have advantages from letter casing (all nouns beginning with 
capital letters in German). 
This model was originally pre-trained using the latest German Wikipedia texts, 
news articles and open legal data sets of German court decisions and citations. 
This data set has a size of approximately 12 GB in total 
(\url{https://deepset.ai/german-bert}, \citet{ostendorffetal2020}). 
In order to improve both the domain and task knowledge, the authors recommend to 
apply a so-called post-training task based on contextually related data because 
fine-tuning BERT directly with a limited amount of labeled data may end up with 
domain and task challenges. 
Post-training on domain knowledge is applied using the pre-trained weights as 
initialization and leveraging both pre-training tasks (MLM and NSP). 
The weights are updated based on the summed losses from both tasks. 
\\

\textbf{Aspect extraction.}
The aspect extraction sub-task is supposed to find aspects in a given text that 
the author has commented on. 
The idea is to use a supervised technique and label each token from a sequence 
with one of three labels: \textit{B – beginning of an aspect}, 
\textit{I – inside of an aspect}, or \textit{O – outside of an aspect}. 
Afterwards, for each position of the sequence, a dense layer with subsequent 
softmax operation is applied to predict one of the three labels for all 
positions of a sequence. 
As shown in the original paper, the aspect extraction task requires, perhaps 
unsurprisingly, exhaustive domain knowledge \citep{xuetal2019}.
\\

\textbf{Aspect sentiment classification.}
Lastly, the aspect sentiment classification sub-task attempts to classify the 
sentiment polarity of a given text, generally into positive, negative or 
neutral (omitting the neutral category in our case). 
The two inputs for this task are an aspect and a sentence or tweet containing 
this aspect, where the aspects are either extracted automatically with the above 
methodology or are made available beforehand by another technique. 
After application of the softmax activation and training with cross-entropy loss 
we obtain probabilities predicted for each of the sentiment categories that are 
eventually thresholded to yield a unique class label.
\citep{xuetal2019}.
\\

% ------------------------------------------------------------------------------

\subsubsection{Implementation}
\label{bert_implementation}

All of the models and methods are applied in \texttt{Python} (version 3.7.10) 
using the computational resources of Google Colaboratory\footnote{
\url{https://colab.research.google}
}, where free GPU power is available to 
researchers.
As mentioned above, we adopt the \texttt{bert-base-german-cased} model as the 
basis for our experiments.

The overall application can be divided into two general parts: first, 
document-level sentiment analysis, and second, ABSA.
For document-level analysis we discern four variations.
Besides using the basis model by directly fine-tuning it on our train set of 
972 tweets (that is, using 80\% of our labeled data), we additionally enrich our 
training instances by 6,444 customer reviews about the German public train 
operator \textit{Deutsche Bahn}, made available in the course of the GermEval 
coding challenges.
The other 20\% of labeled data are used for evaluation. 
Furthermore, we develop our basic model by post-training it on domain knowledge, 
namely on 29,715 unlabeled and previously unused tweets collected in the 
scraping process.
This model is then fine-tuned. 
Finally, a fourth variant is given by another German-language model 
named \texttt{bert-base-german-dbmdz-cased}\footnote{
\url{https://huggingface.co/dbmdz/bert-base-german-cased}
} and pre-trained on a larger data 
source than the basic model.
% : recent Wikipedia dump, EU Bookshop corpus, Open 
% Subtitles, CommonCrawl, ParaCrawl and News Crawl. 
The data set has a size of 16 GB and over 2 billion tokens, suggesting the 
ability to produce better results.

For ABSA\footnote{
The ABSA implementation draws on code from
\url{https://github.com/howardhsu/ABSA_preprocessing} for pre-processing and 
from \url{https://github.com/howardhsu/BERT-for-RRC-ABSA} for post-training.
} we additionally post-train the basic model on the GermEval 
data set but do not use these data as part of training instances, as the aspects 
deviate substantially from those detected in our corpus. 
The methodology for ABSA is widely used in sentiment analysis of review texts 
and is applied to tweets with political context in this project, which is 
arguably a quite different domain. 
Perhaps due to this fact, abstract extraction is not successful for our data. 
Our personal presumption is that this happens because of the lack of clear 
contextual and semantic delimitation of aspect-containing words we observe in 
the training data. 
We see plenty of opportunities for other use cases of this implementation, 
and therefore include it in our submitted code, but will not discuss it any 
further as part of this report.
For the ABSA task we thus resort to the manually assigned aspects for both 
training and evaluation procedures. 

% ------------------------------------------------------------------------------

\subsubsection{Results}
\label{tssa_dl_results}

We assess each model's performance on the test set we have set aside, 
containing 243 observations.
The performance metrics are the same as for evaluating the standard ML solution 
in chapter \ref{tssa_ml_results}.
Table \ref{tab_res_bert} shows the results for the total of eight models 
implemented in both approaches (with SA short for sentiment analysis and overall 
best learners per task shadowed in gray):

\begin{table}[H]
  \small
  \begin{tabular}{l|r|r|r|r|r|r|r|r|}
    \cline{2-9}
    & \multicolumn{4}{c|}{\textbf{ABSA}} & \multicolumn{4}{c|}{\textbf{SA}} 
    \\ \cline{2-9} 
    &
    \multicolumn{1}{r|}{\textbf{GC}} &
    \multicolumn{1}{r|}{\textbf{GC-G}} &
    \multicolumn{1}{r|}{\textbf{GC-T}} &
    \multicolumn{1}{r|}{\textbf{GCD}} &
    \multicolumn{1}{r|}{\textbf{GC}} &
    \multicolumn{1}{r|}{\textbf{GC-G*}} &
    \multicolumn{1}{r|}{\textbf{GC-T*}} &
    \multicolumn{1}{r|}{\textbf{GCD*}} \\ 
    \hline
    \multicolumn{1}{|l|}{accuracy} & 0.893 & 0.905 & \cellcolor{lightgray} 
    \textbf{0.918} & 0.889 
    & 0.889 & 0.901 & 0.905 & \cellcolor{lightgray} \textbf{0.926} \\ 
    \hline
    \multicolumn{1}{|l|}{F1 score} & 0.803 & 0.816 & \cellcolor{lightgray} 
    \textbf{0.851} & 0.791 
    & 0.794 & 0.821 & 0.827 & \cellcolor{lightgray} \textbf{0.864} \\ 
    \hline
    \multicolumn{1}{|l|}{TN} & 164.000 & \textbf{169.000} & 
    \cellcolor{lightgray} 166.000 & 165.000 & 
    164.000 & 164.000 & 165.000 & \cellcolor{lightgray} \textbf{168.000} \\ 
    \hline
    \multicolumn{1}{|l|}{TP} & 53.000 & 51.000 & \cellcolor{lightgray} 
    \textbf{57.000} & 51.000 & 
    52.000 & 55.000 & 55.000 & \cellcolor{lightgray} \textbf{57.000} \\ 
    \hline
    \multicolumn{1}{|l|}{FN} & 14.000 & 16.000 & \cellcolor{lightgray} 
    \textbf{10.000} & 16.000 & 
    15.000 & 12.000 & 12.000 & \cellcolor{lightgray} \textbf{10.000} \\ 
    \hline
    \multicolumn{1}{|l|}{FP} & 12.000 & \textbf{7.000} & \cellcolor{lightgray} 
    10.000 & 11.000 & 
    12.000 & 12.000 & 11.000 & \cellcolor{lightgray} \textbf{8.000} \\ 
    \hline
    \end{tabular}
  \caption[Results for BERT variants]{Results for BERT variants, where \\
  GC = \texttt{bert-base-german-cased}, \\
  GC-G = \texttt{bert-base-german-cased} post-trained with GermEval data \\
  GC-T = \texttt{bert-base-german-cased} post-trained with scraped but unlabeled 
  tweets, and 
  \\
  GCD = \texttt{bert-base-german-dbmdz-cased}.\\
  Asterisks indicate additional fine-tuning with GermEval data; best performance 
  achievements are marked bold; overall best learner per task is marked gray.}
  \label{tab_res_bert}
\end{table}

The BERT-based classifiers clearly outperform the standard ML solutions and also 
achieve fairly good results in absolute terms.
For the document-level sentiment analysis, 
\texttt{bert-base-german-dbmdz-cased} with additional fine-tuning on the 
GermEval data emerges as the overall best classifier with top results across 
all metrics and 92.6\% accuracy.
Regarding the ABSA task, \texttt{bert-base-german-cased} post-trained on the 
pool of scraped but unlabeled tweets performs best with only slightly lower 
accuracy of 91.8\%.
% \newpage

The experiments confirm what the standard ML results have already suggested: 
the additional consideration of aspects, or topics, leads to a marginal increase 
of predictive power at best and even tends to worsen results.
Still, we can conclude that the application of BERT and its variants 
results in satisfactory outcome in classifying tweets into positive and negative 
categories for both document-level sentiment analysis and ABSA tasks. 
