\subsection{Data}
\label{data}

% ------------------------------------------------------------------------------

\subsubsection{Data Collection}
\label{data_collection}

The subject of our analysis are tweets by members of the German parliament
(\textit{Bundestag}) issued after the last federal election in September 
2017\footnote{
The 2017 Bundestag is comprised of 709 seats and seven political parties: the 
right-wing AfD, the Christian Democrats (CDU/CSU), the Liberals (FDP), the 
Greens, the Left Party, and the Social Democrats (SPD).
CDU/CSU and SPD as ruling parties co-exist in a grand coalition.
}.
Twitter makes these publicly accessible via its official API and the number of 
retrievable tweets per user can be exploited generously, so data supply is 
almost unrestricted.
However, with sentiment classification as ultimate goal of analysis, we face a 
major bottleneck in the need for labeled data.
Lacking the resources for large-scale annotation we did the labeling by hand.
The resulting data set, from a vast amount of available data, consists of 1,215 
observations, imposing some practical limits on the analytical scope.
\\

\textbf{Web scraping.}
For data collection from the Web we rely on the scraping procedure developed in 
the predecessor project, with minor modifications.
The process entails four steps: first, gather MPs' names and basic information 
(such as party affiliation and electoral district) from the official Bundestag 
website; second, find Twitter account names (using individual party websites as 
additional sources); 
third, acquire socioeconomic information for the time of the last 
federal election on a per-district level (available at the official website of 
the federal returning officer); and, lastly, scrape actual tweets along with 
some additional variables like the number of retweets.
We use a \texttt{Python} code base and mainly employ selenium webdrivers as well
as the \texttt{BeatifulSoup} library \citep{richardson2007} for parsing HTML
content and the \texttt{tweepy} library \citep{roesslein2020} for accessing the
official Twitter API.
For more details on the procedure and the large data base assembled in the 
predecessor project please refer to \citet{schulzewiegrebe2020}; the code is 
fully submitted in our electronic appendix and a somewhat more compact demo may 
be found among the teaching material.
\\

\textbf{Data labeling.}
In the data annotation phase we extracted a set with some tens of thousands 
of observations according to the above process and manually selected what we 
deem informative examples.
For these we assigned polarities, i.e., predicates \textit{positive} or
\textit{negative}, and also topic descriptions required for BERT's ABSA task.
We noted in the process that a large number of tweets do not appear to carry 
sentiment at all.
The resulting 1,215 training observations, originated by a total of 256 MPs, 
date from the period of October 2017 to January 2021.
In figure \ref{fig_obs_time} we detect both periodical fluctuations in the 
number of tweets over time and a general upward-sloping trend.

\begin{figure}[H]
  \includegraphics[width = \textwidth]{figures/obs_over_time}
\caption[Training data: observations over time]
{\raggedright Observations over time.}
\label{fig_obs_time}
\end{figure}

An exemplary extract from our training data with some of the most important 
variables is shown in table \ref{tab_extract}.
Exactly which features enter sentiment classification is documented in the 
subsequent chapters.

\begin{table}[H]
  \scriptsize
  \begin{tabular}{l|l|l|l|r|r|l}
  \texttt{username} & \texttt{party} & \texttt{created\_at} & \texttt{text} & 
  \texttt{followers} & \texttt{unemployment\_rate} & \texttt{label}\\
  \hline
  karl\_lauterbach & spd & 2019-12-01 09:44:00 & "Die Wahl ..." & 337001 & 8.5 & 
  negative\\
  \hline
  Martin\_Hess\_AfD & afd & 2018-08-17 07:15:00 & "Vor den ..." & 6574 & 3.5 &
  negative\\
  \hline
  BriHasselmann & gruene & 2019-09-25 15:35:00 & "Ich finde ..." & 20299 & 8.6 
  & positive\\
  \hline
  danielakolbe & spd & 2020-05-12 06:05:00 & "Aber verpflichtend ..." & 8158 & 
  8.3 & negative\\
  \hline
  JuergenBraunAfD & afd & 2020-08-13 22:05:00 & "Panik-Latif + ..." & 3188 & 
  3.4 & negative\\
  \end{tabular}
  \caption[Training data: extract]
  {\raggedright Training data extract for selected variables.}
  \label{tab_extract}
\end{table}

Figure \ref{fig_obs_party} depicts the number of observations per party both 
for our labeled training data and the larger sample from which the training data 
have been selected (containing just over 31,000 tweets).
We notice two things.
First, in either case, the share of tweets (blue) does 
not mirror the share of seats in the Bundestag (gray); most notably, the 
Christian Democrats tweet rather little, whereas the Greens are 
disproportionately active on Twitter.
Second, the right-wing AfD and the Greens are over-represented in our training 
data at the expense of the other groups.
This is simply because these two parties, in our personal experience from the
annotation process, more often issue tweets that are strongly opinionated.

\begin{figure}[H]
  \includegraphics[width = \textwidth]{figures/obs_per_party}
\caption[Training data: observations per party]
{Observations per party in labeled training data (\textit{left}) and entire 
scraped data example (\textit{right}), both depicted against seat distribution 
in current parliament.}
\label{fig_obs_party}
\end{figure}

\begin{minipage}[b]{0.5\textwidth}
  Lastly, when we inspect the class label distribution in the training data,
  an imbalance favoring the negative class becomes immediately visible: some 
  72\% of tweets have been marked as negative.
  This reflects our general impression that most tweets which do carry sentiment 
  express negative opinions and might be partly appropriated to the fact that 
  the majority of authors belong to opposition parties.
\end{minipage}%
\begin{minipage}[b]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[b]{0.45\textwidth}
  \begin{figure}[H]
    \includegraphics[width = \textwidth]{figures/class_dist}
    \caption[Training data: class label distribution]
    {\raggedright Distribution of class labels.}
    \label{fig_class_dist}
  \end{figure}
\end{minipage}%

% ------------------------------------------------------------------------------

\subsubsection{Data Pre-Processing}
\label{data_preproc}

The standard ML approach requires a lot more feature engineering 
than BERT does, but some general pre-processing steps are applied for both.
In an initial step all tweets in non-German language are excluded from the data.
We proceed with basic text cleaning, namely transcription of 
German umlauts and ligature s into standard-Latin characters and removal of 
non-informative symbols (such as those induced by ampersand conversion). 
The next block of operations is specific to Twitter data and includes the 
identification, separate storage and subsequent removal of special characters 
such as hashtags, emojis and user tags.
By this we ensure the data are available for explicit analysis but do not 
introduce noise in the text.
We finish the pre-processing procedure by assigning a unique identifier to 
each tweet.

% ------------------------------------------------------------------------------

\subsubsection{Challenges}
\label{challenges}

Text data come with many idiosyncrasies to begin with: language is highly 
diverse, irregular, and subject to constant change.
Contextual dependencies and complex constructs such as colloquialisms or sarcasm 
pose serious obstacles in NLP, besides which profanities like spelling or 
translation mistakes must be handled \citep{mohammad2017}.
Some particular properties of the data at hand add to these challenges.
\\

\textbf{Language-specific.}
Not surprisingly, most work in NLP is concerned with the analysis of 
English documents.
Although German is not a low-resource language and attracts its own share of 
research, many analyses and tools are predominantly tailored to English.
German grammar is another aspect that needs to be considered.
Syntax is heterogeneous, and inflections due to cases and genera 
result in many variations of lexical lemmata \citep{rauh2018}.
\\

\textbf{Twitter-specific.}
Tweets' brevity is arguably the most critical issue for analysis.
The limit of 280 characters means that words rarely appear more than once and 
we cannot expect many indicators of topics or sentiments in each document.
It also prompts the use of abbreviations.
On a similar note, we observe that Twitter posts often refer to certain events 
or topical entities without explicitly mentioning them, which is probably both 
due to the character limit and the real-time character of publications.
The message may be clear for an informed human annotator then but will be hard 
to grasp for machines.
Furthermore, tweets tend to be of rather informal style and use language that 
appears almost exclusively in social media, enlarging the vocabulary the 
classifier must understand.
\\

\textbf{Context-specific.}
The context of our data lessens the degree of informality somewhat; the issued 
documents are mostly political statements and as such more akin to written 
texts from other sources.
Still, the political domain introduces new vocabulary yet again and makes the 
transfer of knowledge from other contexts harder.
Lastly, as mentioned before, we find many tweets to be solely informative and 
detect an imbalance toward negative sentiment in those that do convey opinion.

% ------------------------------------------------------------------------------

\subsection{Standard Machine Learning Solution}
\label{tssa_ml}

% ------------------------------------------------------------------------------

\subsubsection{Feature Extraction}
\label{feat_ext}

Static feature extraction forms the first block of the standard ML procedure and 
provides tabular data which then enter an automated ML (AutoML) pipeline. 
We refer to these features as static because they can be calculated prior to any 
training process, depending solely on single observations.
Dynamic features, by contrast, are computed globally across several 
observations, which requires strict separation between train and test sphere in 
their computation.
\\

\textbf{\textit{Static features}} \\

The static part of features is based on the so-called \textit{bag-of-words 
(BOW)} assumption that leads to texts being treated as arbitrary collections of 
vocabulary instances.
In particular, information about grammar and word order is discarded in BOW 
approaches.
This is obviously a strong simplification but hard to avoid entirely with 
standard classifiers \citep{cambriaetal2017}.

The static feature extraction steps rely heavily on \texttt{R}'s 
\texttt{quanteda} package \citep{pkgquanteda} for organizing documents in corpus 
objects, tokenizing texts and performing look-ups with dictionaries.
We use lists of stopwords in multiple places to exclude uninformative 
tokens such as determiners or auxiliary verbs.
These are compilations from various sources, including \texttt{quanteda}'s 
built-in list, open-source data available on 
\href{https://github.com/stopwords-iso/stopwords-de}{GitHub} and some manually 
appended, domain-specific terms. 
In addition, we repeatedly apply \textit{stemming} in order to reduce words to 
their root form (for instance, cutting back both \textit{"works"} and 
\textit{"working"} to \textit{"work"}).

All of these operations are aimed at compressing document representation from 
the total of unique original terms to more generalized tokens that co-occur 
across texts
This way we create features that are actually shared by multiple observations 
and thus help classifiers infer feature-target relations with the ability to 
generalize.
For the static part we exploit insights from other studies 
(e.g., \citet{balyetal2017}, \citet{correaetal2017}, \citet{jabreelmoreno2017}, 
\citet{sidarenka2019})
and include the following features: 

\begin{enumerate}

  \item \textbf{Lexicon-based polarity counts.} These comprise two sources of 
  prior polarity, namely words and emojis. 
  We use the aggregate of three large German polarity-term dictionaries, 
  \textit{GlobalPolarityClues} \citep{waltinger2010}, \textit{SentiWS} 
  made available \href{https://wortschatz.uni-leipzig.de/en/download}{online}
  by Leipzig university and a collection by \citet{rauh2018}, for word 
  polarities.
  The resulting lexicon distinguishes weak and strong sentiment.
  In each tweet, if a word is part of the dictionary and judged to have 
  either positive or negative connotation (weak or strong), it raises the 
  respective polarity count by one.
  Emojis are treated analogously, albeit without the discrimination between 
  degrees of sentiment, using an emoji polarity list accumulated by 
  \citet{kraljetal2015}.
  
  \item \textbf{Twitter variables.} We make use of the additional information 
  provided by the Twitter API and note for each tweet the number of likes and 
  retweets as well as the number of users tagged.
  For hashtags we resort to the same naive measure since we find them to be so 
  heterogeneous across documents in our training set that no meaningful 
  information can be extracted. 
  This certainly marks an opportunity for future improvement.
  
  \item \textbf{Syntactic features.} This group attempts to mitigate the 
  simplification introduced by the BOW assumption to some extent.
  
\end{enumerate}

\textbf{\textit{Dynamic features}} \\

Foo
\\

\textbf{Structural Topic Model.}
Foo
\\

\textbf{Word Embeddings.}
Foo
\\

% ------------------------------------------------------------------------------

\subsubsection{Sentiment Classifiers}
\label{classifiers}

\textbf{Random Forests.}
foo
\\

\textbf{Regularized Logistic Regression.}
foo
\\

% ------------------------------------------------------------------------------

\subsubsection{Automated Machine Learning Pipeline}
\label{pipeline}

The design of the AutoML pipeline as a graph learner ensures that training and 
test sets remain completely walled off from one another at each resampling step.

% ------------------------------------------------------------------------------

\subsubsection{Results}
\label{tssa_ml_results}

% ------------------------------------------------------------------------------

\subsection{Deep Learning Solution}
\label{tssa_dl}

% ------------------------------------------------------------------------------

\subsubsection{Methodology}
\label{tssa_dl_method}

\textbf{Deep transfer learning} \\

foo
\\

\textbf{BERT} \\

foo

% ------------------------------------------------------------------------------

\subsubsection{Results}
\label{tssa_dl_results}