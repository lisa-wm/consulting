{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the official Bundestag website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import urllib.request\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# see https://stackoverflow.com/questions/2134706/hitting-maximum-recursion-depth-using-pickle-cpickle/2135176#2135176\n",
    "import sys\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Simon\\\\OneDrive\\\\Uni\\\\LMU\\\\SS 2020\\\\Statistisches Consulting\\\\Bundestag-MP-Analyse\\\\data\\\\web_scraping'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up working directory\n",
    "os.path.abspath(os.getcwd()) # initial working directory (should be equal to source file directory if using Jupyter Notebook)\n",
    "os.chdir('../../data/web_scraping') # change to directory where all data files are stored\n",
    "# check working directory\n",
    "os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up selenium web driver\n",
    "chdriver_path = r'C:\\Users\\Simon\\chromedriver.exe' # download chromedriver, save locally\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')\n",
    "driver = webdriver.Chrome(chdriver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize selenium web driver\n",
    "website = \"https://www.bundestag.de/abgeordnete\"\n",
    "driver.get(website)\n",
    "\n",
    "# switching to list view\n",
    "element = driver.find_element_by_class_name('icon-list-bullet') # find list button\n",
    "webdriver.ActionChains(driver).move_to_element(element).click(element).perform() # click\n",
    "time.sleep(random.randint(5,10)) # wait for list view to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many MPs are listed on the website (includes dropouts and successors)\n",
    "len(driver.find_element_by_class_name('bt-list-holder').find_elements_by_tag_name('li'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up a dictionary with MP name and party only (from list)\n",
    "\n",
    "abg_dict = {}\n",
    "abg_count = 0\n",
    "for link in driver.find_element_by_class_name('bt-list-holder').find_elements_by_tag_name('li'):\n",
    "    name = link.find_element_by_class_name('bt-teaser-person-text').find_element_by_tag_name('h3').text\n",
    "    partei = link.find_element_by_class_name('bt-teaser-person-text').find_element_by_tag_name('p').text\n",
    "    value = [name, partei] # value: name & partei\n",
    "    abg_dict[abg_count] = value\n",
    "    abg_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for each MP, retrieve all information (except name and party) from individual page\n",
    "# note that timeout errors might occur when waiting for a site to load\n",
    "# in that case, rerun the loop below for the missing parts, then append to output already generated\n",
    "for abg in abg_dict:\n",
    "    \n",
    "    # (re-)load list view (for all iterations)\n",
    "    driver.get(website)\n",
    "    element = driver.find_element_by_class_name('icon-list-bullet')\n",
    "    webdriver.ActionChains(driver).move_to_element(element).click(element).perform() # click to change to list view\n",
    "    time.sleep(random.randint(8,10)) # wait for list view to load\n",
    "    \n",
    "    driver.find_element_by_class_name('bt-list-holder').find_elements_by_tag_name('li')[abg].click() # click to open individual page\n",
    "    time.sleep(random.randint(8,10)) # wait for page to load\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml') # convert page to soup\n",
    "    \n",
    "    # extract election mode (wahlart), state (bundesland), and electoral district (wahlkreis)\n",
    "    wahlart = soup.find_all('h4')[4].text\n",
    "    bundesland = soup.find('div', attrs = {'class': 'col-xs-12 col-sm-6 bt-standard-content'}).h5.text\n",
    "    wahlkreis = soup.find('div', attrs = {'class': 'col-xs-12 col-sm-6 bt-standard-content'}).a.text \\\n",
    "        if soup.find('div', attrs = {'class': 'col-xs-12 col-sm-6 bt-standard-content'}).a is not None else \"n.a.\"   \n",
    "\n",
    "    # extract committee memberships (ausschuesse)\n",
    "    ausschuesse = {}\n",
    "\n",
    "    ## keys: position within committee (e.g., ordentliches mitglied, stellvertretendes mitglied)\n",
    "    keys = []\n",
    "    for heading in soup.find_all(class_ = 'bt-collapse-padding-bottom')[4].find_all('h5'):\n",
    "        key = heading.text.strip('\\n').strip() \n",
    "        keys.append(key)\n",
    "\n",
    "    ## values: the actual committees\n",
    "    values = []\n",
    "    for ul in soup.find_all(class_ = 'bt-collapse-padding-bottom')[4].find_all('ul'):\n",
    "        entries = []\n",
    "        for x in ul.find_all('a'):\n",
    "            entries.append(x['title'])\n",
    "        values.append(entries)\n",
    "\n",
    "    ## in case of memberships in other (non-Bundestag) bodies\n",
    "    if len(soup.find_all('h4', string = 'Mitgliedschaften in sonstigen Gremien')) == 1:\n",
    "        for heading in soup.find_all(class_ = 'bt-collapse-padding-bottom')[5].find_all('h5'):\n",
    "            interim_key = heading.text.strip('\\n').strip()\n",
    "            key = f'{interim_key} (sonstige Gremien)'\n",
    "            keys.append(key)\n",
    "\n",
    "        for ul in soup.find_all(class_ = 'bt-collapse-padding-bottom')[5].find_all('ul'):\n",
    "            entries = []\n",
    "            for x in ul.find_all('a'):\n",
    "                entries.append(x['title'])\n",
    "            values.append(entries)\n",
    "    \n",
    "    if len(soup.find_all('h4', string = 'Funktion')) == 0:\n",
    "        for i in range(len(keys)):\n",
    "            ausschuesse[keys[i]] = values[i]\n",
    "    else:\n",
    "        for i in range(len(keys)):\n",
    "            ausschuesse[keys[i]] = []  \n",
    "\n",
    "    # exctract social media accounts(soziale medien)\n",
    "    social_media = {}\n",
    "\n",
    "    if len(soup.find_all('h5', string = 'Profile im Internet')) == 1:\n",
    "        for link in soup.find_all(class_ = 'bt-linkliste')[0].find_all('a'):\n",
    "            social_media[link['title']] = link.get('href')\n",
    "\n",
    "    # extract biography (briografie)   \n",
    "    biografie = str()\n",
    "    for p in soup.find(class_ = 'bt-collapse-padding-bottom').find_all('p'):\n",
    "        biografie += p.text.strip('\\n').replace(u'\\xa0', u' ')\n",
    "\n",
    "    abg_dict[abg].append(wahlart)\n",
    "    abg_dict[abg].append(bundesland)\n",
    "    abg_dict[abg].append(wahlkreis)\n",
    "    abg_dict[abg].append(ausschuesse)\n",
    "    abg_dict[abg].append(social_media)\n",
    "    abg_dict[abg].append(biografie)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save raw MP dictionary\n",
    "with open('abg_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(abg_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into dataframe, add headers\n",
    "df = pd.DataFrame(abg_dict).transpose()\n",
    "df.columns = ['Name', 'Partei', 'Wahlart', 'Bundesland', 'Wahlkreis', 'Ausschuesse', 'Soziale Medien', 'Biografie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split Wahlkreis into number and name\n",
    "df['Wahlkreis-Nr.'] = df['Wahlkreis'].apply(lambda x: int(x.split(':')[0].strip('Wahlkreis').strip('')) if x != \"n.a.\" else \"\")\n",
    "df['Wahlkreis'] = df['Wahlkreis'].apply(lambda x: x.split(':')[1].strip(' ') if x != \"n.a.\" else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "columns_titles = ['Name', 'Partei', 'Wahlart', 'Bundesland', 'Wahlkreis', 'Wahlkreis-Nr.', 'Ausschuesse', 'Soziale Medien', 'Biografie']\n",
    "df=df.reindex(columns=columns_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving final dataframe\n",
    "with open('abg_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(abg_df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
