######################################################################## 
################################# BERT ################################# 
########################################################################

Pretrained models available in transformers huggingface: 

bert-base-german-cased (trained by Deepset.ai)
bert-base-german-dbmdz-cased (trained by by DBMDZ)
bert-base-german-dbmdz-uncased (trained by by DBMDZ)
distilbert-base-german-cased (distilled from DBMDZ)


-> for uncased models no difference whether lower cased preprocessing or not. 
-> for cased models we have to do without .lower() in the preprocessing step


## Try to understand every single step ##

1. preprocess your data using

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
encoded_input = tokenizer("Hello, I'm a single sentence!")
print(encoded_input)

--> Automatically downloads the vocab used during pretraining a given model 
--> First split a given text in tokens: words (or part of words, punctuation symbols, etc.) 
--> Then convert those tokens into numbers, to be able to build a tensor out of them & feed them to the model. 
--> Output: a dictionary string to list of ints {input_ids, token_type_ids, attention_mask}; in case of several sentences we get LISTS OF LISTS of ints as output.

- input_ids are the indices corresponding to each token in our sentence; 
- attention_mask points out which tokens the model should pay attention to and which ones it should not (because they represent padding in this case)
-  token_type_ids are for: they indicate to the model which part of the inputs correspond to the first sentence and which part corresponds to the second sentence (if we are interested in pairs of sentences, i.e., if you want to classify if two sentences in a pair are similar, or for question-answering models


--> To double-check what is fed to the model: as these numbers (tensors) are unique we can decode the tokenization and come back to our initial sentence
tokenizer.decode(encoded_input["input_ids"]) 
"[CLS] Hello, I'm a single sentence! [SEP]"

--> padding=True, truncation=True, return_tensors="pt" 
- To pad each sentence to the maximum length there is in your batch.
- To truncate each sentence to the maximum length the model can accept (if applicable).
- To return tensors.

FINALLY: You can automatically pad your inputs to the maximum sentence length in the batch, truncate to the maximum length the model can accept and return tensors directly with the following:
tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors="pt")


--> The tokenizer also accept pre-tokenized inputs: is_split_into_words=True


2. Training and fine-tuning: 

When we instantiate a model with from_pretrained(), the model configuration and pre-trained weights of the specified model are used to initialize the model.

BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
--> Creates a BERT model instance with encoder weights copied from the bert-base-uncased model and a randomly initialized sequence classification layer on top of the encoder with an output size of 2.

--> Use PyTorch optimizer AdamW() to do bias correction and weight decay
from transformers import AdamW
optimizer = AdamW(model.parameters(), lr=1e-5)


from transformers import BertForSequenceClassification, Trainer, TrainingArguments

model = BertForSequenceClassification.from_pretrained("bert-large-uncased")

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total # of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)

trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=test_dataset            # evaluation dataset
)



3. Evaluate: 


from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }


## ALL IN ALL: 

- Pretraining: 'learns' embedding representations after you take the output -> BERT perfoms ‘feature extraction’ here
& use the learnt parameters (weights, bias) of a pre-trained network to a new task -> take the internal representation of all or some of BERT's layers

- Finetuning: text classification task by adding an output layer or layers to pretrained BERT and retraining the whole (both the classification layers and BERT's layers). Like optimization. We optimize the network to achieve the optimal results. May be we can change the number of layers used, learning rate and we have many parameters of the model to optimize






























