{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NALMPI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Normalization Function: \n",
    "\n",
    "# Does tokenization, lemmatization, filters characters only, removes stopwords, \n",
    "# converts to lower case, handels umlauts - all optional\n",
    "\n",
    "# Input: Tweets as array and the options listed above\n",
    "# Output: Normalized tweets as array\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt') \n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pattern.de import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from germalemma import GermaLemma\n",
    "lemmatizer = GermaLemma()\n",
    "\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('german')\n",
    "stopword_list = stopword_list + ['frau', 'herr', 'komme', 'go', 'get',\n",
    "                                 'tell', 'listen', 'ein', 'two', 'three',\n",
    "                                 'vier', 'fünf', 'sechs', 'sieben', 'acht',\n",
    "                                 'neun', 'null', 'dass']\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text, language='german') \n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('ADJ...'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V...'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N...'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('ADV...'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "    return tagged_lower_text\n",
    "    \n",
    "# lemmatize text based on POS tags    \n",
    "    \n",
    "from germalemma import GermaLemma\n",
    "lemmatizer = GermaLemma()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [lemmatizer.find_lemma(word, pos_tag) if pos_tag\n",
    "                         else word                     \n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens]) #pattern.sub(replacement,string), if pattern found then replace with replacement, if not found return string unchanged\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "    \n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "def keep_text_characters(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenize_text(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def remove_repeatition(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text) # if a letter occurs more than once\n",
    "\n",
    "# Replace umlauts for a given text\n",
    "def umlauts(text):\n",
    "    tempVar = text # local variable\n",
    "    \n",
    "    # Using str.replace() \n",
    "    \n",
    "    tempVar = tempVar.replace('ä', 'ae')\n",
    "    tempVar = tempVar.replace('ö', 'oe')\n",
    "    tempVar = tempVar.replace('ü', 'ue')\n",
    "    tempVar = tempVar.replace('Ä', 'Ae')\n",
    "    tempVar = tempVar.replace('Ö', 'Oe')\n",
    "    tempVar = tempVar.replace('Ü', 'Ue')\n",
    "    tempVar = tempVar.replace('ß', 'ss')\n",
    "    \n",
    "    return tempVar\n",
    "\n",
    "\n",
    "def normalize_tweet(tweet, lemmatize=True, \n",
    "                     only_text_chars=True,\n",
    "                     tokenize=False, \n",
    "                     stopwords_removal = True,\n",
    "                     lower_case = False,\n",
    "                     umlauts_removal = True): #\n",
    "    \n",
    "    normalized_tweet = []    \n",
    "    for index, text in enumerate(tweet):\n",
    "        if lemmatize:\n",
    "            text = lemmatize_text(text) # ok\n",
    "        if lower_case:\n",
    "            text = text.lower()\n",
    "        text = remove_special_characters(text) # ok\n",
    "        if stopwords_removal:\n",
    "            text = remove_stopwords(text) # ok\n",
    "        if umlauts_removal:\n",
    "            text = umlauts(text)\n",
    "        if only_text_chars:\n",
    "            text = keep_text_characters(text) # ok  \n",
    "        if tokenize:\n",
    "            text = tokenize_text(text) # ok\n",
    "            normalized_tweet.append(text)\n",
    "        else:\n",
    "            normalized_tweet.append(text)\n",
    "            \n",
    "    return normalized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction Function\n",
    "\n",
    "# Works basically for BoW, Tf-IDF and different n-grams (or n-gram-ranges)\n",
    "\n",
    "# Input: Tweets as arrays, type of feature extraction, ngram_range, \n",
    "# max_df / min_df (ignore terms that have a document frequency strictly higher\n",
    "# /lower than the given threshold)\n",
    "\n",
    "# Output: Vectorizer and feature matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def build_feature_matrix(documents, feature_type='frequency',\n",
    "                         ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n",
    "\n",
    "    feature_type = feature_type.lower().strip()  \n",
    "    \n",
    "    if feature_type == 'binary':\n",
    "        vectorizer = CountVectorizer(binary=True, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "    elif feature_type == 'frequency':\n",
    "        vectorizer = CountVectorizer(binary=False, min_df=min_df,\n",
    "                                     max_df=max_df, ngram_range=ngram_range, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "    elif feature_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, \n",
    "                                     ngram_range=ngram_range, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "    else:\n",
    "        raise Exception(\"Wrong feature type entered. Possible values: 'binary', 'frequency', 'tfidf'\")\n",
    "\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    \n",
    "    return vectorizer, feature_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares arrays for both relevant variables (tweets and labels) for later \n",
    "# training and easier handling\n",
    "\n",
    "def prepare_arrays(dataset):\n",
    "    train_tweets = np.array(dataset[\"full_text\"])\n",
    "    train_labels_binary = np.array(dataset[\"label_binary\"])\n",
    "    \n",
    "    return train_tweets, train_labels_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document frequency matrix after feature extraction\n",
    "\n",
    "def get_dfm(features, names):\n",
    "    df = pd.DataFrame(data = features, columns = names)\n",
    "    df_trans = df.T\n",
    "    return df_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
