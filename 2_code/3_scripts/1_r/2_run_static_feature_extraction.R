# ------------------------------------------------------------------------------
# STATIC FEATURE EXTRACTION
# ------------------------------------------------------------------------------

# IN: corpus object of cleaned tweets and meta data
# OUT: corpus object of cleaned tweets, meta data and static features

# EXTRACT TWITTER-SPECIFIC FEATURES --------------------------------------------

load_rdata_files(tweets_corpus, folder = "2_code/1_data/2_tmp_data")

tweets_features_twitter <- convert_qtda_to_dt(
  tweets_corpus,
  key = "doc_id")

tweets_features_twitter <- tweets_features_twitter[
  , .(doc_id, hashtags, tags, retweet_count, favorite_count)
  ][, `:=` (
    n_hashtags = lengths(hashtags),
    n_tags = lengths(tags)),
    by = doc_id
    ][, `:=` (hashtags = NULL, tags = NULL)]

save_rdata_files(tweets_features_twitter, folder = "2_code/1_data/2_tmp_data")

# TODO find sth to do w/ hashtags

# EXTRACT DICTIONARY-BASED FEATURES --------------------------------------------

# Get polarity dictionaries from various sources
# TODO check whether strong weak is so beneficial, rauh does not fit in

dict_german_polarity_clues <- get_dict_gpc()
dict_sentiws <- get_dict_sentiws()
dict_rauh <- get_dict_rauh()

# Create global dictionary

dict_global <- quanteda::dictionary(
  list(
    positive_global_strong = unique(c(
      dict_german_polarity_clues$positive[polarity_degree == "strong", term], 
      dict_sentiws$positive[polarity_degree == "strong", term])),
    negative_global_strong = unique(c(
      dict_german_polarity_clues$negative[polarity_degree == "strong", term], 
      dict_sentiws$negative[polarity_degree == "strong", term])),
    positive_global_weak = unique(c(
      dict_german_polarity_clues$positive[polarity_degree == "weak", term],
      dict_sentiws$positive[polarity_degree == "weak", term],
      dict_rauh[polarity == "positive", term])),
    negative_global_weak = unique(c(
      dict_german_polarity_clues$negative[polarity_degree == "weak", term],
      dict_sentiws$negative[polarity_degree == "weak", term],
      dict_rauh[polarity == "negative", term]))))

# Create tokens object (keeping case and punctuation for the moment, will be
# needed later on)

tweets_tokens_basic <- quanteda::tokens(
  tweets_corpus,
  what = "word",
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_separators = TRUE,
  split_hyphens = TRUE,
  include_docvars = TRUE)

tweets_tokens_basic <- quanteda::tokens_wordstem(
  tweets_tokens_basic, 
  language = "german")

save_rdata_files(tweets_tokens_basic, folder = "2_code/1_data/2_tmp_data")

# Customize stopwords list, excluding some potentially sentiment-bearing words

stopwords_sfe <- stringr::str_remove_all(
  make_stopwords(),
  stringr::str_c(unique(SnowballC::wordStem(remove_umlauts(tolower(c(
    "gegen",
    "kein",
    "nicht",
    "sehr",
    "besser",
    "beste",
    "gut",
    "gern",
    "kaum",
    "nein",
    "nie",
    "niemand",
    "richtig",
    "schlecht"))),
    language = "de")),
    collapse = "|"))

stopwords_sfe <- stopwords_sfe[nchar(stopwords_sfe) > 0]

# Create dfm object

tweets_tokens_sfe <- quanteda::tokens_remove(
  quanteda::tokens_tolower(tweets_tokens_basic),
  pattern = stopwords_sfe) 

tweets_tokens_sfe <- quanteda::tokens_remove(
  tweets_tokens_sfe,
  pattern = "[[:punct:]]",
  valuetype = "regex")

tweets_dfm_sfe <- quanteda::dfm(tweets_tokens_sfe)

save_rdata_files(tweets_tokens_sfe, folder = "2_code/1_data/2_tmp_data")
save_rdata_files(tweets_dfm_sfe, folder = "2_code/1_data/2_tmp_data")

# Match with polarities

tweets_sentiments_global <- convert_qtda_to_dt(
  quanteda::dfm_lookup(tweets_dfm_sfe, dict_global),
  key = "doc_id")

# ------------------------------------------------------------------------------

# Read emoji ranking data

emojis_ranking <- data.table::fread(
  here("2_code/1_data/0_external_data", "emojis-sentiment-ranking.csv"),
  drop = c(1, 3, 4, 8, 9), 
  col.names = c(
    "unicode", 
    "negative", 
    "neutral",
    "positive"),
  encoding = "UTF-8")

# Transform to appropriate format and convert to int representation to match
# with tweets

emojis_ranking[
  , principal_emotion := which.max(.SD),
  .SDcols = c("negative", "neutral", "positive"),
  by = seq_len(nrow(emojis_ranking))
  ][, polarity := data.table::fcase(
    principal_emotion == 1, "negative",
    principal_emotion == 2, "neutral",
    principal_emotion == 3, "positive")
    ][, unicode := as.character(utf8ToInt(intToUtf8(unicode)))]

dict_emojis <- quanteda::dictionary(
  list(
    positive_emojis = emojis_ranking[polarity == "positive", unicode],
    negative_emojis = emojis_ranking[polarity == "negative", unicode]))

# Find tweets using emojis and convert those to integer representation

data_dt <- data.table::as.data.table(
  cbind(
    doc_id = quanteda::docid(tweets_corpus),
    quanteda::docvars(tweets_corpus)), 
  key = "doc_id")

tweets_emojis <- data_dt[
  , .(doc_id, emojis)
  ][lengths(emojis) > 0
    ][, emojis := paste(unlist(emojis), collapse = " "), by = doc_id
      ][, emojis := as.character(emojis)
        ][, emojis := paste(as.character(
          sapply(unlist(stringr::str_split(emojis, " ")), utf8ToInt)),
          collapse = " "),
          by = doc_id
          ][, n_emojis := stringr::str_count(emojis, "\\w+"), by = doc_id]

# Convert to dfm object

tweets_corpus_emojis <- quanteda::corpus(
  tweets_emojis, 
  docid_field = "doc_id",
  text_field = "emojis")

tweets_dfm_emojis <- quanteda::dfm(tweets_corpus_emojis)

# TODO look into that - non-matches veritable non-matches?

tweets_sentiments_emojis <- convert_qtda_to_dt(
  quanteda::dfm_lookup(tweets_dfm_emojis, dict_emojis),
  key = "doc_id")

tweets_sentiments_emojis <- 
  tweets_sentiments_emojis[tweets_emojis[, .(doc_id, n_emojis)]]

# ------------------------------------------------------------------------------

# Combine dictionary-based features

tweets_features_dict <- tweets_sentiments_emojis[
  tweets_sentiments_global, on = "doc_id"
  ][, c("positive_emojis", "negative_emojis", "n_emojis") :=
      lapply(.SD, function(i) {ifelse(is.na(i), 0, i)}),
    .SDcols = c("positive_emojis", "negative_emojis", "n_emojis"),
    by = "doc_id"]

save_rdata_files(tweets_features_dict, folder = "2_code/1_data/2_tmp_data")

# EXTRACT LEXICAL FEATURES -----------------------------------------------------

# Define patterns for negations, intensifications, punctuations, repetitions

tokens_negation <- SnowballC::wordStem(
  remove_umlauts(c(
    "nicht", 
    "nie", 
    "niemals", 
    "nein", 
    "niemand", 
    "nix", 
    "nirgends", 
    "kein")),
  language = "de")

tokens_intensification <- SnowballC::wordStem(
  remove_umlauts(c(
    "sehr", 
    "besonders", 
    "total", 
    "absolut", 
    "vÃ¶llig", 
    "enorm", 
    "maximal")),
  language = "de")

tokens_punctuation <- c(
  dotdotdot = "[.]{3}", 
  exclamation_mark_single = "!", 
  question_mark_single = "\\?", 
  exclamation_mark_rep = "[!]{2}", 
  question_mark_rep = "[\\?]{2}")

tokens_repetition <- c(
  repeated_char = "(.)\\1{2}", 
  repeated_char_seq = "\\b(\\S+?)\\1\\S*\\b")

# Match patterns

tweets_negation <- convert_qtda_to_dt(
  quanteda::dfm_match(tweets_dfm_sfe, tokens_negation),
  key = "doc_id")[
    , n_negations := sum(.SD),
    .SDcols = -c("doc_id"),
    by = doc_id
    ][, .(doc_id, n_negations)]

tweets_intensification <- convert_qtda_to_dt(
  quanteda::dfm_match(tweets_dfm_sfe, tokens_intensification),
  key = "doc_id")[
    , n_intensifications := sum(.SD),
    .SDcols = -c("doc_id"),
    by = doc_id
    ][, .(doc_id, n_intensifications)]

tweets_punctuation <- convert_qtda_to_dt(
  quanteda::dfm_match(tweets_dfm_sfe, tokens_punctuation),
  key = "doc_id")
data.table::setnames(tweets_punctuation, c("doc_id", names(tokens_punctuation)))

tweets_repetition <- convert_qtda_to_dt(
  quanteda::dfm_match(tweets_dfm_sfe, tokens_repetition),
  key = "doc_id")
data.table::setnames(tweets_repetition, c("doc_id", names(tokens_repetition)))

# Get character unigrams

tweets_char_unigrams <- quanteda::tokens(
  tweets_corpus,
  what = "character",
  remove_punct = TRUE,
  remove_symbols = TRUE,
  remove_numbers = TRUE,
  remove_separators = TRUE,
  split_hyphens = TRUE) 

tweets_char_unigrams <- convert_qtda_to_dt(
  quanteda::dfm(tweets_char_unigrams),
  key = "doc_id")

data.table::setcolorder(tweets_char_unigrams, c("doc_id", letters))

# EXTRACT POS TAGS -------------------------------------------------------------

if (FALSE) spacyr::spacy_install()
if (FALSE) spacyr::spacy_download_langmodel("de")

if (FALSE) {
  
  spacyr::spacy_initialize(model = "de_core_news_sm")
  
  tweets_corpus_tagged <- data.table::as.data.table(
    spacyr::spacy_parse(
      tweets_corpus,
      lemma = FALSE,
      entity = FALSE),
    key = "doc_id")
  
  save_rdata_files(tweets_corpus_tagged, folder = "2_code/1_data/2_tmp_data")
  
}

load_rdata_files(tweets_corpus_tagged, folder = "2_code/1_data/2_tmp_data")

tweets_pos_tags <- tweets_corpus_tagged[
  , .(doc_id, pos)
  ][, aux := 1L
    ][, n_tags := sum(aux), by = list(doc_id, pos)
      ][, aux := NULL]

tweets_pos_tags <- data.table::dcast(
  unique(tweets_pos_tags), 
  doc_id ~ pos, 
  value.var = "n_tags",
  fun.aggregate = sum)

data.table::setnames(tweets_pos_tags, tolower(names(tweets_pos_tags)))

tweets_pos_tags <- tweets_pos_tags[
  , .(doc_id, adj, adv, cconj, noun, propn, verb)]

# COLLECT ALL STATIC FEATURES --------------------------------------------------

tweets_features_static <- tweets_pos_tags[
  tweets_negation
  ][tweets_intensification,
    ][tweets_punctuation,
      ][tweets_repetition,
        ][tweets_char_unigrams, 
          ][tweets_features_twitter, 
            ][tweets_features_dict, ]

save_rdata_files(tweets_features_static, folder = "2_code/1_data/2_tmp_data")

quanteda::docvars(tweets_corpus) <- as.data.frame(
  tweets_features_static[data_dt, on = "doc_id"])

save_rdata_files(tweets_corpus, folder = "2_code/1_data/2_tmp_data")
