{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Fine-Tuning_pure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09FrEZkpUv0F"
      },
      "source": [
        "# BERT Fine-Tuning (pure)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WrhLjZdVN49"
      },
      "source": [
        "*By Asmik Nalmpatian and Lisa Wimmer*\n",
        "\n",
        "*Last edited on 30.12.20* \n",
        "\n",
        "*For our consulting project: Aspect-Based Sentiment Analysis for Twitter Data of German MPs*\n",
        "\n",
        "*Methodology based on: https://arxiv.org/pdf/1810.04805.pdf*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxj_Yq-iWJxb"
      },
      "source": [
        "This notebook is supposed to give an overview over the used functionalities. We will show how to use BERT - Bidirectional Encoder Representations from Transformers - with PyTorch library (huggingface) to fine-tune a pretrained model in tweet classification. \n",
        "\n",
        "The following pretrained BERT models can be used and are available in transformers huggingface: \n",
        "\n",
        "*   *bert-base-german-cased (trained by Deepset.ai)*\n",
        "*   *bert-base-german-dbmdz-cased (trained by by DBMDZ)*\n",
        "*   *bert-base-german-dbmdz-uncased (trained by by DBMDZ)*\n",
        "*   *distilbert-base-german-cased (distilled from DBMDZ)*\n",
        "\n",
        "After using these models to extract (hopefully) high quality features from our text data, we aim to fine-tune them on our specific task using a manually labeled sample of German tweets to gain state of the art predictions.\n",
        "\n",
        "All in all the tweets will be classified into *positive* and *negative* classes using a pretrained BERT model. We will take it, add an untrained layer of neurons on the end and train a new model specifically for classification task. \n",
        "Advantages of Fine-Tuning:  \n",
        "\n",
        "\n",
        "*   Quick because we have already hardly pretrained layers of the network (only 2-4 epochs after adding 1 fully-connected layer on top are enough to train as the authors recommend)\n",
        "*   Less data is sometimes enough to achieve good performance\n",
        "*   Usually preferable results: Because of task-specific adjustments of the weights\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGC297qbdzRf"
      },
      "source": [
        "**Prepare GPU:**\n",
        "\n",
        "1. Check: Edit --> Notebook settings -> Hardware accelerator -> *GPU* \n",
        "\n",
        "\n",
        "2. Datasets are uploaded in *content*-folder: *data-germeval-2017-train.tsv* & *labeling_asmik_final.csv*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26YQ5UYt1aGw"
      },
      "source": [
        "# Google Colab GPU Connection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY5QDtKc4pCe"
      },
      "source": [
        "Otherwise training a large NN may take a very long time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpojrKi87B0j",
        "outputId": "c9308566-3bba-4458-f2ad-b85920de5cce"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLk647UpAUMZ",
        "outputId": "8e3cf5e1-a932-4a63-bc52-cdaaa77f1d50"
      },
      "source": [
        "# Identify and specify the GPU as the device\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB8ju8Ue_wNb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec85bdda-6168-449d-ce1b-0a9e40df6c04"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luQSEYNNUbqZ"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-fpUHgyTOUuW",
        "outputId": "b298fa91-0971-4f69-82b8-1af6a7ba3562"
      },
      "source": [
        "# Where to save our data\n",
        "import os, sys\n",
        "\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65Bv29YjMpNq",
        "outputId": "272edea2-20a7-4855-8512-55e11a3a7dd8"
      },
      "source": [
        "# # Clear RAM for more memory\n",
        "#!nvidia-smi\n",
        "#!kill 460.27.04"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: kill: 460.27.04: arguments must be process or job IDs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLhki6Q67HB5"
      },
      "source": [
        "# Fine-Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV4Ao1h59Tbj"
      },
      "source": [
        "Install Huggingface Library / transformers package and specify the pretrained transformer model. (Uncased means that the texts have only lowercase letters)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ynJrSjrQLQu"
      },
      "source": [
        "#!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "pretrained_model = \"./drive/MyDrive/BERT_Files/tweets_unlabeled_pt/\" # \"bert_german_cased\"\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained(pretrained_model) # , do_lower_case=True for uncased models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKXUHB8CIoNV"
      },
      "source": [
        "Load and prepare the tweets and labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ejwgMfT7D-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "34fe2c7e-04e2-493f-89ad-2e60f77955f9"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./drive/MyDrive/BERT_Files/BERT_Fine-Tuning_pure/data_labeled_processed_bert.csv\" , delimiter=',', header = 0)\n",
        "\n",
        "df['label_binary'] = [1 if label == 'positive' else 0 for label in df.label]\n",
        "\n",
        "df = df.dropna(subset=['label'])\n",
        "\n",
        "# Get the lists of tweets and their labels.\n",
        "tweets = df.twitter_full_text.values\n",
        "labels = df.label_binary.values\n",
        "\n",
        "df.head()\n",
        "#len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>twitter_username</th>\n",
              "      <th>twitter_available</th>\n",
              "      <th>twitter_created_at</th>\n",
              "      <th>twitter_full_text_topic</th>\n",
              "      <th>twitter_full_text</th>\n",
              "      <th>twitter_retweet_count</th>\n",
              "      <th>twitter_favorite_count</th>\n",
              "      <th>twitter_followers_count</th>\n",
              "      <th>twitter_location</th>\n",
              "      <th>twitter_word_count</th>\n",
              "      <th>twitter_year</th>\n",
              "      <th>twitter_month</th>\n",
              "      <th>twitter_week</th>\n",
              "      <th>twitter_time_index_month</th>\n",
              "      <th>twitter_time_index_week</th>\n",
              "      <th>twitter_emojis</th>\n",
              "      <th>twitter_hashtags</th>\n",
              "      <th>twitter_tags</th>\n",
              "      <th>label</th>\n",
              "      <th>topic</th>\n",
              "      <th>from</th>\n",
              "      <th>to</th>\n",
              "      <th>label_binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABaerbock15156546001</td>\n",
              "      <td>ABaerbock</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-01-11T07:10:00Z</td>\n",
              "      <td>Habe mir das Gro Ko-Sondierungspapier zu Klima...</td>\n",
              "      <td>Habe mir das Gro Ko-Sondierungspapier zu Klima...</td>\n",
              "      <td>111</td>\n",
              "      <td>233</td>\n",
              "      <td>107693</td>\n",
              "      <td>Brandenburg</td>\n",
              "      <td>32</td>\n",
              "      <td>2018</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#GroKo|#Klima|#Klimadiplomatie|#Merkel|#kohlea...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>negative</td>\n",
              "      <td>Klimapolitik</td>\n",
              "      <td>239</td>\n",
              "      <td>251</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ABaerbock15210084001</td>\n",
              "      <td>ABaerbock</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-03-14T06:20:00Z</td>\n",
              "      <td>Auch weltweit sieht man, dass Angela Merkel s ...</td>\n",
              "      <td>Auch weltweit sieht man, dass Angela Merkel s ...</td>\n",
              "      <td>24</td>\n",
              "      <td>96</td>\n",
              "      <td>107693</td>\n",
              "      <td>Brandenburg</td>\n",
              "      <td>27</td>\n",
              "      <td>2018</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>7</td>\n",
              "      <td>26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#Merkel|#GroKo</td>\n",
              "      <td>NaN</td>\n",
              "      <td>negative</td>\n",
              "      <td>Klimapolitik</td>\n",
              "      <td>203</td>\n",
              "      <td>215</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ABaerbock15216341401</td>\n",
              "      <td>ABaerbock</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-03-21T12:09:00Z</td>\n",
              "      <td>Der groessten globalen Herausforderung, der Kl...</td>\n",
              "      <td>Der groessten globalen Herausforderung, der Kl...</td>\n",
              "      <td>39</td>\n",
              "      <td>153</td>\n",
              "      <td>107693</td>\n",
              "      <td>Brandenburg</td>\n",
              "      <td>38</td>\n",
              "      <td>2018</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>27</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#Klimakrise|#Regierungserklaerung|#Koalitionsv...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>negative</td>\n",
              "      <td>Klimapolitik</td>\n",
              "      <td>284</td>\n",
              "      <td>296</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ABaerbock15252349801</td>\n",
              "      <td>ABaerbock</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-05-02T04:23:00Z</td>\n",
              "      <td>Wir brauchen eine andere Verkehrspolitik - weg...</td>\n",
              "      <td>Wir brauchen eine andere Verkehrspolitik - weg...</td>\n",
              "      <td>49</td>\n",
              "      <td>213</td>\n",
              "      <td>94139</td>\n",
              "      <td>Brandenburg</td>\n",
              "      <td>38</td>\n",
              "      <td>2018</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>9</td>\n",
              "      <td>33</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>negative</td>\n",
              "      <td>Verkehrspolitik</td>\n",
              "      <td>25</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ABaerbock15256297201</td>\n",
              "      <td>ABaerbock</td>\n",
              "      <td>True</td>\n",
              "      <td>2018-05-06T18:02:00Z</td>\n",
              "      <td>Das ist die Leistung von Hunderten Wahlkaempfe...</td>\n",
              "      <td>Das ist die Leistung von Hunderten Wahlkaempfe...</td>\n",
              "      <td>39</td>\n",
              "      <td>301</td>\n",
              "      <td>107693</td>\n",
              "      <td>Brandenburg</td>\n",
              "      <td>40</td>\n",
              "      <td>2018</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>9</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#kwsh|#kow18|#Kommunalwahl</td>\n",
              "      <td>NaN</td>\n",
              "      <td>positive</td>\n",
              "      <td>Kommunalpolitik</td>\n",
              "      <td>251</td>\n",
              "      <td>266</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 doc_id twitter_username  ...   to label_binary\n",
              "0  ABaerbock15156546001        ABaerbock  ...  251            0\n",
              "1  ABaerbock15210084001        ABaerbock  ...  215            0\n",
              "2  ABaerbock15216341401        ABaerbock  ...  296            0\n",
              "3  ABaerbock15252349801        ABaerbock  ...   40            0\n",
              "4  ABaerbock15256297201        ABaerbock  ...  266            1\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-3Xsv0PJ0X0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c869f1-1c65-47fa-ecb4-84fba5228660"
      },
      "source": [
        "# Prepare GermEval Dataset\n",
        "# data-germeval-2017-train.tsv\n",
        "germeval = pd.read_csv(\"./drive/MyDrive/BERT_Files/BERT_Fine-Tuning_pure/data-germeval-2017-train.tsv\", \n",
        "                   sep='\\t',\n",
        "                   header=None, \n",
        "                   names = [\"full_text\", \"0\", \"label\", \"2\"])\n",
        "germeval['label_binary'] = [1 if label == 'positive' else 0 for label in germeval.label]\n",
        "germeval = germeval.loc[lambda x: x['label'] != \"neutral\"]\n",
        "germeval['label'].value_counts()\n",
        "\n",
        "\n",
        "# Get the lists of tweets and their labels.\n",
        "tweets_germeval = germeval.full_text.values[0:100]\n",
        "labels_germeval = germeval.label_binary.values[0:100]\n",
        "\n",
        "germeval.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6444, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utKWWD02IyBh"
      },
      "source": [
        "First, we need to tokenize our text to be able to feet it into the BERT model. \n",
        "\n",
        "Below an example of tokenized and raw tweet versions is shown. To tokenize the text we have to specify and use the pre-trained BERT because each model has a fixed vocabulary (containing tokens, so wordpieces) and the BERT tokenizer handles words which are not in the certain vocabulary in a specific way.\n",
        "\n",
        "Each token is then mapped to their index in the tokenizer vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZL1eSQeVs6-",
        "outputId": "6e25f40b-9c53-4fcb-e0bd-cc1e5788ea78"
      },
      "source": [
        "# Print the raw tweet.\n",
        "print('Raw: ', tweets[0])\n",
        "\n",
        "# Print the tweet split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(tweets[0]))\n",
        "\n",
        "# Print the tweet mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw:  Habe mir das Gro Ko-Sondierungspapier zu Klima nochmal genau angeschaut. Krass: De facto wird sogar das Kyoto-Protokoll, der Meilenstein der Klimadiplomatie fuer nichtig erklaert! Frau Merkel, das geht so nicht! Nachbessern! kohleausstieg\n",
            "Tokenized:  ['habe', 'mir', 'das', 'gro', 'ko', '-', 'so', '##nd', '##ierungs', '##papier', 'zu', 'kl', '##ima', 'noch', '##mal', 'genau', 'angesch', '##aut', '.', 'k', '##rass', ':', 'de', 'fa', '##ct', '##o', 'wird', 'sogar', 'das', 'k', '##yo', '##to', '-', 'pro', '##to', '##kol', '##l', ',', 'der', 'mei', '##len', '##stein', 'der', 'kl', '##ima', '##di', '##plomat', '##ie', 'f', '##uer', 'nichtig', 'erk', '##la', '##ert', '!', 'fra', '##u', 'merk', '##el', ',', 'das', 'geht', 'so', 'nicht', '!', 'nach', '##besser', '##n', '!', 'ko', '##hle', '##auss', '##ti', '##eg']\n",
            "Token IDs:  [555, 3667, 93, 649, 7424, 26935, 181, 12251, 1737, 14900, 81, 714, 3988, 357, 446, 2971, 16745, 956, 26914, 96, 21242, 26964, 900, 20568, 1920, 26910, 292, 2215, 93, 96, 18886, 14481, 26935, 1017, 14481, 13603, 26907, 26918, 21, 2377, 2941, 1407, 21, 714, 3988, 3748, 13461, 12, 69, 667, 20719, 895, 129, 335, 26982, 6458, 26906, 14540, 77, 26918, 93, 1398, 181, 149, 26982, 188, 4379, 26898, 26982, 7424, 2039, 10685, 15099, 640]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZJe8-JoKAaL"
      },
      "source": [
        "In the next step we add special tokens to the start *CLS* and end of each tweet *SEP*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKR6G5w4dFsy",
        "outputId": "495895c4-4291-4045-d1cd-69b2c7b92526"
      },
      "source": [
        "# Tokenize all of the tweets and map the tokens to their word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every tweet...\n",
        "for tweet in tweets:\n",
        "    encoded_tweet = tokenizer.encode(\n",
        "                        tweet,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded tweet to the list.\n",
        "    input_ids.append(encoded_tweet)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', tweets[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  Habe mir das Gro Ko-Sondierungspapier zu Klima nochmal genau angeschaut. Krass: De facto wird sogar das Kyoto-Protokoll, der Meilenstein der Klimadiplomatie fuer nichtig erklaert! Frau Merkel, das geht so nicht! Nachbessern! kohleausstieg\n",
            "Token IDs: [3, 555, 3667, 93, 649, 7424, 26935, 181, 12251, 1737, 14900, 81, 714, 3988, 357, 446, 2971, 16745, 956, 26914, 96, 21242, 26964, 900, 20568, 1920, 26910, 292, 2215, 93, 96, 18886, 14481, 26935, 1017, 14481, 13603, 26907, 26918, 21, 2377, 2941, 1407, 21, 714, 3988, 3748, 13461, 12, 69, 667, 20719, 895, 129, 335, 26982, 6458, 26906, 14540, 77, 26918, 93, 1398, 181, 149, 26982, 188, 4379, 26898, 26982, 7424, 2039, 10685, 15099, 640, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ_QN3DKkUFy",
        "outputId": "27466e5d-be25-4d37-aa3c-4990e7532dbe"
      },
      "source": [
        "# Do the same for GermEval \n",
        "\n",
        "input_ids_germeval = []\n",
        "\n",
        "# For every tweet...\n",
        "for tweet_germeval in tweets_germeval:\n",
        "    encoded_tweet = tokenizer.encode(\n",
        "                        tweet_germeval,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded tweet to the list.\n",
        "    input_ids_germeval.append(encoded_tweet)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', tweets_germeval[0])\n",
        "print('Token IDs:', input_ids_germeval[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  @nordschaf theoretisch kannste dir überall im Kölner Stadtbereich was suchen. Mit der KVB + S-Bahn kommt man überall fix hin.\n",
            "Token IDs: [3, 26991, 3194, 17051, 23888, 479, 116, 14843, 2118, 73, 211, 106, 18313, 344, 19516, 5492, 1859, 961, 8083, 26914, 114, 21, 96, 26919, 26912, 26986, 19, 26935, 40, 558, 1471, 478, 2118, 73, 211, 19004, 461, 26914, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk4o_F6XKIUF"
      },
      "source": [
        "Then we pad and truncate all tweets to a constant fixed length "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_tBkChIdyOC",
        "outputId": "b5f0dfa6-645a-44e5-e09b-f2474fd7c446"
      },
      "source": [
        "print('Max sentence length: ', max([len(tweet) for tweet in input_ids]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTux34E5kYND",
        "outputId": "547aa253-2246-47d2-e368-56701a6fcf35"
      },
      "source": [
        "print('Max sentence length for GermEval data: ', max([len(tweet_germeval) for tweet_germeval in input_ids_germeval]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length for GermEval data:  868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTtngalseGJs",
        "outputId": "a90e20e8-22c7-4b9e-dbe4-9fd5dc0f932a"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# A bit larger than the maximum training tweet length of 91/110... (with germeval 3202/3300 -> bert has a max length limit of tokens = 512, we cut the longer texts off and only use the first 512 Tokens)\n",
        "MAX_LEN = 250\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" - pad and truncate at the end of the sequence, as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 250 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 30000\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh4DpiocklcK"
      },
      "source": [
        "# same for germeval \n",
        "\n",
        "# MAX_LEN_germeval = 3300\n",
        "input_ids_germeval = pad_sequences(input_ids_germeval, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vstmJJnvOEJ6"
      },
      "source": [
        "Create attention masks which indicates which tokens are words and which are padding (if token ID is 0 then it is padding and the attention mask is set to 0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoRYw4_jg4QP"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for tweet in input_ids:\n",
        "    \n",
        "    att_mask = [int(token_id > 0) for token_id in tweet]\n",
        "    \n",
        "    # Store the attention mask for each tweet.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yinz_P3vk1oJ"
      },
      "source": [
        "# Create attention masks for Germeval data \n",
        "\n",
        "attention_masks_germeval = []\n",
        "\n",
        "# For each sentence...\n",
        "for tweet_germeval in input_ids_germeval:\n",
        "    \n",
        "    att_mask = [int(token_id > 0) for token_id in tweet_germeval]\n",
        "    \n",
        "    # Store the attention mask for each tweet.\n",
        "    attention_masks_germeval.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vp0fFEsOifw"
      },
      "source": [
        "Now, we use train_test_split to split our data into train, test sets first  and then split the initial train set further into a final train set and validation set for training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV4b99xCgEsu"
      },
      "source": [
        "# # without germeval case\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import numpy as np\n",
        "\n",
        "# # Use 80% for training and 20% for validation.\n",
        "# train_inputs2, test_inputs, train_labels2, test_labels = train_test_split(input_ids, labels, \n",
        "#                                                             random_state=2020, test_size=0.2, stratify = labels)\n",
        "\n",
        "# # Do the same for the masks.\n",
        "# train_masks2, test_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "#                                              random_state=2020, test_size=0.2, stratify = labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Use 20% of train set as validation set \n",
        "# train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_inputs2, train_labels2, \n",
        "#                                                                                     test_size=0.2, random_state=2020) \n",
        "\n",
        "# # Do the same for the masks.\n",
        "# train_masks, validation_masks, _, _ = train_test_split(train_masks2, train_labels2,\n",
        "#                                              random_state=2020, test_size=0.2)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPxoRdC5-7YD",
        "outputId": "1e60a400-2325-4831-a139-3308f5f82b60"
      },
      "source": [
        "len(test_inputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHDnNsImhFKd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Use 80% for training and 20% for validation.\n",
        "train_inputs1, test_inputs, train_labels1, test_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2020, test_size=0.2, stratify = labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks1, test_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2020, test_size=0.2, stratify = labels)\n",
        "\n",
        "# Mix train_inputs1, train_labels1, train_masks1 with germeval data\n",
        "train_inputs2 = np.concatenate((train_inputs1, input_ids_germeval), axis=0)\n",
        "train_labels2 = np.concatenate((train_labels1, labels_germeval), axis=None)\n",
        "train_masks2  = np.concatenate((train_masks1, attention_masks_germeval), axis=0)\n",
        "\n",
        "\n",
        "\n",
        "# Use 20% of train set as validation set \n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_inputs2, train_labels2, \n",
        "                                                                                    test_size=0.2, random_state=2020) \n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(train_masks2, train_labels2,\n",
        "                                             random_state=2020, test_size=0.2)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTaFTXqFO080"
      },
      "source": [
        "Convert all inputs and labels into torch tensors, the required datatype \n",
        "for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg71HZ7qifpo"
      },
      "source": [
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "test_masks = torch.tensor(test_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8bEkw8kO9Zr"
      },
      "source": [
        "The torch DataLoader class enables to create  an iterator for our data in order to save memory during training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBRWzCgriurl"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Define batch size here to let DataLoader know. \n",
        "# BERT-authors recommend a batch size of 16 or 32 for fine-tuning .\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our test set.\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIEy4SR4R6ch"
      },
      "source": [
        "Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "linear classification layer on top. This is the one for classification tasks in general. *(see for more details https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xILaX9ulICZ"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    pretrained_model, \n",
        "    num_labels = 2, # The number of output labels--2 for binary classification. Can be increased for multiclass\n",
        "   output_attentions = False, \n",
        "   output_hidden_states = False \n",
        ")\n",
        "\n",
        "# Run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wddbSBdwl6MM"
      },
      "source": [
        "# All of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOogCCNSS1_z"
      },
      "source": [
        "Get the optimizer after loading the model to train the hyperparameters. \n",
        "\n",
        "We chose following values based on authors' recommendations:\n",
        "\n",
        "Batch size: 32 \n",
        "Learning rate (Adam): 2e-5\n",
        "Number of epochs: 4\n",
        "\n",
        "*(see for more details on AdamW Optimizer https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5YMJeG7m1GW"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TypGI2W5m5X9"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fgx-tuPnc8Z"
      },
      "source": [
        "# helper function for calculating accuracy\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcSWb-YhpNo4"
      },
      "source": [
        "# helper function for formatting elapsed times\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dgc1fHkwTxaf"
      },
      "source": [
        "Let's start the actual training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acrG210Npcb4"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "seed_val = 55\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch:\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    ## TRAIN\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # how long does the training epoch take.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Turn on the training mode for the model. \n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data:\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack the batch and load onto the GPU.\n",
        "        # Each batch contains input ids, attention masks, labels. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Remove any previously calculated gradients before performing a\n",
        "        # backward pass. \n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Do a forward pass. \n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        \n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. \n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Do a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0. \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step.\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Avg loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Need for plotting the learning curve later.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Avg training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Epoch time: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    ## VALIDATE\n",
        "    # After each training epoch, we measure performance on\n",
        "    # validation set.\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Turn on the evaluation mode of the model.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        \n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get logit values predicted for each class.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Accuracy for this batch of validation tweets.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation time: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "yKJGT2uxqsNu",
        "outputId": "625a82bc-cdbe-4451-bccd-82169b06e230"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5b150a6234d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Plot the learning curve.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Label the plot.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_values' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVV9-_GMY76z"
      },
      "source": [
        "# Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tocnbPMDRN00",
        "outputId": "3ce0a19a-143b-4293-8969-a187f737c3c1"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test tweets...'.format(len(test_inputs)))\n",
        "\n",
        "# Turn on the evaluation mode of the model\n",
        "model.eval()\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Compute gradients, save memory and speed up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 243 test tweets...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOIrNxOukbY6",
        "outputId": "ac29cb59-b31b-4445-ae7d-680d1dd10599"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label_binary.sum(), len(df.label_binary), (df.label_binary.sum() / len(df.label_binary) * 100.0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 335 of 1215 (27.57%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iNrCwTYa6cI"
      },
      "source": [
        "Performance measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YzWo-1qp3kn"
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "# list of keys of the highest scores, i.e. predictions (indices of the maximum values along an axis)\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x7g2bzgb-o9"
      },
      "source": [
        "Calculate F1-Score, Accuracy and Show the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAoOFsCUwsLq",
        "outputId": "af61f5ef-d4fa-4886-dfe7-79c7cfe49209"
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "\n",
        "f1_value = f1_score(flat_predictions, flat_true_labels, average=\"weighted\")\n",
        "accuracy = accuracy_score(flat_predictions, flat_true_labels)\n",
        "\n",
        "print(\"F1 Score (Weighted): {}\".format(f1_value))\n",
        "print(\"Accuracy: {}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score (Weighted): 0.9055715566326372\n",
            "Accuracy: 0.9053497942386831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4HRr8IOxxYO"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s22lIKFxrOu"
      },
      "source": [
        "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
        "    \n",
        "    cm = confusion_matrix(y_true=true_labels, \n",
        "                                  y_pred=predicted_labels, \n",
        "                                  labels=classes)\n",
        "    cm_frame = pd.DataFrame(data=cm, \n",
        "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
        "                                                  codes=[[0,0],[0,1]]), \n",
        "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
        "                                                codes=[[0,0],[0,1]])) \n",
        "    return cm_frame   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "aARx5G5vy5vZ",
        "outputId": "b1337053-4a51-412f-b0a1-6cdd730d75e0"
      },
      "source": [
        "confusion_mat = display_confusion_matrix(true_labels = flat_true_labels, predicted_labels = flat_predictions)\n",
        "\n",
        "confusion_mat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">Predicted:</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Actual:</th>\n",
              "      <th>1</th>\n",
              "      <td>55</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>165</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Predicted:     \n",
              "                   1    0\n",
              "Actual: 1         55   12\n",
              "        0         11  165"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    }
  ]
}