{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_ASC_Task.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "040c6380fc93425994d610f7a0693026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f41f0a782c8b431db179e7695040fad7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ccb6e0916294b3e82ef683d484159b0",
              "IPY_MODEL_eb0e663ae7dd4691b701be7a4dac2074"
            ]
          }
        },
        "f41f0a782c8b431db179e7695040fad7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ccb6e0916294b3e82ef683d484159b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cb49b75ad0674b8c838cf3ad9bc03b02",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 254728,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 254728,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0baea528d6844279dbd0cece6e01e47"
          }
        },
        "eb0e663ae7dd4691b701be7a4dac2074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28344c7173f246a18647b598445b2d03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 255k/255k [00:00&lt;00:00, 631kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ae9b900798534975bf8bc239b5a2143c"
          }
        },
        "cb49b75ad0674b8c838cf3ad9bc03b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0baea528d6844279dbd0cece6e01e47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28344c7173f246a18647b598445b2d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ae9b900798534975bf8bc239b5a2143c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_WquHzdJg8r",
        "outputId": "17625e6c-b075-48cd-8d8c-c0abc2919e24"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Get the GPU device name.\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "# The device name should look like the following:\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOLxa8-jKn5X",
        "outputId": "5e6b6e70-2fb8-47eb-95f5-400dbc282dcc"
      },
      "source": [
        "# Identify and specify the GPU as the device\r\n",
        "import torch\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNs085AsKrGE",
        "outputId": "5e6023ad-482e-4193-84d1-72308ecb5722"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovra0b6QIu2G"
      },
      "source": [
        "# **Install packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HIelG_zKt6u",
        "outputId": "a22b9789-e8a7-47b7-ebfe-add9f4a8e7dc"
      },
      "source": [
        " #!pip install pytorch_pretrained_bert\r\n",
        " #!pip install transformers==3.5.1\r\n",
        " !pip install nltk\r\n",
        "import os\r\n",
        "import logging\r\n",
        "import argparse\r\n",
        "import random\r\n",
        "import json\r\n",
        "import sklearn.metrics\r\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\r\n",
        "\r\n",
        "import nltk\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "\r\n",
        "from transformers import BertTokenizer\r\n",
        "from pytorch_pretrained_bert.modeling import BertModel, BertPreTrainedModel, BertForSequenceClassification\r\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\r\n",
        "\r\n",
        "\r\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\r\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\r\n",
        "                    level = logging.INFO)\r\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEpEtwZnJDbf"
      },
      "source": [
        "# **Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADJHFjW4KxfM"
      },
      "source": [
        "def warmup_linear(x, warmup=0.002):\r\n",
        "    if x < warmup:\r\n",
        "        return x/warmup\r\n",
        "    return 1.0 - x\r\n",
        "\r\n",
        "class InputExample(object):\r\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\r\n",
        "        \"\"\"Constructs a InputExample.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            guid: Unique id for the example.\r\n",
        "            text_a: string. The untokenized text of the first sequence. For single\r\n",
        "            sequence tasks, only this sequence must be specified.\r\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\r\n",
        "            Only must be specified for sequence pair tasks.\r\n",
        "            label: (Optional) string. The label of the example. This should be\r\n",
        "            specified for train and dev examples, but not for test examples.\r\n",
        "        \"\"\"\r\n",
        "        self.guid = guid\r\n",
        "        self.text_a = text_a\r\n",
        "        self.text_b = text_b\r\n",
        "        self.label = label\r\n",
        "\r\n",
        "# specifically for asc task\r\n",
        "\r\n",
        "def create_examples(lines, set_type):\r\n",
        "    examples = []\r\n",
        "    for (i, ids) in enumerate(lines[\"data\"]):\r\n",
        "        guid = \"%s-%s\" % (set_type, ids )\r\n",
        "        text_a = lines[\"data\"][ids]['term']\r\n",
        "        text_b = lines[\"data\"][ids]['sentence']\r\n",
        "        label = lines[\"data\"][ids]['polarity']\r\n",
        "        examples.append(\r\n",
        "            InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label) )\r\n",
        "    return examples   \r\n",
        "\r\n",
        "def read_json(input_file):\r\n",
        "    \"\"\"Reads a json file for tasks in sentiment analysis.\"\"\"\r\n",
        "    with open(input_file) as f:\r\n",
        "        return json.load(f)\r\n",
        "        \r\n",
        "def get_train_examples(data_dir):\r\n",
        "    \"\"\"See base class.\"\"\"\r\n",
        "    return create_examples(\r\n",
        "        read_json(data_dir), \"train\")\r\n",
        "\r\n",
        "def get_dev_examples(data_dir):\r\n",
        "    \"\"\"See base class.\"\"\"\r\n",
        "    return create_examples(\r\n",
        "        read_json(data_dir), \"dev\")    \r\n",
        "\r\n",
        "def get_test_examples(data_dir):\r\n",
        "    \"\"\"See base class.\"\"\"\r\n",
        "    return create_examples(\r\n",
        "        read_json(data_dir), \"test\")    \r\n",
        "    \r\n",
        "class BertForSequenceLabeling(BertPreTrainedModel):\r\n",
        "    def __init__(self, config, num_labels=3):\r\n",
        "        super(BertForSequenceLabeling, self).__init__(config)\r\n",
        "        self.num_labels = num_labels\r\n",
        "        self.bert = BertModel(config)\r\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\r\n",
        "        self.apply(self.init_bert_weights)\r\n",
        "\r\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\r\n",
        "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\r\n",
        "        sequence_output = self.dropout(sequence_output)\r\n",
        "        logits = self.classifier(sequence_output)\r\n",
        "\r\n",
        "        if labels is not None:\r\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\r\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n",
        "            return loss\r\n",
        "        else:\r\n",
        "            return logits\r\n",
        "\r\n",
        "\r\n",
        "class InputFeatures(object):\r\n",
        "    \"\"\"A single set of features of data.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\r\n",
        "        self.input_ids = input_ids\r\n",
        "        self.input_mask = input_mask\r\n",
        "        self.segment_ids = segment_ids\r\n",
        "        self.label_id = label_id\r\n",
        "\r\n",
        "\r\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\r\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\r\n",
        "\r\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\r\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\r\n",
        "    # of tokens from each, since if one sequence is very short then each token\r\n",
        "    # that's truncated likely contains more information than a longer sequence.\r\n",
        "    while True:\r\n",
        "        total_length = len(tokens_a) + len(tokens_b)\r\n",
        "        if total_length <= max_length:\r\n",
        "            break\r\n",
        "        if len(tokens_a) > len(tokens_b):\r\n",
        "            tokens_a.pop()\r\n",
        "        else:\r\n",
        "            tokens_b.pop()\r\n",
        "\r\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, mode):\r\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\" #check later if we can merge this function with the SQuAD preprocessing \r\n",
        "    label_map = {}\r\n",
        "    for (i, label) in enumerate(label_list):\r\n",
        "        label_map[label] = i\r\n",
        "\r\n",
        "    features = []\r\n",
        "    for (ex_index, example) in enumerate(examples):\r\n",
        "        if mode!=\"ae\":\r\n",
        "            tokens_a = tokenizer.tokenize(example.text_a)\r\n",
        "        else: #only do subword tokenization.\r\n",
        "            tokens_a, labels_a, example.idx_map= tokenizer.subword_tokenize([token.lower() for token in example.text_a], example.label )\r\n",
        "\r\n",
        "        tokens_b = None\r\n",
        "        if example.text_b:\r\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\r\n",
        "\r\n",
        "        if tokens_b:\r\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\r\n",
        "            # length is less than the specified length.\r\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\r\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\r\n",
        "        else:\r\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\r\n",
        "            if len(tokens_a) > max_seq_length - 2:\r\n",
        "                tokens_a = tokens_a[0:(max_seq_length - 2)]\r\n",
        "\r\n",
        "        tokens = []\r\n",
        "        segment_ids = []\r\n",
        "        tokens.append(\"[CLS]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "        for token in tokens_a:\r\n",
        "            tokens.append(token)\r\n",
        "            segment_ids.append(0)\r\n",
        "        tokens.append(\"[SEP]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "\r\n",
        "        if tokens_b:\r\n",
        "            for token in tokens_b:\r\n",
        "                tokens.append(token)\r\n",
        "                segment_ids.append(1)\r\n",
        "            tokens.append(\"[SEP]\")\r\n",
        "            segment_ids.append(1)\r\n",
        "\r\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "\r\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n",
        "        # tokens are attended to.\r\n",
        "        input_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "        # Zero-pad up to the sequence length.\r\n",
        "        while len(input_ids) < max_seq_length:\r\n",
        "            input_ids.append(0)\r\n",
        "            input_mask.append(0)\r\n",
        "            segment_ids.append(0)\r\n",
        "\r\n",
        "        assert len(input_ids) == max_seq_length\r\n",
        "        assert len(input_mask) == max_seq_length\r\n",
        "        assert len(segment_ids) == max_seq_length\r\n",
        "\r\n",
        "        if mode!=\"ae\":\r\n",
        "            label_id = label_map[example.label]\r\n",
        "        else:\r\n",
        "            label_id = [-1] * len(input_ids) #-1 is the index to ignore\r\n",
        "            #truncate the label length if it exceeds the limit.\r\n",
        "            lb=[label_map[label] for label in labels_a]\r\n",
        "            if len(lb) > max_seq_length - 2:\r\n",
        "                lb = lb[0:(max_seq_length - 2)]\r\n",
        "            label_id[1:len(lb)+1] = lb\r\n",
        "\r\n",
        "        features.append(\r\n",
        "                InputFeatures(\r\n",
        "                        input_ids=input_ids,\r\n",
        "                        input_mask=input_mask,\r\n",
        "                        segment_ids=segment_ids,\r\n",
        "                        label_id=label_id))\r\n",
        "    return features\r\n",
        "\r\n",
        "class ABSATokenizer(BertTokenizer):     \r\n",
        "    def subword_tokenize(self, tokens, labels): # for AE\r\n",
        "        split_tokens, split_labels= [], []\r\n",
        "        idx_map=[]\r\n",
        "        for ix, token in enumerate(tokens):\r\n",
        "            sub_tokens=self.wordpiece_tokenizer.tokenize(token)\r\n",
        "            for jx, sub_token in enumerate(sub_tokens):\r\n",
        "                split_tokens.append(sub_token)\r\n",
        "                if labels[ix]==\"B\" and jx>0:\r\n",
        "                    split_labels.append(\"I\")\r\n",
        "                else:\r\n",
        "                    split_labels.append(labels[ix])\r\n",
        "                idx_map.append(ix)\r\n",
        "        return split_tokens, split_labels, idx_map  \r\n",
        "\r\n",
        "### Text preprocessing         \r\n",
        "\r\n",
        "LABEL_SET = {    \r\n",
        "        \"BIO\": {\r\n",
        "            \"label_map\": {\r\n",
        "                \"O\": \"O\", \r\n",
        "                \"B-positive\": \"B\", \r\n",
        "                \"B-negative\": \"B\", \r\n",
        "                \"B-neutral\": \"B\",\r\n",
        "                \"B-conflict\": \"B\",\r\n",
        "                \"I-positive\": \"I\",\r\n",
        "                \"I-negative\": \"I\",\r\n",
        "                \"I-neutral\": \"I\",\r\n",
        "                \"I-conflict\": \"I\",\r\n",
        "            },\r\n",
        "            \"label_list\": [\"O\", \"B\", \"I\"]\r\n",
        "        },\r\n",
        "        \"AO\": {\r\n",
        "            \"label_map\": {\r\n",
        "                \"O\": \"O\",\r\n",
        "                \"B-positive\": \"A\", \r\n",
        "                \"B-negative\": \"A\", \r\n",
        "                \"B-neutral\": \"A\",\r\n",
        "                \"B-conflict\": \"A\",\r\n",
        "                \"I-positive\": \"A\",\r\n",
        "                \"I-negative\": \"A\",\r\n",
        "                \"I-neutral\": \"A\",\r\n",
        "                \"I-conflict\": \"A\",\r\n",
        "            },\r\n",
        "            \"label_list\": [\"O\", \"A\"]\r\n",
        "        },\r\n",
        "        \"PNNO\": {\r\n",
        "            \"label_map\": {\r\n",
        "                \"O\": \"O\",\r\n",
        "                \"B-positive\": \"positive\", \r\n",
        "                \"B-negative\": \"negative\", \r\n",
        "                \"B-neutral\": \"neutral\",\r\n",
        "                \"B-conflict\": \"neutral\",\r\n",
        "                \"I-positive\": \"positive\",\r\n",
        "                \"I-negative\": \"negative\",\r\n",
        "                \"I-neutral\": \"neutral\",\r\n",
        "                \"I-conflict\": \"neutral\",\r\n",
        "            },\r\n",
        "            \"label_list\": [\"O\", \"positive\", \"negative\", \"neutral\"]\r\n",
        "        }\r\n",
        "    }\r\n",
        "\r\n",
        "\r\n",
        "def _tokenize_text(raw_text, from_index, to_index, label_map, sentiment):\r\n",
        "    # warning this code this borrowed from the project : https://github.com/howardhsu/DE-CNN\r\n",
        "    # the code needs cleaner re-writing.\r\n",
        "    text = []\r\n",
        "    for ix, c in enumerate(raw_text):\r\n",
        "        # assuming that the tokenizer always yields fine-grained tokens for aspects \r\n",
        "        # so tokenizer won't affect the performance of AE.\r\n",
        "        if (c=='/' or c=='*' or c=='-' or c=='=') and len(text)>0 and text[-1]!=' ':\r\n",
        "            text.append(' ')\r\n",
        "        if ix==int(from_index ) and len(text)>0 and text[-1]!=' ':\r\n",
        "            text.append(' ')\r\n",
        "        elif ix==int(to_index ) and len(text)>0 and text[-1]!=' ' and c!=' ': \r\n",
        "            text.append(' ')\r\n",
        "        text.append(c)\r\n",
        "        if (c=='/' or c=='*' or c=='-' or c=='=') and text[-1]!=' ':\r\n",
        "            text.append(' ')\r\n",
        "\r\n",
        "    text=\"\".join(text)\r\n",
        "    tokens=nltk.word_tokenize(text, language='german')\r\n",
        "    lb = [label_map[\"O\"]]*len(tokens)\r\n",
        "    \r\n",
        "    token_idx, pt, tag_on=0, 0, False\r\n",
        "    for ix, c in enumerate(raw_text):\r\n",
        "        if pt>=len(tokens[token_idx] ):\r\n",
        "            pt=0\r\n",
        "            token_idx+=1\r\n",
        "            if token_idx >= len(tokens):\r\n",
        "                break\r\n",
        "        if ix==from_index: #from\r\n",
        "            assert pt == 0 and c != ' '\r\n",
        "            lb[token_idx] = label_map[\"B-\" + sentiment]\r\n",
        "            tag_on = True\r\n",
        "        elif ix==to_index: #to\r\n",
        "            assert pt == 0\r\n",
        "            tag_on = False\r\n",
        "        elif tag_on and pt==0 and c!=' ':\r\n",
        "            lb[token_idx] = label_map[\"I-\" + sentiment]\r\n",
        "        if c==' ' or ord(c)==160: # skip spaces.\r\n",
        "            pass\r\n",
        "        elif tokens[token_idx][pt:pt+2]=='``' or tokens[token_idx][pt:pt+2]==\"''\":\r\n",
        "            pt+=2\r\n",
        "        else:\r\n",
        "            pt+=1\r\n",
        "    return tokens, lb\r\n",
        "\r\n",
        "def ae_task(data):\r\n",
        "    corpus = []\r\n",
        "    for index, tweet in data.iterrows():\r\n",
        "        tokens, lb = _tokenize_text(raw_text = tweet['full_text'], \r\n",
        "                                from_index = tweet['from'], \r\n",
        "                                to_index = tweet['to'],\r\n",
        "                                label_map = LABEL_SET[\"BIO\"][\"label_map\"],\r\n",
        "                                sentiment = tweet['label'])\r\n",
        "        corpus.append({\"id\": tweet['doc_id'], \r\n",
        "                       \"tokens\": tokens, \r\n",
        "                       \"labels\": lb}) \r\n",
        "        \r\n",
        "    return corpus\r\n",
        "\r\n",
        "def asc_task(data):\r\n",
        "    corpus = []\r\n",
        "    for index, tweet in data.iterrows():\r\n",
        "        corpus.append({\"id\": tweet['doc_id'], \r\n",
        "                       \"sentence\": tweet['full_text'], \r\n",
        "                       \"term\": tweet['topic'], \r\n",
        "                       \"polarity\": tweet['label']})     \r\n",
        "        \r\n",
        "    return corpus\r\n",
        "\r\n",
        "def write_json(filename, corpus, meta=['O', 'B', 'I'], target_dir = 'D:/UNI/01_Master/3. Semester/Consulting/00_BERT/'):\r\n",
        "    path = os.path.join(target_dir, filename)\r\n",
        "    with open(path, \"w\") as fw:\r\n",
        "        json.dump({\"data\": {rec[\"id\"]: rec for rec in corpus}, \"meta\": meta}, fw)    \r\n",
        "\r\n",
        "### Evaluation part\r\n",
        "\r\n",
        "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\r\n",
        "    \r\n",
        "    cm = confusion_matrix(y_true=true_labels, \r\n",
        "                                  y_pred=predicted_labels, \r\n",
        "                                  labels=classes)\r\n",
        "    cm_frame = pd.DataFrame(data=cm, \r\n",
        "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \r\n",
        "                                                  codes=[[0,0],[0,1]]), \r\n",
        "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \r\n",
        "                                                codes=[[0,0],[0,1]])) \r\n",
        "    return cm_frame   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj7d_aEXJnKa"
      },
      "source": [
        "# **Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryocy13eLI8p"
      },
      "source": [
        "train_batch_size = 32            \r\n",
        "num_train_epochs = 4      \r\n",
        "max_seq_length = 100\r\n",
        "learning_rate = 3e-5\r\n",
        "warmup_proportion = 0.1\r\n",
        "do_valid = True\r\n",
        "pretrained_model = \"bert-base-german-cased\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xb11uU4JsRx"
      },
      "source": [
        "# **Training Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "040c6380fc93425994d610f7a0693026",
            "f41f0a782c8b431db179e7695040fad7",
            "7ccb6e0916294b3e82ef683d484159b0",
            "eb0e663ae7dd4691b701be7a4dac2074",
            "cb49b75ad0674b8c838cf3ad9bc03b02",
            "e0baea528d6844279dbd0cece6e01e47",
            "28344c7173f246a18647b598445b2d03",
            "ae9b900798534975bf8bc239b5a2143c"
          ]
        },
        "id": "jthYcSLfLJpc",
        "outputId": "3e9201aa-9bba-4557-a0c2-a28ba79a191f"
      },
      "source": [
        "label_list = [\"positive\", \"negative\", \"neutral\"] \r\n",
        "\r\n",
        "tokenizer = ABSATokenizer.from_pretrained(pretrained_model)\r\n",
        "train_examples = get_train_examples(data_dir = \"train.json\")\r\n",
        "num_train_steps = int(len(train_examples) / train_batch_size) * num_train_epochs\r\n",
        "\r\n",
        "train_features = convert_examples_to_features(examples = train_examples, \r\n",
        "                                              label_list = label_list, \r\n",
        "                                              max_seq_length = max_seq_length, \r\n",
        "                                              tokenizer = tokenizer, \r\n",
        "                                              mode = \"asc\")\r\n",
        "logger.info(\"***** Running training *****\")\r\n",
        "logger.info(\"  Num examples = %d\", len(train_examples))\r\n",
        "logger.info(\"  Batch size = %d\", train_batch_size)\r\n",
        "logger.info(\"  Num steps = %d\", num_train_steps)\r\n",
        "\r\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\r\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\r\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\r\n",
        "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\r\n",
        "\r\n",
        "train_data = TensorDataset(all_input_ids, all_segment_ids, all_input_mask, all_label_ids)\r\n",
        "\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 23:34:25 - INFO - filelock -   Lock 140192104248096 acquired on /root/.cache/torch/transformers/da299cdd121a3d71e1626f2908dda0d02658f42e925a3d6abd8273ec08cf41a6.31ccc255fc2bad3578089a3997f16b286498ba78c0adc43b5bb2a3f9a0d2c85c.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "040c6380fc93425994d610f7a0693026",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=254728.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 23:34:26 - INFO - filelock -   Lock 140192104248096 released on /root/.cache/torch/transformers/da299cdd121a3d71e1626f2908dda0d02658f42e925a3d6abd8273ec08cf41a6.31ccc255fc2bad3578089a3997f16b286498ba78c0adc43b5bb2a3f9a0d2c85c.lock\n",
            "02/09/2021 23:34:26 - INFO - __main__ -   ***** Running training *****\n",
            "02/09/2021 23:34:26 - INFO - __main__ -     Num examples = 2\n",
            "02/09/2021 23:34:26 - INFO - __main__ -     Batch size = 32\n",
            "02/09/2021 23:34:26 - INFO - __main__ -     Num steps = 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_rHFMS0Jx6I"
      },
      "source": [
        "# **Validation Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3eiIs-fbcMc",
        "outputId": "53c01efa-0ae8-4b08-b404-d0d0bec88b97"
      },
      "source": [
        "valid_examples = get_dev_examples(data_dir = \"dev.json\")\r\n",
        "valid_features= convert_examples_to_features(\r\n",
        "    valid_examples, label_list,  max_seq_length, tokenizer, \"asc\")\r\n",
        "valid_all_input_ids = torch.tensor([f.input_ids for f in valid_features], dtype=torch.long)\r\n",
        "valid_all_segment_ids = torch.tensor([f.segment_ids for f in valid_features], dtype=torch.long)\r\n",
        "valid_all_input_mask = torch.tensor([f.input_mask for f in valid_features], dtype=torch.long)\r\n",
        "valid_all_label_ids = torch.tensor([f.label_id for f in valid_features], dtype=torch.long)\r\n",
        "valid_data = TensorDataset(valid_all_input_ids, valid_all_segment_ids, valid_all_input_mask, valid_all_label_ids)\r\n",
        "\r\n",
        "logger.info(\"***** Running validations *****\")\r\n",
        "logger.info(\"  Num orig examples = %d\", len(valid_examples))\r\n",
        "logger.info(\"  Num split examples = %d\", len(valid_features))\r\n",
        "logger.info(\"  Batch size = %d\",  train_batch_size)\r\n",
        "\r\n",
        "valid_sampler = SequentialSampler(valid_data)\r\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size= train_batch_size)    \r\n",
        "\r\n",
        "best_valid_loss=float('inf')\r\n",
        "valid_losses=[]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 23:34:50 - INFO - __main__ -   ***** Running validations *****\n",
            "02/09/2021 23:34:50 - INFO - __main__ -     Num orig examples = 3\n",
            "02/09/2021 23:34:50 - INFO - __main__ -     Num split examples = 3\n",
            "02/09/2021 23:34:50 - INFO - __main__ -     Batch size = 32\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OXQy-EoJ7tJ"
      },
      "source": [
        "# **Optimization & Actual Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxPjhpcQbzyl",
        "outputId": "8afe1770-4fff-4735-c910-ca6dc74f5f7d"
      },
      "source": [
        "    model = BertForSequenceClassification.from_pretrained(\"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased.tar.gz\", num_labels = len(label_list))\r\n",
        "    model.cuda()\r\n",
        "    # Prepare optimizer\r\n",
        "    param_optimizer = [(k, v) for k, v in model.named_parameters() if v.requires_grad==True]\r\n",
        "    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\r\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\r\n",
        "    optimizer_grouped_parameters = [\r\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\r\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\r\n",
        "        ]\r\n",
        "    t_total = num_train_steps\r\n",
        "    optimizer = BertAdam(optimizer_grouped_parameters,\r\n",
        "                         lr=learning_rate,\r\n",
        "                         warmup= warmup_proportion,\r\n",
        "                         t_total=t_total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 23:36:09 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased.tar.gz from cache at /root/.pytorch_pretrained_bert/4983b4079d9a070d00f914e5e669c1b1376f201367e3507366bc8a9de9d62fd0.d994c35add5d5ceb797a0bce40d17a4abb02c9c37b804e92c448d31d8de34734\n",
            "02/09/2021 23:36:09 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/4983b4079d9a070d00f914e5e669c1b1376f201367e3507366bc8a9de9d62fd0.d994c35add5d5ceb797a0bce40d17a4abb02c9c37b804e92c448d31d8de34734 to temp dir /tmp/tmpx7ef8nns\n",
            "02/09/2021 23:36:12 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "02/09/2021 23:36:15 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "02/09/2021 23:36:15 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH3NrVGscmkI",
        "outputId": "faad3c13-60af-4457-87ca-5713075312b5"
      },
      "source": [
        "global_step = 0\r\n",
        "model.train()\r\n",
        "for _ in range(num_train_epochs):\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "        batch = tuple(t.cuda() for t in batch)\r\n",
        "        input_ids, segment_ids, input_mask, label_ids = batch\r\n",
        "        loss = model(input_ids, segment_ids, input_mask, label_ids)\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\r\n",
        "        for param_group in optimizer.param_groups:\r\n",
        "            param_group['lr'] = lr_this_step\r\n",
        "        optimizer.step()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        global_step += 1\r\n",
        "        #>>>> perform validation at the end of each epoch .\r\n",
        "    if do_valid:\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            losses=[]\r\n",
        "            valid_size=0\r\n",
        "            for step, batch in enumerate(valid_dataloader):\r\n",
        "                batch = tuple(t.cuda() for t in batch) # multi-gpu does scattering it-self\r\n",
        "                input_ids, segment_ids, input_mask, label_ids = batch\r\n",
        "                loss = model(input_ids, segment_ids, input_mask, label_ids)\r\n",
        "                losses.append(loss.data.item()*input_ids.size(0) )\r\n",
        "                valid_size+=input_ids.size(0)\r\n",
        "            valid_loss=sum(losses)/valid_size\r\n",
        "            logger.info(\"validation loss: %f\", valid_loss)\r\n",
        "            valid_losses.append(valid_loss)\r\n",
        "        if valid_loss<best_valid_loss:\r\n",
        "            torch.save(model, \"model.pt\")\r\n",
        "            best_valid_loss=valid_loss\r\n",
        "        model.train()\r\n",
        "if do_valid:\r\n",
        "    with open(\"valid.json\", \"w\") as fw:\r\n",
        "        json.dump({\"valid_losses\": valid_losses}, fw)\r\n",
        "else:\r\n",
        "    torch.save(model, \"model.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "02/09/2021 23:36:23 - INFO - __main__ -   validation loss: 1.117757\n",
            "02/09/2021 23:36:25 - INFO - __main__ -   validation loss: 0.960526\n",
            "02/09/2021 23:36:26 - INFO - __main__ -   validation loss: 0.867670\n",
            "02/09/2021 23:36:28 - INFO - __main__ -   validation loss: 0.827143\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJFcHTbtI1yy"
      },
      "source": [
        "# **Final Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ualbw4LZf9"
      },
      "source": [
        "This is the End to End Analysis, the combined AE and ASC tasks will be evaluated in this section: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTjYxW70EUA5"
      },
      "source": [
        "\r\n",
        "1.   Insert original Tweets without topics and labels: tweets\r\n",
        "2.   Insert predicted and post-processed aspects: y_pred_aspects\r\n",
        "3.   Insert tweets in tokenized form during AE task: pred_json[\"raw_X\"]\r\n",
        "4.   Result: Aspect Extraction. Fill in the topic-column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZzPEIhPGbTM"
      },
      "source": [
        "with open(\"predictions_ae.json\") as f:\r\n",
        "    pred_json=json.load(f)   \r\n",
        "y_pred_aspects=[]\r\n",
        "for ix, logit in enumerate(pred_json[\"logits\"]):\r\n",
        "    pred=[0]*len(pred_json[\"raw_X\"][ix])\r\n",
        "    for jx, idx in enumerate(pred_json[\"idx_map\"][ix]):\r\n",
        "        lb=np.argmax(logit[jx])\r\n",
        "        if lb==1: #B\r\n",
        "            pred[idx]=1\r\n",
        "        elif lb==2: #I\r\n",
        "            if pred[idx]==0: #only when O->I (I->I and B->I ignored)\r\n",
        "                pred[idx]=2\r\n",
        "    y_pred_aspects.append(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDdg3eW62G-u"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "tweets = pd.read_csv('bert_toy.csv') \r\n",
        "tweets = tweets[0:2]\r\n",
        "tweets['topic'] = \"\"\r\n",
        "label = y_pred_aspects\r\n",
        "corpus = pred_json[\"raw_X\"]\r\n",
        "pred_y=[] #zx, sent \r\n",
        "for zx, tweet in tweets.iterrows():\r\n",
        "    tokens = corpus[zx]\r\n",
        "    lb = label[zx]\r\n",
        "    # opin = tweet['topic']\r\n",
        "    token_idx, pt, tag_on=0, 0, False\r\n",
        "    start, end=-1, -1\r\n",
        "    for ix, c in enumerate(tweet['full_text']):\r\n",
        "        if token_idx<len(tokens) and pt>=len(tokens[token_idx] ):\r\n",
        "            pt=0\r\n",
        "            token_idx+=1\r\n",
        "\r\n",
        "        if token_idx<len(tokens) and lb[token_idx]==1 and pt==0 and c!=' ':\r\n",
        "            if tag_on:\r\n",
        "                end=ix\r\n",
        "                tag_on=False\r\n",
        "                #opin=ET.Element(\"aspectTerm\")\r\n",
        "                tweet['topic']=tweet['full_text'][start:end]\r\n",
        "                tweet[\"from\"]=str(start)\r\n",
        "                tweet['to']=str(end)\r\n",
        "                #opins.append(opin)\r\n",
        "            start=ix\r\n",
        "            tag_on=True\r\n",
        "        elif token_idx<len(tokens) and lb[token_idx]==2 and pt==0 and c!=' ' and not tag_on:\r\n",
        "            start=ix\r\n",
        "            tag_on=True\r\n",
        "        elif token_idx<len(tokens) and (lb[token_idx]==0 or lb[token_idx]==1) and tag_on and pt==0:\r\n",
        "            end=ix\r\n",
        "            tag_on=False \r\n",
        "            #opin=ET.Element(\"aspectTerm\")\r\n",
        "            tweet['topic']=tweet['full_text'][start:end]\r\n",
        "            tweet[\"from\"]=str(start)\r\n",
        "            tweet['to']=str(end)\r\n",
        "            #opins.append(opin)\r\n",
        "        elif token_idx>=len(tokens) and tag_on:\r\n",
        "            end=ix\r\n",
        "            tag_on=False \r\n",
        "            #opin=ET.Element(\"aspectTerm\")\r\n",
        "            tweet['topic']=tweet['full_text'][start:end]\r\n",
        "            tweet[\"from\"]=str(start)\r\n",
        "            tweet['to']=str(end)\r\n",
        "            #opins.append(opin)\r\n",
        "        if c==' ' or ord(c)==160:\r\n",
        "            pass\r\n",
        "        elif tokens[token_idx][pt:pt+2]=='``' or tokens[token_idx][pt:pt+2]==\"''\":\r\n",
        "            pt+=2\r\n",
        "        else:\r\n",
        "            pt+=1\r\n",
        "    if tag_on:\r\n",
        "        tag_on=False\r\n",
        "        end=len(tweet['full_text'])\r\n",
        "        #opin=ET.Element(\"aspectTerm\")\r\n",
        "        tweet['topic']=tweet['full_text'][start:end]\r\n",
        "        tweet[\"from\"]=str(start)\r\n",
        "        tweet['to']=str(end)\r\n",
        "        #opins.append(opin)\r\n",
        "    tweets.at[zx,'topic'] = tweet['topic'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRXcN20LH22B"
      },
      "source": [
        "Prepare test examples based on topics, gained due to AE Task    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPJUx-9U2OED"
      },
      "source": [
        "test_corpus = asc_task(data = tweets) \r\n",
        "write_json(\"test.json\", corpus = test_corpus, target_dir = \"\")\r\n",
        "\r\n",
        "eval_batch_size = 8\r\n",
        "eval_examples = get_test_examples(data_dir = \"test.json\")\r\n",
        "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer, \"asc\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axEqYuBZdaH0",
        "outputId": "3f4afcfb-d54e-4a37-da8c-649236acedb5"
      },
      "source": [
        "\r\n",
        "\r\n",
        "logger.info(\"***** Running evaluation *****\")\r\n",
        "logger.info(\"  Num examples = %d\", len(eval_examples))\r\n",
        "logger.info(\"  Batch size = %d\", eval_batch_size)\r\n",
        "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\r\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\r\n",
        "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\r\n",
        "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\r\n",
        "eval_data = TensorDataset(all_input_ids, all_segment_ids, all_input_mask, all_label_ids)\r\n",
        "# Run prediction for full data\r\n",
        "eval_sampler = SequentialSampler(eval_data)\r\n",
        "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size= eval_batch_size)\r\n",
        "\r\n",
        "model = torch.load(\"model.pt\" )\r\n",
        "model.cuda()\r\n",
        "model.eval()\r\n",
        "\r\n",
        "full_logits=[]\r\n",
        "full_label_ids=[]\r\n",
        "for step, batch in enumerate(eval_dataloader):\r\n",
        "    batch = tuple(t.cuda() for t in batch)\r\n",
        "    input_ids, segment_ids, input_mask, label_ids = batch\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "        logits = model(input_ids, segment_ids, input_mask)\r\n",
        "\r\n",
        "    logits = logits.detach().cpu().numpy()\r\n",
        "    label_ids = label_ids.cpu().numpy()\r\n",
        "\r\n",
        "    full_logits.extend(logits.tolist() )\r\n",
        "    full_label_ids.extend(label_ids.tolist() )\r\n",
        "\r\n",
        "with open(\"predictions_asc.json\", \"w\") as fw:\r\n",
        "    json.dump({\"logits\": full_logits, \"label_ids\": full_label_ids}, fw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 23:37:22 - INFO - __main__ -   ***** Running evaluation *****\n",
            "02/09/2021 23:37:22 - INFO - __main__ -     Num examples = 2\n",
            "02/09/2021 23:37:22 - INFO - __main__ -     Batch size = 8\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AixrI4WeNbe"
      },
      "source": [
        "# **Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "d4fSpYE8eQDG",
        "outputId": "b058cfdc-29e6-495f-b78f-c69371184b27"
      },
      "source": [
        "# Various evaluation metrics\r\n",
        "\r\n",
        "with open( \"predictions_asc.json\" ) as f:\r\n",
        "    results=json.load(f)\r\n",
        "y_true=results['label_ids']\r\n",
        "y_pred=[np.argmax(logit) for logit in results['logits'] ]\r\n",
        "p_macro, r_macro, f_macro, _=sklearn.metrics.precision_recall_fscore_support(y_true, y_pred, average='macro')\r\n",
        "f_macro = 2*p_macro*r_macro/(p_macro+r_macro)\r\n",
        "acc = 100*sklearn.metrics.accuracy_score(y_true, y_pred)\r\n",
        "f_mac = 100*f_macro \r\n",
        "\r\n",
        "# Confusion Matrix\r\n",
        "display_confusion_matrix(true_labels = y_true, predicted_labels = y_pred)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">Predicted:</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">Actual:</th>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Predicted:   \n",
              "                   1  0\n",
              "Actual: 1          1  0\n",
              "        0          0  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    }
  ]
}