{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Post-Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgi6-OZ_45Ap"
      },
      "source": [
        "# BERT Post-Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ljDguR9B5Lo"
      },
      "source": [
        "By Asmik Nalmpatian and Lisa Wimmer\n",
        "\n",
        "Last edited on 31.01.21\n",
        "\n",
        "For our consulting project: Aspect-Based Sentiment Analysis for Twitter Data of German MPs\n",
        "\n",
        "Methodology based on: https://www.aclweb.org/anthology/N19-1242.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlSJNFMbMDwi"
      },
      "source": [
        "**Prepare GPU:**\n",
        "\n",
        "1. Check: Edit --> Notebook settings -> Hardware accelerator -> *GPU* \n",
        "\n",
        "\n",
        "2. Datasets are uploaded in *content*-folder: *germeval.txt* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcIJCx_BwQk"
      },
      "source": [
        "# Google Colab GPU Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJo83hwzCFWk",
        "outputId": "fd5a9dc4-3281-4d7a-c980-eff7e716d26f"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBgJAuCSCHHU",
        "outputId": "68b98845-8d8f-4c82-9b4c-0a5b9fd43092"
      },
      "source": [
        "# Identify and specify the GPU as the device\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sym8VMcUCI9f"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65Bv29YjMpNq",
        "outputId": "318759d0-c801-4059-c5ef-5a415b5380ed"
      },
      "source": [
        "# # Clear RAM for more memory\n",
        "#!nvidia-smi\n",
        "#!kill 418.67"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb  9 13:57:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    25W /  75W |   7603MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "/bin/bash: line 0: kill: 418.67: arguments must be process or job IDs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjOa4v4pX9EL"
      },
      "source": [
        "#!pip install torch==1.4.0\n",
        "#!pip install transformers #==3.5.1\n",
        "#!pip install pytorch_pretrained_bert\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import random\n",
        "import json\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#from transformers import BertTokenizer\n",
        "#from pytorch_pretrained_bert import tokenization\n",
        "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertModel, BertPreTrainingHeads, BertPreTrainedModel \n",
        "from pytorch_pretrained_bert.optimization import BertAdam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8pIv_CQgAHz"
      },
      "source": [
        "# Prepare Tweets for Post-Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olHypK0WhoTz"
      },
      "source": [
        "Create functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKgt3rQrgctV"
      },
      "source": [
        "class TrainingInstance(object):\n",
        "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
        "\n",
        "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
        "                 is_random_next):\n",
        "        self.tokens = tokens\n",
        "        self.segment_ids = segment_ids\n",
        "        self.is_random_next = is_random_next\n",
        "        self.masked_lm_positions = masked_lm_positions\n",
        "        self.masked_lm_labels = masked_lm_labels\n",
        "\n",
        "    def __str__(self):\n",
        "        s = \"\"\n",
        "        s += \"tokens: %s\\n\" % (\" \".join(\n",
        "            [x for x in self.tokens]))\n",
        "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x)\n",
        "                                              for x in self.segment_ids]))\n",
        "        s += \"is_random_next: %s\\n\" % self.is_random_next\n",
        "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
        "            [str(x) for x in self.masked_lm_positions]))\n",
        "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
        "            [x for x in self.masked_lm_labels]))\n",
        "        s += \"\\n\"\n",
        "        return s\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP2JxyqhghX0"
      },
      "source": [
        "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
        "                                    max_predictions_per_seq, output_files):\n",
        "    \"\"\"Create np example files from `TrainingInstance`s.\"\"\"\n",
        "    \n",
        "    input_ids_np= np.zeros((len(instances), max_seq_length), np.int16)\n",
        "    segment_ids_np = np.zeros((len(instances), max_seq_length), np.int16)\n",
        "    input_mask_np = np.zeros((len(instances), max_seq_length), np.int16)\n",
        "    masked_lm_ids_np = -np.ones((len(instances), max_seq_length), np.int16)\n",
        "    masked_lm_weights_np = np.zeros((len(instances), max_seq_length), np.int16)\n",
        "    next_sentence_labels_np = np.zeros((len(instances), ), np.int16)\n",
        "    \n",
        "    for (inst_index, instance) in enumerate(instances):\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
        "        segment_ids = list(instance.segment_ids)\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        assert len(input_ids) <= max_seq_length\n",
        "\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(0)\n",
        "            segment_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "\n",
        "        masked_lm_ids = [-1]*len(instance.tokens)\n",
        "        for ix, ids in enumerate(tokenizer.convert_tokens_to_ids(instance.masked_lm_labels) ):\n",
        "            masked_lm_ids[instance.masked_lm_positions[ix]]=ids\n",
        "        masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
        "\n",
        "        while len(masked_lm_ids) < max_seq_length:\n",
        "            masked_lm_ids.append(-1) #ignore index for pytorch\n",
        "            masked_lm_weights.append(0.0)\n",
        "\n",
        "        next_sentence_label = 1 if instance.is_random_next else 0\n",
        "            \n",
        "        input_ids_np[inst_index]=input_ids \n",
        "        segment_ids_np[inst_index] = segment_ids\n",
        "        input_mask_np[inst_index] = input_mask\n",
        "        masked_lm_ids_np[inst_index] = masked_lm_ids\n",
        "        masked_lm_weights_np[inst_index] = masked_lm_weights\n",
        "        next_sentence_labels_np[inst_index] = next_sentence_label\n",
        "\n",
        "        if inst_index < 5:\n",
        "            logging.info(\"*** Example ***\")\n",
        "            logging.info(\"tokens: %s\" % \" \".join([x for x in instance.tokens]))\n",
        "        \n",
        "    np.savez_compressed(output_files, \n",
        "                        input_ids=input_ids_np, \n",
        "                        input_mask = input_mask_np, \n",
        "                        segment_ids = segment_ids_np, \n",
        "                        masked_lm_ids = masked_lm_ids_np, \n",
        "                        masked_lm_weights = masked_lm_weights_np, \n",
        "                        next_sentence_labels = next_sentence_labels_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C08RhFgCgqFB"
      },
      "source": [
        "def create_training_instances(input_files, tokenizer, max_seq_length,\n",
        "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
        "                              max_predictions_per_seq, rng):\n",
        "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
        "    all_documents = [[]]\n",
        "\n",
        "    # Input file format:\n",
        "    # (1) One sentence per line. These should ideally be actual sentences, not\n",
        "    # entire paragraphs or arbitrary spans of text. (Because we use the\n",
        "    # sentence boundaries for the \"next sentence prediction\" task).\n",
        "    # (2) Blank lines between documents. Document boundaries are needed so\n",
        "    # that the \"next sentence prediction\" task doesn't span between documents.\n",
        "\n",
        "    with open(input_files, \"r\") as reader:\n",
        "        while True:\n",
        "            line = reader.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            line = line.strip()\n",
        "\n",
        "            # Empty lines are used as document delimiters\n",
        "            if not line:\n",
        "                all_documents.append([])\n",
        "            tokens = tokenizer.tokenize(line)\n",
        "            if tokens:\n",
        "                all_documents[-1].append(tokens)\n",
        "\n",
        "    # Remove empty documents\n",
        "    all_documents = [x for x in all_documents if x]\n",
        "    rng.shuffle(all_documents)\n",
        "\n",
        "    vocab_words = list(tokenizer.vocab.keys())\n",
        "    instances = []\n",
        "    for _ in range(dupe_factor):\n",
        "        for document_index in range(len(all_documents)):\n",
        "            instances.extend(\n",
        "                create_instances_from_document(\n",
        "                    all_documents, document_index, max_seq_length, short_seq_prob,\n",
        "                    masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
        "\n",
        "    rng.shuffle(instances)\n",
        "    return instances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATsdgbuegvaU"
      },
      "source": [
        "def create_instances_from_document(\n",
        "        all_documents, document_index, max_seq_length, short_seq_prob,\n",
        "        masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
        "    \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
        "    document = all_documents[document_index]\n",
        "\n",
        "    # Account for [CLS], [SEP], [SEP]\n",
        "    max_num_tokens = max_seq_length - 3\n",
        "\n",
        "    # We *usually* want to fill up the entire sequence since we are padding\n",
        "    # to `max_seq_length` anyways, so short sequences are generally wasted\n",
        "    # computation. However, we *sometimes*\n",
        "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
        "    # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
        "    # The `target_seq_length` is just a rough target however, whereas\n",
        "    # `max_seq_length` is a hard limit.\n",
        "    target_seq_length = max_num_tokens\n",
        "    if rng.random() < short_seq_prob:\n",
        "        target_seq_length = rng.randint(2, max_num_tokens)\n",
        "\n",
        "    # We DON'T just concatenate all of the tokens from a document into a long\n",
        "    # sequence and choose an arbitrary split point because this would make the\n",
        "    # next sentence prediction task too easy. Instead, we split the input into\n",
        "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
        "    # input.\n",
        "    instances = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    i = 0\n",
        "    while i < len(document):\n",
        "        segment = document[i]\n",
        "        current_chunk.append(segment)\n",
        "        current_length += len(segment)\n",
        "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
        "            if current_chunk:\n",
        "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
        "                # (first) sentence.\n",
        "                a_end = 1\n",
        "                if len(current_chunk) >= 2:\n",
        "                    a_end = rng.randint(1, len(current_chunk) - 1)\n",
        "\n",
        "                tokens_a = []\n",
        "                for j in range(a_end):\n",
        "                    tokens_a.extend(current_chunk[j])\n",
        "\n",
        "                tokens_b = []\n",
        "                # Random next\n",
        "                is_random_next = False\n",
        "                if len(current_chunk) == 1 or rng.random() < 0.5:\n",
        "                    is_random_next = True\n",
        "                    target_b_length = target_seq_length - len(tokens_a)\n",
        "\n",
        "                    # This should rarely go for more than one iteration for large\n",
        "                    # corpora. However, just to be careful, we try to make sure that\n",
        "                    # the random document is not the same as the document\n",
        "                    # we're processing.\n",
        "                    for _ in range(10):\n",
        "                        random_document_index = rng.randint(\n",
        "                            0, len(all_documents) - 1)\n",
        "                        if random_document_index != document_index:\n",
        "                            break\n",
        "\n",
        "                    random_document = all_documents[random_document_index]\n",
        "                    random_start = rng.randint(0, len(random_document) - 1)\n",
        "                    for j in range(random_start, len(random_document)):\n",
        "                        tokens_b.extend(random_document[j])\n",
        "                        if len(tokens_b) >= target_b_length:\n",
        "                            break\n",
        "                    # We didn't actually use these segments so we \"put them back\" so\n",
        "                    # they don't go to waste.\n",
        "                    num_unused_segments = len(current_chunk) - a_end\n",
        "                    i -= num_unused_segments\n",
        "                # Actual next\n",
        "                else:\n",
        "                    is_random_next = False\n",
        "                    for j in range(a_end, len(current_chunk)):\n",
        "                        tokens_b.extend(current_chunk[j])\n",
        "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
        "\n",
        "                assert len(tokens_a) >= 1\n",
        "                assert len(tokens_b) >= 1\n",
        "\n",
        "                tokens = []\n",
        "                segment_ids = []\n",
        "                tokens.append(\"[CLS]\")\n",
        "                segment_ids.append(0)\n",
        "                for token in tokens_a:\n",
        "                    tokens.append(token)\n",
        "                    segment_ids.append(0)\n",
        "\n",
        "                tokens.append(\"[SEP]\")\n",
        "                segment_ids.append(0)\n",
        "\n",
        "                for token in tokens_b:\n",
        "                    tokens.append(token)\n",
        "                    segment_ids.append(1)\n",
        "                tokens.append(\"[SEP]\")\n",
        "                segment_ids.append(1)\n",
        "\n",
        "                (tokens, masked_lm_positions,\n",
        "                 masked_lm_labels) = create_masked_lm_predictions(\n",
        "                     tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
        "                instance = TrainingInstance(\n",
        "                    tokens=tokens,\n",
        "                    segment_ids=segment_ids,\n",
        "                    is_random_next=is_random_next,\n",
        "                    masked_lm_positions=masked_lm_positions,\n",
        "                    masked_lm_labels=masked_lm_labels)\n",
        "                instances.append(instance)\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "        i += 1\n",
        "\n",
        "    return instances\n",
        "\n",
        "\n",
        "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
        "                                          [\"index\", \"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6_HJd_Hgyoi"
      },
      "source": [
        "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
        "                                 max_predictions_per_seq, vocab_words, rng):\n",
        "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
        "\n",
        "    cand_indexes = []\n",
        "    for (i, token) in enumerate(tokens):\n",
        "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
        "            continue\n",
        "        cand_indexes.append(i)\n",
        "\n",
        "    rng.shuffle(cand_indexes)\n",
        "\n",
        "    output_tokens = list(tokens)\n",
        "\n",
        "    num_to_predict = min(max_predictions_per_seq,\n",
        "                         max(1, int(round(len(tokens) * masked_lm_prob))))\n",
        "\n",
        "    masked_lms = []\n",
        "    covered_indexes = set()\n",
        "    for index in cand_indexes:\n",
        "        if len(masked_lms) >= num_to_predict:\n",
        "            break\n",
        "        if index in covered_indexes:\n",
        "            continue\n",
        "        covered_indexes.add(index)\n",
        "\n",
        "        masked_token = None\n",
        "        # 80% of the time, replace with [MASK]\n",
        "        if rng.random() < 0.8:\n",
        "            masked_token = \"[MASK]\"\n",
        "        else:\n",
        "            # 10% of the time, keep original\n",
        "            if rng.random() < 0.5:\n",
        "                masked_token = tokens[index]\n",
        "            # 10% of the time, replace with random word\n",
        "            else:\n",
        "                masked_token = vocab_words[rng.randint(\n",
        "                    0, len(vocab_words) - 1)]\n",
        "\n",
        "        output_tokens[index] = masked_token\n",
        "\n",
        "        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
        "\n",
        "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
        "\n",
        "    masked_lm_positions = []\n",
        "    masked_lm_labels = []\n",
        "    for p in masked_lms:\n",
        "        masked_lm_positions.append(p.index)\n",
        "        masked_lm_labels.append(p.label)\n",
        "\n",
        "    return (output_tokens, masked_lm_positions, masked_lm_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vlQM0VYg08o"
      },
      "source": [
        "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
        "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_num_tokens:\n",
        "            break\n",
        "\n",
        "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
        "        assert len(trunc_tokens) >= 1\n",
        "\n",
        "        # We want to sometimes truncate from the front and sometimes from the\n",
        "        # back to add more randomness and avoid biases.\n",
        "        if rng.random() < 0.5:\n",
        "            del trunc_tokens[0]\n",
        "        else:\n",
        "            trunc_tokens.pop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_KZARawhibQ"
      },
      "source": [
        "Apply: Create training instances out of Germeval Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tROroPhj6E7w"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('./drive/MyDrive/BERT_Files/BERT_Post-Training/tweets_unlabeled.txt' , delimiter=',', header = 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs0PzqJT6NW3",
        "outputId": "842a7166-3c9b-49a5-cf68-c26e9420f3b0"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29715"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsjIooVzhsal"
      },
      "source": [
        "random.seed(12345)\n",
        "np.random.seed(12345)\n",
        "    \n",
        "tokenizer = BertTokenizer.from_pretrained(\"./drive/MyDrive/BERT_Files/bert_german_cased/\") # \"bert-base-german-cased\") \n",
        "rng = random.Random(12345)\n",
        "    \n",
        "instances = create_training_instances(\n",
        "        input_files = './drive/MyDrive/BERT_Files/BERT_Post-Training/tweets_unlabeled.txt', # germeval.txt\n",
        "        tokenizer = tokenizer, \n",
        "        max_seq_length = 320, \n",
        "        dupe_factor = 10, \n",
        "        short_seq_prob = 0.1, \n",
        "        masked_lm_prob = 0.15, \n",
        "        max_predictions_per_seq = 40, \n",
        "        rng = rng)\n",
        "\n",
        "write_instance_to_example_files(\n",
        "    instances = instances, \n",
        "    tokenizer = tokenizer, \n",
        "    max_seq_length = 320, \n",
        "    max_predictions_per_seq = 40, \n",
        "    output_files = \"tweets_unlabeled.npz\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRJYm-QCgY0z"
      },
      "source": [
        "# The actual training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1w6Ex2-ZS-N"
      },
      "source": [
        "class BertForMTPostTraining(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super(BertForMTPostTraining, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
        "        self.qa_outputs = torch.nn.Linear(config.hidden_size, 2)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, mode, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None, start_positions=None, end_positions=None):\n",
        "        \n",
        "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        \n",
        "        if mode==\"review\":\n",
        "            prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "            if masked_lm_labels is not None and next_sentence_label is not None:\n",
        "                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "                masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))        \n",
        "                next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "                total_loss = masked_lm_loss + next_sentence_loss\n",
        "                \n",
        "                return total_loss\n",
        "            else:\n",
        "                return prediction_scores, seq_relationship_score\n",
        "\n",
        "        elif mode==\"squad\":\n",
        "            logits = self.qa_outputs(sequence_output)\n",
        "            start_logits, end_logits = logits.split(1, dim=-1)\n",
        "            start_logits = start_logits.squeeze(-1)\n",
        "            end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "            if start_positions is not None and end_positions is not None:\n",
        "                # If we are on multi-GPU, split add a dimension\n",
        "                if len(start_positions.size()) > 1:\n",
        "                    start_positions = start_positions.squeeze(-1)\n",
        "                if len(end_positions.size()) > 1:\n",
        "                    end_positions = end_positions.squeeze(-1)\n",
        "                # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "                ignored_index = start_logits.size(1)\n",
        "                start_positions.clamp_(0, ignored_index)\n",
        "                end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
        "                start_loss = loss_fct(start_logits, start_positions)\n",
        "                end_loss = loss_fct(end_logits, end_positions)\n",
        "                qa_loss = (start_loss + end_loss) / 2\n",
        "                return qa_loss\n",
        "            else:\n",
        "                return start_logits, end_logits\n",
        "        else:\n",
        "            raise Exception(\"unknown mode.\")\n",
        "\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up3NaA-iZUHy"
      },
      "source": [
        "def warmup_linear(x, warmup=0.002):\n",
        "    if x < warmup:\n",
        "        return x/warmup\n",
        "    return 1.0 - x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cge4g_LIZYil",
        "outputId": "c7fa7b5b-9abc-4420-976b-02bba7ec549e"
      },
      "source": [
        "# Training arguments\n",
        "\n",
        "gradient_accumulation_steps = 2\n",
        "train_batch_size = int(16 / gradient_accumulation_steps)   \n",
        "review_data_dir = ''\n",
        "num_train_steps = 12\n",
        "fp16 = False\n",
        "learning_rate = 3e-5\n",
        "warmup_proportion = 0.1\n",
        "loss_scale = 2\n",
        "save_checkpoints_steps = 10000\n",
        "output_dir = ''\n",
        "random.seed(12345)\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efbb0088630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pgUitTNZ9SC"
      },
      "source": [
        "review_train_examples = np.load(\"tweets_unlabeled.npz\")\n",
        "    \n",
        "# load bert pre-train data.\n",
        "review_train_data = TensorDataset(\n",
        "    torch.from_numpy(review_train_examples[\"input_ids\"]),\n",
        "    torch.from_numpy(review_train_examples[\"segment_ids\"]),\n",
        "    torch.from_numpy(review_train_examples[\"input_mask\"]),        \n",
        "    torch.from_numpy(review_train_examples[\"masked_lm_ids\"]),\n",
        "    torch.from_numpy(review_train_examples[\"next_sentence_labels\"]) )\n",
        "    \n",
        "review_train_dataloader = DataLoader(review_train_data, \n",
        "                                     sampler=RandomSampler(review_train_data), \n",
        "                                     batch_size = train_batch_size , \n",
        "                                     drop_last=True)\n",
        "\n",
        "# we do not have any validation for pretuning\n",
        "model = BertForMTPostTraining.from_pretrained(\"./drive/MyDrive/BERT_Files/bert_german_cased/\") # \"bert-base-german-cased\"\n",
        "\n",
        "model.cuda()\n",
        "        \n",
        "# Prepare optimizer\n",
        "param_optimizer = [(k, v) for k, v in model.named_parameters() if v.requires_grad==True]\n",
        "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "t_total = num_train_steps\n",
        "        \n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                             lr = learning_rate,\n",
        "                             warmup = warmup_proportion,\n",
        "                             t_total = t_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc2uLiWjaDsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53acc42-3aee-4b6f-e65d-9cf2ff03e2e1"
      },
      "source": [
        "global_step=0\n",
        "step=0\n",
        "batch_loss=0.\n",
        "model.train()\n",
        "model.zero_grad()\n",
        "    \n",
        "training=True\n",
        "\n",
        "review_iter=iter(review_train_dataloader)\n",
        "    \n",
        "while training:\n",
        "    try:\n",
        "        batch = next(review_iter)\n",
        "    except:\n",
        "        review_iter=iter(review_train_dataloader)\n",
        "        batch = next(review_iter)\n",
        "        \n",
        "    batch = tuple(t.cuda() for t in batch)\n",
        "    \n",
        "    input_ids, segment_ids, input_mask, masked_lm_ids, next_sentence_labels = batch\n",
        "    \n",
        "    review_loss = model(\"review\", input_ids.long(), segment_ids.long(), input_mask.long(), masked_lm_ids.long(), next_sentence_labels.long(), None, None)\n",
        "\n",
        "\n",
        "    batch = tuple(t.cuda() for t in batch)\n",
        "    input_ids, segment_ids, input_mask, start_positions, end_positions = batch\n",
        "\n",
        "    loss=review_loss \n",
        "\n",
        "    if gradient_accumulation_steps > 1:\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "    batch_loss+=loss\n",
        "    if fp16:\n",
        "        optimizer.backward(loss)\n",
        "    else:\n",
        "        loss.backward()\n",
        "    \n",
        "    if (step + 1) % gradient_accumulation_steps == 0:\n",
        "        # modify learning rate with special warm up BERT uses\n",
        "        lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr_this_step\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "        if global_step % 50 ==0:\n",
        "            logging.info(\"step %d batch_loss %f \", global_step, batch_loss)\n",
        "        batch_loss=0.\n",
        "\n",
        "        if global_step % save_checkpoints_steps==0:\n",
        "            model.float()\n",
        "            torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model_\"+str(global_step)+\".bin\") )\n",
        "            if args.fp16:\n",
        "                model.half()\n",
        "        if global_step>=num_train_steps:\n",
        "            training=False\n",
        "            break\n",
        "    step+=1\n",
        "model.float()\n",
        "torch.save(model.state_dict(), \"pytorch_model.bin\" )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}