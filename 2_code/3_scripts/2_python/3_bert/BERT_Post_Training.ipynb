{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Post-Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgi6-OZ_45Ap"
      },
      "source": [
        "# BERT Post-Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ljDguR9B5Lo"
      },
      "source": [
        "By Asmik Nalmpatian and Lisa Wimmer\r\n",
        "\r\n",
        "Last edited on 31.01.21\r\n",
        "\r\n",
        "For our consulting project: Aspect-Based Sentiment Analysis for Twitter Data of German MPs\r\n",
        "\r\n",
        "Methodology based on: https://www.aclweb.org/anthology/N19-1242.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcIJCx_BwQk"
      },
      "source": [
        "# Google Colab GPU Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJo83hwzCFWk",
        "outputId": "3fcb5cfa-bd12-43b0-9f5e-5a011bd84468"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Get the GPU device name.\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "# The device name should look like the following:\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBgJAuCSCHHU",
        "outputId": "1d297d12-744e-4757-f0e2-d1e59187823c"
      },
      "source": [
        "# Identify and specify the GPU as the device\r\n",
        "import torch\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sym8VMcUCI9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e11061-d473-48c0-addb-077b70b74f0f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65Bv29YjMpNq",
        "outputId": "318759d0-c801-4059-c5ef-5a415b5380ed"
      },
      "source": [
        "# # Clear RAM for more memory\r\n",
        "#!nvidia-smi\r\n",
        "#!kill 418.67"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Feb  9 13:57:43 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    25W /  75W |   7603MiB /  7611MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "/bin/bash: line 0: kill: 418.67: arguments must be process or job IDs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjOa4v4pX9EL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "131142dc-df22-42d6-98ae-a1369ecc5770"
      },
      "source": [
        "!pip install transformers==3.5.1\r\n",
        "!pip install pytorch_pretrained_bert\r\n",
        "\r\n",
        "import logging\r\n",
        "import argparse\r\n",
        "import random\r\n",
        "import json\r\n",
        "import collections\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "from transformers import BertTokenizer\r\n",
        "#from pytorch_pretrained_bert import tokenization\r\n",
        "#from pytorch_pretrained_bert.tokenization import BertTokenizer\r\n",
        "from pytorch_pretrained_bert.modeling import BertModel, BertPreTrainingHeads, BertPreTrainedModel \r\n",
        "from pytorch_pretrained_bert.optimization import BertAdam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.5.1 in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.8)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (20.9)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.9.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.1) (53.0.0)\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.7.0+cu101)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.4 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.20.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.21.0,>=1.20.4->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.4->boto3->pytorch_pretrained_bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8pIv_CQgAHz"
      },
      "source": [
        "# Prepare Tweets for Post-Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olHypK0WhoTz"
      },
      "source": [
        "Create functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKgt3rQrgctV"
      },
      "source": [
        "class TrainingInstance(object):\r\n",
        "    \"\"\"A single training instance (sentence pair).\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\r\n",
        "                 is_random_next):\r\n",
        "        self.tokens = tokens\r\n",
        "        self.segment_ids = segment_ids\r\n",
        "        self.is_random_next = is_random_next\r\n",
        "        self.masked_lm_positions = masked_lm_positions\r\n",
        "        self.masked_lm_labels = masked_lm_labels\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        s = \"\"\r\n",
        "        s += \"tokens: %s\\n\" % (\" \".join(\r\n",
        "            [x for x in self.tokens]))\r\n",
        "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x)\r\n",
        "                                              for x in self.segment_ids]))\r\n",
        "        s += \"is_random_next: %s\\n\" % self.is_random_next\r\n",
        "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\r\n",
        "            [str(x) for x in self.masked_lm_positions]))\r\n",
        "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\r\n",
        "            [x for x in self.masked_lm_labels]))\r\n",
        "        s += \"\\n\"\r\n",
        "        return s\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return self.__str__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP2JxyqhghX0"
      },
      "source": [
        "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\r\n",
        "                                    max_predictions_per_seq, output_files):\r\n",
        "    \"\"\"Create np example files from `TrainingInstance`s.\"\"\"\r\n",
        "    \r\n",
        "    input_ids_np= np.zeros((len(instances), max_seq_length), np.int16)\r\n",
        "    segment_ids_np = np.zeros((len(instances), max_seq_length), np.int16)\r\n",
        "    input_mask_np = np.zeros((len(instances), max_seq_length), np.int16)\r\n",
        "    masked_lm_ids_np = -np.ones((len(instances), max_seq_length), np.int16)\r\n",
        "    masked_lm_weights_np = np.zeros((len(instances), max_seq_length), np.int16)\r\n",
        "    next_sentence_labels_np = np.zeros((len(instances), ), np.int16)\r\n",
        "    \r\n",
        "    for (inst_index, instance) in enumerate(instances):\r\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\r\n",
        "        segment_ids = list(instance.segment_ids)\r\n",
        "        input_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "        assert len(input_ids) <= max_seq_length\r\n",
        "\r\n",
        "        while len(input_ids) < max_seq_length:\r\n",
        "            input_ids.append(0)\r\n",
        "            segment_ids.append(0)\r\n",
        "            input_mask.append(0)\r\n",
        "\r\n",
        "        assert len(input_ids) == max_seq_length\r\n",
        "        assert len(segment_ids) == max_seq_length\r\n",
        "        assert len(input_mask) == max_seq_length\r\n",
        "\r\n",
        "        masked_lm_ids = [-1]*len(instance.tokens)\r\n",
        "        for ix, ids in enumerate(tokenizer.convert_tokens_to_ids(instance.masked_lm_labels) ):\r\n",
        "            masked_lm_ids[instance.masked_lm_positions[ix]]=ids\r\n",
        "        masked_lm_weights = [1.0] * len(masked_lm_ids)\r\n",
        "\r\n",
        "        while len(masked_lm_ids) < max_seq_length:\r\n",
        "            masked_lm_ids.append(-1) #ignore index for pytorch\r\n",
        "            masked_lm_weights.append(0.0)\r\n",
        "\r\n",
        "        next_sentence_label = 1 if instance.is_random_next else 0\r\n",
        "            \r\n",
        "        input_ids_np[inst_index]=input_ids \r\n",
        "        segment_ids_np[inst_index] = segment_ids\r\n",
        "        input_mask_np[inst_index] = input_mask\r\n",
        "        masked_lm_ids_np[inst_index] = masked_lm_ids\r\n",
        "        masked_lm_weights_np[inst_index] = masked_lm_weights\r\n",
        "        next_sentence_labels_np[inst_index] = next_sentence_label\r\n",
        "\r\n",
        "        if inst_index < 5:\r\n",
        "            logging.info(\"*** Example ***\")\r\n",
        "            logging.info(\"tokens: %s\" % \" \".join([x for x in instance.tokens]))\r\n",
        "        \r\n",
        "    np.savez_compressed(output_files, \r\n",
        "                        input_ids=input_ids_np, \r\n",
        "                        input_mask = input_mask_np, \r\n",
        "                        segment_ids = segment_ids_np, \r\n",
        "                        masked_lm_ids = masked_lm_ids_np, \r\n",
        "                        masked_lm_weights = masked_lm_weights_np, \r\n",
        "                        next_sentence_labels = next_sentence_labels_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C08RhFgCgqFB"
      },
      "source": [
        "def create_training_instances(input_files, tokenizer, max_seq_length,\r\n",
        "                              dupe_factor, short_seq_prob, masked_lm_prob,\r\n",
        "                              max_predictions_per_seq, rng):\r\n",
        "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\r\n",
        "    all_documents = [[]]\r\n",
        "\r\n",
        "    # Input file format:\r\n",
        "    # (1) One sentence per line. These should ideally be actual sentences, not\r\n",
        "    # entire paragraphs or arbitrary spans of text. (Because we use the\r\n",
        "    # sentence boundaries for the \"next sentence prediction\" task).\r\n",
        "    # (2) Blank lines between documents. Document boundaries are needed so\r\n",
        "    # that the \"next sentence prediction\" task doesn't span between documents.\r\n",
        "\r\n",
        "    with open(input_files, \"r\") as reader:\r\n",
        "        while True:\r\n",
        "            line = reader.readline()\r\n",
        "            if not line:\r\n",
        "                break\r\n",
        "            line = line.strip()\r\n",
        "\r\n",
        "            # Empty lines are used as document delimiters\r\n",
        "            if not line:\r\n",
        "                all_documents.append([])\r\n",
        "            tokens = tokenizer.tokenize(line)\r\n",
        "            if tokens:\r\n",
        "                all_documents[-1].append(tokens)\r\n",
        "\r\n",
        "    # Remove empty documents\r\n",
        "    all_documents = [x for x in all_documents if x]\r\n",
        "    rng.shuffle(all_documents)\r\n",
        "\r\n",
        "    vocab_words = list(tokenizer.vocab.keys())\r\n",
        "    instances = []\r\n",
        "    for _ in range(dupe_factor):\r\n",
        "        for document_index in range(len(all_documents)):\r\n",
        "            instances.extend(\r\n",
        "                create_instances_from_document(\r\n",
        "                    all_documents, document_index, max_seq_length, short_seq_prob,\r\n",
        "                    masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\r\n",
        "\r\n",
        "    rng.shuffle(instances)\r\n",
        "    return instances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATsdgbuegvaU"
      },
      "source": [
        "def create_instances_from_document(\r\n",
        "        all_documents, document_index, max_seq_length, short_seq_prob,\r\n",
        "        masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\r\n",
        "    \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\r\n",
        "    document = all_documents[document_index]\r\n",
        "\r\n",
        "    # Account for [CLS], [SEP], [SEP]\r\n",
        "    max_num_tokens = max_seq_length - 3\r\n",
        "\r\n",
        "    # We *usually* want to fill up the entire sequence since we are padding\r\n",
        "    # to `max_seq_length` anyways, so short sequences are generally wasted\r\n",
        "    # computation. However, we *sometimes*\r\n",
        "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\r\n",
        "    # sequences to minimize the mismatch between pre-training and fine-tuning.\r\n",
        "    # The `target_seq_length` is just a rough target however, whereas\r\n",
        "    # `max_seq_length` is a hard limit.\r\n",
        "    target_seq_length = max_num_tokens\r\n",
        "    if rng.random() < short_seq_prob:\r\n",
        "        target_seq_length = rng.randint(2, max_num_tokens)\r\n",
        "\r\n",
        "    # We DON'T just concatenate all of the tokens from a document into a long\r\n",
        "    # sequence and choose an arbitrary split point because this would make the\r\n",
        "    # next sentence prediction task too easy. Instead, we split the input into\r\n",
        "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\r\n",
        "    # input.\r\n",
        "    instances = []\r\n",
        "    current_chunk = []\r\n",
        "    current_length = 0\r\n",
        "    i = 0\r\n",
        "    while i < len(document):\r\n",
        "        segment = document[i]\r\n",
        "        current_chunk.append(segment)\r\n",
        "        current_length += len(segment)\r\n",
        "        if i == len(document) - 1 or current_length >= target_seq_length:\r\n",
        "            if current_chunk:\r\n",
        "                # `a_end` is how many segments from `current_chunk` go into the `A`\r\n",
        "                # (first) sentence.\r\n",
        "                a_end = 1\r\n",
        "                if len(current_chunk) >= 2:\r\n",
        "                    a_end = rng.randint(1, len(current_chunk) - 1)\r\n",
        "\r\n",
        "                tokens_a = []\r\n",
        "                for j in range(a_end):\r\n",
        "                    tokens_a.extend(current_chunk[j])\r\n",
        "\r\n",
        "                tokens_b = []\r\n",
        "                # Random next\r\n",
        "                is_random_next = False\r\n",
        "                if len(current_chunk) == 1 or rng.random() < 0.5:\r\n",
        "                    is_random_next = True\r\n",
        "                    target_b_length = target_seq_length - len(tokens_a)\r\n",
        "\r\n",
        "                    # This should rarely go for more than one iteration for large\r\n",
        "                    # corpora. However, just to be careful, we try to make sure that\r\n",
        "                    # the random document is not the same as the document\r\n",
        "                    # we're processing.\r\n",
        "                    for _ in range(10):\r\n",
        "                        random_document_index = rng.randint(\r\n",
        "                            0, len(all_documents) - 1)\r\n",
        "                        if random_document_index != document_index:\r\n",
        "                            break\r\n",
        "\r\n",
        "                    random_document = all_documents[random_document_index]\r\n",
        "                    random_start = rng.randint(0, len(random_document) - 1)\r\n",
        "                    for j in range(random_start, len(random_document)):\r\n",
        "                        tokens_b.extend(random_document[j])\r\n",
        "                        if len(tokens_b) >= target_b_length:\r\n",
        "                            break\r\n",
        "                    # We didn't actually use these segments so we \"put them back\" so\r\n",
        "                    # they don't go to waste.\r\n",
        "                    num_unused_segments = len(current_chunk) - a_end\r\n",
        "                    i -= num_unused_segments\r\n",
        "                # Actual next\r\n",
        "                else:\r\n",
        "                    is_random_next = False\r\n",
        "                    for j in range(a_end, len(current_chunk)):\r\n",
        "                        tokens_b.extend(current_chunk[j])\r\n",
        "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\r\n",
        "\r\n",
        "                assert len(tokens_a) >= 1\r\n",
        "                assert len(tokens_b) >= 1\r\n",
        "\r\n",
        "                tokens = []\r\n",
        "                segment_ids = []\r\n",
        "                tokens.append(\"[CLS]\")\r\n",
        "                segment_ids.append(0)\r\n",
        "                for token in tokens_a:\r\n",
        "                    tokens.append(token)\r\n",
        "                    segment_ids.append(0)\r\n",
        "\r\n",
        "                tokens.append(\"[SEP]\")\r\n",
        "                segment_ids.append(0)\r\n",
        "\r\n",
        "                for token in tokens_b:\r\n",
        "                    tokens.append(token)\r\n",
        "                    segment_ids.append(1)\r\n",
        "                tokens.append(\"[SEP]\")\r\n",
        "                segment_ids.append(1)\r\n",
        "\r\n",
        "                (tokens, masked_lm_positions,\r\n",
        "                 masked_lm_labels) = create_masked_lm_predictions(\r\n",
        "                     tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\r\n",
        "                instance = TrainingInstance(\r\n",
        "                    tokens=tokens,\r\n",
        "                    segment_ids=segment_ids,\r\n",
        "                    is_random_next=is_random_next,\r\n",
        "                    masked_lm_positions=masked_lm_positions,\r\n",
        "                    masked_lm_labels=masked_lm_labels)\r\n",
        "                instances.append(instance)\r\n",
        "            current_chunk = []\r\n",
        "            current_length = 0\r\n",
        "        i += 1\r\n",
        "\r\n",
        "    return instances\r\n",
        "\r\n",
        "\r\n",
        "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\r\n",
        "                                          [\"index\", \"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6_HJd_Hgyoi"
      },
      "source": [
        "def create_masked_lm_predictions(tokens, masked_lm_prob,\r\n",
        "                                 max_predictions_per_seq, vocab_words, rng):\r\n",
        "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\r\n",
        "\r\n",
        "    cand_indexes = []\r\n",
        "    for (i, token) in enumerate(tokens):\r\n",
        "        if token == \"[CLS]\" or token == \"[SEP]\":\r\n",
        "            continue\r\n",
        "        cand_indexes.append(i)\r\n",
        "\r\n",
        "    rng.shuffle(cand_indexes)\r\n",
        "\r\n",
        "    output_tokens = list(tokens)\r\n",
        "\r\n",
        "    num_to_predict = min(max_predictions_per_seq,\r\n",
        "                         max(1, int(round(len(tokens) * masked_lm_prob))))\r\n",
        "\r\n",
        "    masked_lms = []\r\n",
        "    covered_indexes = set()\r\n",
        "    for index in cand_indexes:\r\n",
        "        if len(masked_lms) >= num_to_predict:\r\n",
        "            break\r\n",
        "        if index in covered_indexes:\r\n",
        "            continue\r\n",
        "        covered_indexes.add(index)\r\n",
        "\r\n",
        "        masked_token = None\r\n",
        "        # 80% of the time, replace with [MASK]\r\n",
        "        if rng.random() < 0.8:\r\n",
        "            masked_token = \"[MASK]\"\r\n",
        "        else:\r\n",
        "            # 10% of the time, keep original\r\n",
        "            if rng.random() < 0.5:\r\n",
        "                masked_token = tokens[index]\r\n",
        "            # 10% of the time, replace with random word\r\n",
        "            else:\r\n",
        "                masked_token = vocab_words[rng.randint(\r\n",
        "                    0, len(vocab_words) - 1)]\r\n",
        "\r\n",
        "        output_tokens[index] = masked_token\r\n",
        "\r\n",
        "        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\r\n",
        "\r\n",
        "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\r\n",
        "\r\n",
        "    masked_lm_positions = []\r\n",
        "    masked_lm_labels = []\r\n",
        "    for p in masked_lms:\r\n",
        "        masked_lm_positions.append(p.index)\r\n",
        "        masked_lm_labels.append(p.label)\r\n",
        "\r\n",
        "    return (output_tokens, masked_lm_positions, masked_lm_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vlQM0VYg08o"
      },
      "source": [
        "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\r\n",
        "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\r\n",
        "    while True:\r\n",
        "        total_length = len(tokens_a) + len(tokens_b)\r\n",
        "        if total_length <= max_num_tokens:\r\n",
        "            break\r\n",
        "\r\n",
        "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\r\n",
        "        assert len(trunc_tokens) >= 1\r\n",
        "\r\n",
        "        # We want to sometimes truncate from the front and sometimes from the\r\n",
        "        # back to add more randomness and avoid biases.\r\n",
        "        if rng.random() < 0.5:\r\n",
        "            del trunc_tokens[0]\r\n",
        "        else:\r\n",
        "            trunc_tokens.pop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_KZARawhibQ"
      },
      "source": [
        "Apply: Create training instances out of Germeval Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsjIooVzhsal"
      },
      "source": [
        "random.seed(12345)\r\n",
        "np.random.seed(12345)\r\n",
        "    \r\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\") \r\n",
        "rng = random.Random(12345)\r\n",
        "    \r\n",
        "instances = create_training_instances(\r\n",
        "        input_files = 'germeval.txt', \r\n",
        "        tokenizer = tokenizer, \r\n",
        "        max_seq_length = 320, \r\n",
        "        dupe_factor = 10, \r\n",
        "        short_seq_prob = 0.1, \r\n",
        "        masked_lm_prob = 0.15, \r\n",
        "        max_predictions_per_seq = 40, \r\n",
        "        rng = rng)\r\n",
        "\r\n",
        "write_instance_to_example_files(\r\n",
        "    instances = instances, \r\n",
        "    tokenizer = tokenizer, \r\n",
        "    max_seq_length = 320, \r\n",
        "    max_predictions_per_seq = 40, \r\n",
        "    output_files = \"germeval.npz\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRJYm-QCgY0z"
      },
      "source": [
        "# The actual training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1w6Ex2-ZS-N"
      },
      "source": [
        "class BertForMTPostTraining(BertPreTrainedModel):\r\n",
        "    def __init__(self, config):\r\n",
        "        super(BertForMTPostTraining, self).__init__(config)\r\n",
        "        self.bert = BertModel(config)\r\n",
        "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\r\n",
        "        self.qa_outputs = torch.nn.Linear(config.hidden_size, 2)\r\n",
        "        self.apply(self.init_bert_weights)\r\n",
        "\r\n",
        "    def forward(self, mode, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None, start_positions=None, end_positions=None):\r\n",
        "        \r\n",
        "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\r\n",
        "        \r\n",
        "        if mode==\"review\":\r\n",
        "            prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\r\n",
        "            if masked_lm_labels is not None and next_sentence_label is not None:\r\n",
        "                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\r\n",
        "                masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))        \r\n",
        "                next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\r\n",
        "                total_loss = masked_lm_loss + next_sentence_loss\r\n",
        "                \r\n",
        "                return total_loss\r\n",
        "            else:\r\n",
        "                return prediction_scores, seq_relationship_score\r\n",
        "\r\n",
        "        elif mode==\"squad\":\r\n",
        "            logits = self.qa_outputs(sequence_output)\r\n",
        "            start_logits, end_logits = logits.split(1, dim=-1)\r\n",
        "            start_logits = start_logits.squeeze(-1)\r\n",
        "            end_logits = end_logits.squeeze(-1)\r\n",
        "\r\n",
        "            if start_positions is not None and end_positions is not None:\r\n",
        "                # If we are on multi-GPU, split add a dimension\r\n",
        "                if len(start_positions.size()) > 1:\r\n",
        "                    start_positions = start_positions.squeeze(-1)\r\n",
        "                if len(end_positions.size()) > 1:\r\n",
        "                    end_positions = end_positions.squeeze(-1)\r\n",
        "                # sometimes the start/end positions are outside our model inputs, we ignore these terms\r\n",
        "                ignored_index = start_logits.size(1)\r\n",
        "                start_positions.clamp_(0, ignored_index)\r\n",
        "                end_positions.clamp_(0, ignored_index)\r\n",
        "\r\n",
        "                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)\r\n",
        "                start_loss = loss_fct(start_logits, start_positions)\r\n",
        "                end_loss = loss_fct(end_logits, end_positions)\r\n",
        "                qa_loss = (start_loss + end_loss) / 2\r\n",
        "                return qa_loss\r\n",
        "            else:\r\n",
        "                return start_logits, end_logits\r\n",
        "        else:\r\n",
        "            raise Exception(\"unknown mode.\")\r\n",
        "\r\n",
        "\r\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\r\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\r\n",
        "                    level=logging.INFO)\r\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up3NaA-iZUHy"
      },
      "source": [
        "def warmup_linear(x, warmup=0.002):\r\n",
        "    if x < warmup:\r\n",
        "        return x/warmup\r\n",
        "    return 1.0 - x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cge4g_LIZYil",
        "outputId": "40e1823c-d67b-4f31-b712-c206768740ec"
      },
      "source": [
        "# Training arguments\r\n",
        "\r\n",
        "gradient_accumulation_steps = 2\r\n",
        "train_batch_size = int(16 / gradient_accumulation_steps)   \r\n",
        "review_data_dir = ''\r\n",
        "num_train_steps = 12\r\n",
        "fp16 = False\r\n",
        "learning_rate = 3e-5\r\n",
        "warmup_proportion = 0.1\r\n",
        "loss_scale = 2\r\n",
        "save_checkpoints_steps = 10000\r\n",
        "output_dir = ''\r\n",
        "random.seed(12345)\r\n",
        "np.random.seed(12345)\r\n",
        "torch.manual_seed(12345)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f9e12b8a2a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "3pgUitTNZ9SC",
        "outputId": "3cef5bed-5b5a-49b4-a828-d067dbac7a6e"
      },
      "source": [
        "review_train_examples = np.load(\"germeval.npz\")\r\n",
        "    \r\n",
        "# load bert pre-train data.\r\n",
        "review_train_data = TensorDataset(\r\n",
        "    torch.from_numpy(review_train_examples[\"input_ids\"]),\r\n",
        "    torch.from_numpy(review_train_examples[\"segment_ids\"]),\r\n",
        "    torch.from_numpy(review_train_examples[\"input_mask\"]),        \r\n",
        "    torch.from_numpy(review_train_examples[\"masked_lm_ids\"]),\r\n",
        "    torch.from_numpy(review_train_examples[\"next_sentence_labels\"]) )\r\n",
        "    \r\n",
        "review_train_dataloader = DataLoader(review_train_data, \r\n",
        "                                     sampler=RandomSampler(review_train_data), \r\n",
        "                                     batch_size = train_batch_size , \r\n",
        "                                     drop_last=True)\r\n",
        "\r\n",
        "# we do not have any validation for pretuning\r\n",
        "model = BertForMTPostTraining.from_pretrained(\"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased.tar.gz\") # \"bert-base-german-cased\"\r\n",
        "\r\n",
        "model.cuda()\r\n",
        "        \r\n",
        "# Prepare optimizer\r\n",
        "param_optimizer = [(k, v) for k, v in model.named_parameters() if v.requires_grad==True]\r\n",
        "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\r\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\r\n",
        "optimizer_grouped_parameters = [\r\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\r\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\r\n",
        "    ]\r\n",
        "t_total = num_train_steps\r\n",
        "        \r\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\r\n",
        "                             lr = learning_rate,\r\n",
        "                             warmup = warmup_proportion,\r\n",
        "                             t_total = t_total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-6e9c5fbb3938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMTPostTraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased.tar.gz\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# \"bert-base-german-cased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Prepare optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 88.00 MiB (GPU 0; 7.43 GiB total capacity; 6.58 GiB already allocated; 14.94 MiB free; 6.69 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc2uLiWjaDsB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "512347a1-4e2f-4702-e043-b80f3204f108"
      },
      "source": [
        "global_step=0\r\n",
        "step=0\r\n",
        "batch_loss=0.\r\n",
        "model.train()\r\n",
        "model.zero_grad()\r\n",
        "    \r\n",
        "training=True\r\n",
        "\r\n",
        "review_iter=iter(review_train_dataloader)\r\n",
        "    \r\n",
        "while training:\r\n",
        "    try:\r\n",
        "        batch = next(review_iter)\r\n",
        "    except:\r\n",
        "        review_iter=iter(review_train_dataloader)\r\n",
        "        batch = next(review_iter)\r\n",
        "        \r\n",
        "    batch = tuple(t.cuda() for t in batch)\r\n",
        "    \r\n",
        "    input_ids, segment_ids, input_mask, masked_lm_ids, next_sentence_labels = batch\r\n",
        "    \r\n",
        "    review_loss = model(\"review\", input_ids.long(), segment_ids.long(), input_mask.long(), masked_lm_ids.long(), next_sentence_labels.long(), None, None)\r\n",
        "\r\n",
        "\r\n",
        "    batch = tuple(t.cuda() for t in batch)\r\n",
        "    input_ids, segment_ids, input_mask, start_positions, end_positions = batch\r\n",
        "\r\n",
        "    loss=review_loss \r\n",
        "\r\n",
        "    if gradient_accumulation_steps > 1:\r\n",
        "        loss = loss / gradient_accumulation_steps\r\n",
        "    batch_loss+=loss\r\n",
        "    if fp16:\r\n",
        "        optimizer.backward(loss)\r\n",
        "    else:\r\n",
        "        loss.backward()\r\n",
        "    \r\n",
        "    if (step + 1) % gradient_accumulation_steps == 0:\r\n",
        "        # modify learning rate with special warm up BERT uses\r\n",
        "        lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\r\n",
        "        for param_group in optimizer.param_groups:\r\n",
        "            param_group['lr'] = lr_this_step\r\n",
        "        optimizer.step()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        global_step += 1\r\n",
        "        if global_step % 50 ==0:\r\n",
        "            logging.info(\"step %d batch_loss %f \", global_step, batch_loss)\r\n",
        "        batch_loss=0.\r\n",
        "\r\n",
        "        if global_step % save_checkpoints_steps==0:\r\n",
        "            model.float()\r\n",
        "            torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model_\"+str(global_step)+\".bin\") )\r\n",
        "            if args.fp16:\r\n",
        "                model.half()\r\n",
        "        if global_step>=num_train_steps:\r\n",
        "            training=False\r\n",
        "            break\r\n",
        "    step+=1\r\n",
        "model.float()\r\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\") )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-cdf23cc878cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_sentence_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mreview_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_sentence_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-7691f85645a1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, mode, input_ids, token_type_ids, attention_mask, masked_lm_labels, next_sentence_label, start_positions, end_positions)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_sentence_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 0; 7.43 GiB total capacity; 6.48 GiB already allocated; 8.94 MiB free; 6.70 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    }
  ]
}