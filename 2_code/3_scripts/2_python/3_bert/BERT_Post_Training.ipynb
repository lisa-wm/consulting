{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Post-Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgi6-OZ_45Ap"
      },
      "source": [
        "# BERT Post-Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ljDguR9B5Lo"
      },
      "source": [
        "By Asmik Nalmpatian and Lisa Wimmer\r\n",
        "\r\n",
        "Last edited on 31.01.21\r\n",
        "\r\n",
        "For our consulting project: Aspect-Based Sentiment Analysis for Twitter Data of German MPs\r\n",
        "\r\n",
        "Methodology based on: https://www.aclweb.org/anthology/N19-1242.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcIJCx_BwQk"
      },
      "source": [
        "# Google Colab GPU Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJo83hwzCFWk",
        "outputId": "751733a2-11c8-48f6-99a6-313934d35353"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Get the GPU device name.\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "# The device name should look like the following:\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBgJAuCSCHHU",
        "outputId": "8f8f1316-646b-4ee5-9ed8-66b2a0e67533"
      },
      "source": [
        "# Identify and specify the GPU as the device\r\n",
        "import torch\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sym8VMcUCI9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f48720-6b20-45fe-a39b-a64fa16b3a75"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RugJz3lYCKwI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d80dba03-d322-4751-e715-bfc929193dba"
      },
      "source": [
        "# Where to save our data\r\n",
        "import os, sys\r\n",
        "\r\n",
        "os.getcwd()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjOa4v4pX9EL"
      },
      "source": [
        "import logging\r\n",
        "import argparse\r\n",
        "import random\r\n",
        "import json\r\n",
        "import collections\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "from transformers import BertTokenizer\r\n",
        "#from pytorch_pretrained_bert import tokenization\r\n",
        "#from pytorch_pretrained_bert.tokenization import BertTokenizer\r\n",
        "from pytorch_pretrained_bert.modeling import BertModel, BertPreTrainingHeads, BertPreTrainedModel \r\n",
        "from pytorch_pretrained_bert.optimization import BertAdam"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8pIv_CQgAHz"
      },
      "source": [
        "# Prepare Tweets for Post-Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olHypK0WhoTz"
      },
      "source": [
        "Create functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKgt3rQrgctV"
      },
      "source": [
        "class TrainingInstance(object):\r\n",
        "    \"\"\"A single training instance (sentence pair).\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\r\n",
        "                 is_random_next):\r\n",
        "        self.tokens = tokens\r\n",
        "        self.segment_ids = segment_ids\r\n",
        "        self.is_random_next = is_random_next\r\n",
        "        self.masked_lm_positions = masked_lm_positions\r\n",
        "        self.masked_lm_labels = masked_lm_labels\r\n",
        "\r\n",
        "    def __str__(self):\r\n",
        "        s = \"\"\r\n",
        "        s += \"tokens: %s\\n\" % (\" \".join(\r\n",
        "            [x for x in self.tokens]))\r\n",
        "        s += \"segment_ids: %s\\n\" % (\" \".join([str(x)\r\n",
        "                                              for x in self.segment_ids]))\r\n",
        "        s += \"is_random_next: %s\\n\" % self.is_random_next\r\n",
        "        s += \"masked_lm_positions: %s\\n\" % (\" \".join(\r\n",
        "            [str(x) for x in self.masked_lm_positions]))\r\n",
        "        s += \"masked_lm_labels: %s\\n\" % (\" \".join(\r\n",
        "            [x for x in self.masked_lm_labels]))\r\n",
        "        s += \"\\n\"\r\n",
        "        return s\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        return self.__str__()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP2JxyqhghX0"
      },
      "source": [
        "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\r\n",
        "                                    max_predictions_per_seq, output_files):\r\n",
        "    \"\"\"Create np example files from `TrainingInstance`s.\"\"\"\r\n",
        "    \r\n",
        "    input_ids_np= np.zeros((len(instances), max_seq_length), np.int16)\r\n",
        "    segment_ids_np = np.zeros((len(instances), max_seq_length), np.int16)\r\n",
        "    input_mask_np = np.zeros((len(instances), max_seq_length), np.int16)\r\n",
        "    masked_lm_ids_np = -np.ones((len(instances), max_seq_length), np.int16)\r\n",
        "    masked_lm_weights_np = np.zeros((len(instances), max_seq_length), np.int16)\r\n",
        "    next_sentence_labels_np = np.zeros((len(instances), ), np.int16)\r\n",
        "    \r\n",
        "    for (inst_index, instance) in enumerate(instances):\r\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\r\n",
        "        segment_ids = list(instance.segment_ids)\r\n",
        "        input_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "        assert len(input_ids) <= max_seq_length\r\n",
        "\r\n",
        "        while len(input_ids) < max_seq_length:\r\n",
        "            input_ids.append(0)\r\n",
        "            segment_ids.append(0)\r\n",
        "            input_mask.append(0)\r\n",
        "\r\n",
        "        assert len(input_ids) == max_seq_length\r\n",
        "        assert len(segment_ids) == max_seq_length\r\n",
        "        assert len(input_mask) == max_seq_length\r\n",
        "\r\n",
        "        masked_lm_ids = [-1]*len(instance.tokens)\r\n",
        "        for ix, ids in enumerate(tokenizer.convert_tokens_to_ids(instance.masked_lm_labels) ):\r\n",
        "            masked_lm_ids[instance.masked_lm_positions[ix]]=ids\r\n",
        "        masked_lm_weights = [1.0] * len(masked_lm_ids)\r\n",
        "\r\n",
        "        while len(masked_lm_ids) < max_seq_length:\r\n",
        "            masked_lm_ids.append(-1) #ignore index for pytorch\r\n",
        "            masked_lm_weights.append(0.0)\r\n",
        "\r\n",
        "        next_sentence_label = 1 if instance.is_random_next else 0\r\n",
        "            \r\n",
        "        input_ids_np[inst_index]=input_ids \r\n",
        "        segment_ids_np[inst_index] = segment_ids\r\n",
        "        input_mask_np[inst_index] = input_mask\r\n",
        "        masked_lm_ids_np[inst_index] = masked_lm_ids\r\n",
        "        masked_lm_weights_np[inst_index] = masked_lm_weights\r\n",
        "        next_sentence_labels_np[inst_index] = next_sentence_label\r\n",
        "\r\n",
        "        if inst_index < 5:\r\n",
        "            logging.info(\"*** Example ***\")\r\n",
        "            logging.info(\"tokens: %s\" % \" \".join([x for x in instance.tokens]))\r\n",
        "        \r\n",
        "    np.savez_compressed(output_files, \r\n",
        "                        input_ids=input_ids_np, \r\n",
        "                        input_mask = input_mask_np, \r\n",
        "                        segment_ids = segment_ids_np, \r\n",
        "                        masked_lm_ids = masked_lm_ids_np, \r\n",
        "                        masked_lm_weights = masked_lm_weights_np, \r\n",
        "                        next_sentence_labels = next_sentence_labels_np)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C08RhFgCgqFB"
      },
      "source": [
        "def create_training_instances(input_files, tokenizer, max_seq_length,\r\n",
        "                              dupe_factor, short_seq_prob, masked_lm_prob,\r\n",
        "                              max_predictions_per_seq, rng):\r\n",
        "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\r\n",
        "    all_documents = [[]]\r\n",
        "\r\n",
        "    # Input file format:\r\n",
        "    # (1) One sentence per line. These should ideally be actual sentences, not\r\n",
        "    # entire paragraphs or arbitrary spans of text. (Because we use the\r\n",
        "    # sentence boundaries for the \"next sentence prediction\" task).\r\n",
        "    # (2) Blank lines between documents. Document boundaries are needed so\r\n",
        "    # that the \"next sentence prediction\" task doesn't span between documents.\r\n",
        "\r\n",
        "    with open(input_files, \"r\") as reader:\r\n",
        "        while True:\r\n",
        "            line = reader.readline()\r\n",
        "            if not line:\r\n",
        "                break\r\n",
        "            line = line.strip()\r\n",
        "\r\n",
        "            # Empty lines are used as document delimiters\r\n",
        "            if not line:\r\n",
        "                all_documents.append([])\r\n",
        "            tokens = tokenizer.tokenize(line)\r\n",
        "            if tokens:\r\n",
        "                all_documents[-1].append(tokens)\r\n",
        "\r\n",
        "    # Remove empty documents\r\n",
        "    all_documents = [x for x in all_documents if x]\r\n",
        "    rng.shuffle(all_documents)\r\n",
        "\r\n",
        "    vocab_words = list(tokenizer.vocab.keys())\r\n",
        "    instances = []\r\n",
        "    for _ in range(dupe_factor):\r\n",
        "        for document_index in range(len(all_documents)):\r\n",
        "            instances.extend(\r\n",
        "                create_instances_from_document(\r\n",
        "                    all_documents, document_index, max_seq_length, short_seq_prob,\r\n",
        "                    masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\r\n",
        "\r\n",
        "    rng.shuffle(instances)\r\n",
        "    return instances"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATsdgbuegvaU"
      },
      "source": [
        "def create_instances_from_document(\r\n",
        "        all_documents, document_index, max_seq_length, short_seq_prob,\r\n",
        "        masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\r\n",
        "    \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\r\n",
        "    document = all_documents[document_index]\r\n",
        "\r\n",
        "    # Account for [CLS], [SEP], [SEP]\r\n",
        "    max_num_tokens = max_seq_length - 3\r\n",
        "\r\n",
        "    # We *usually* want to fill up the entire sequence since we are padding\r\n",
        "    # to `max_seq_length` anyways, so short sequences are generally wasted\r\n",
        "    # computation. However, we *sometimes*\r\n",
        "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\r\n",
        "    # sequences to minimize the mismatch between pre-training and fine-tuning.\r\n",
        "    # The `target_seq_length` is just a rough target however, whereas\r\n",
        "    # `max_seq_length` is a hard limit.\r\n",
        "    target_seq_length = max_num_tokens\r\n",
        "    if rng.random() < short_seq_prob:\r\n",
        "        target_seq_length = rng.randint(2, max_num_tokens)\r\n",
        "\r\n",
        "    # We DON'T just concatenate all of the tokens from a document into a long\r\n",
        "    # sequence and choose an arbitrary split point because this would make the\r\n",
        "    # next sentence prediction task too easy. Instead, we split the input into\r\n",
        "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\r\n",
        "    # input.\r\n",
        "    instances = []\r\n",
        "    current_chunk = []\r\n",
        "    current_length = 0\r\n",
        "    i = 0\r\n",
        "    while i < len(document):\r\n",
        "        segment = document[i]\r\n",
        "        current_chunk.append(segment)\r\n",
        "        current_length += len(segment)\r\n",
        "        if i == len(document) - 1 or current_length >= target_seq_length:\r\n",
        "            if current_chunk:\r\n",
        "                # `a_end` is how many segments from `current_chunk` go into the `A`\r\n",
        "                # (first) sentence.\r\n",
        "                a_end = 1\r\n",
        "                if len(current_chunk) >= 2:\r\n",
        "                    a_end = rng.randint(1, len(current_chunk) - 1)\r\n",
        "\r\n",
        "                tokens_a = []\r\n",
        "                for j in range(a_end):\r\n",
        "                    tokens_a.extend(current_chunk[j])\r\n",
        "\r\n",
        "                tokens_b = []\r\n",
        "                # Random next\r\n",
        "                is_random_next = False\r\n",
        "                if len(current_chunk) == 1 or rng.random() < 0.5:\r\n",
        "                    is_random_next = True\r\n",
        "                    target_b_length = target_seq_length - len(tokens_a)\r\n",
        "\r\n",
        "                    # This should rarely go for more than one iteration for large\r\n",
        "                    # corpora. However, just to be careful, we try to make sure that\r\n",
        "                    # the random document is not the same as the document\r\n",
        "                    # we're processing.\r\n",
        "                    for _ in range(10):\r\n",
        "                        random_document_index = rng.randint(\r\n",
        "                            0, len(all_documents) - 1)\r\n",
        "                        if random_document_index != document_index:\r\n",
        "                            break\r\n",
        "\r\n",
        "                    random_document = all_documents[random_document_index]\r\n",
        "                    random_start = rng.randint(0, len(random_document) - 1)\r\n",
        "                    for j in range(random_start, len(random_document)):\r\n",
        "                        tokens_b.extend(random_document[j])\r\n",
        "                        if len(tokens_b) >= target_b_length:\r\n",
        "                            break\r\n",
        "                    # We didn't actually use these segments so we \"put them back\" so\r\n",
        "                    # they don't go to waste.\r\n",
        "                    num_unused_segments = len(current_chunk) - a_end\r\n",
        "                    i -= num_unused_segments\r\n",
        "                # Actual next\r\n",
        "                else:\r\n",
        "                    is_random_next = False\r\n",
        "                    for j in range(a_end, len(current_chunk)):\r\n",
        "                        tokens_b.extend(current_chunk[j])\r\n",
        "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\r\n",
        "\r\n",
        "                assert len(tokens_a) >= 1\r\n",
        "                assert len(tokens_b) >= 1\r\n",
        "\r\n",
        "                tokens = []\r\n",
        "                segment_ids = []\r\n",
        "                tokens.append(\"[CLS]\")\r\n",
        "                segment_ids.append(0)\r\n",
        "                for token in tokens_a:\r\n",
        "                    tokens.append(token)\r\n",
        "                    segment_ids.append(0)\r\n",
        "\r\n",
        "                tokens.append(\"[SEP]\")\r\n",
        "                segment_ids.append(0)\r\n",
        "\r\n",
        "                for token in tokens_b:\r\n",
        "                    tokens.append(token)\r\n",
        "                    segment_ids.append(1)\r\n",
        "                tokens.append(\"[SEP]\")\r\n",
        "                segment_ids.append(1)\r\n",
        "\r\n",
        "                (tokens, masked_lm_positions,\r\n",
        "                 masked_lm_labels) = create_masked_lm_predictions(\r\n",
        "                     tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\r\n",
        "                instance = TrainingInstance(\r\n",
        "                    tokens=tokens,\r\n",
        "                    segment_ids=segment_ids,\r\n",
        "                    is_random_next=is_random_next,\r\n",
        "                    masked_lm_positions=masked_lm_positions,\r\n",
        "                    masked_lm_labels=masked_lm_labels)\r\n",
        "                instances.append(instance)\r\n",
        "            current_chunk = []\r\n",
        "            current_length = 0\r\n",
        "        i += 1\r\n",
        "\r\n",
        "    return instances\r\n",
        "\r\n",
        "\r\n",
        "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\r\n",
        "                                          [\"index\", \"label\"])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6_HJd_Hgyoi"
      },
      "source": [
        "def create_masked_lm_predictions(tokens, masked_lm_prob,\r\n",
        "                                 max_predictions_per_seq, vocab_words, rng):\r\n",
        "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\r\n",
        "\r\n",
        "    cand_indexes = []\r\n",
        "    for (i, token) in enumerate(tokens):\r\n",
        "        if token == \"[CLS]\" or token == \"[SEP]\":\r\n",
        "            continue\r\n",
        "        cand_indexes.append(i)\r\n",
        "\r\n",
        "    rng.shuffle(cand_indexes)\r\n",
        "\r\n",
        "    output_tokens = list(tokens)\r\n",
        "\r\n",
        "    num_to_predict = min(max_predictions_per_seq,\r\n",
        "                         max(1, int(round(len(tokens) * masked_lm_prob))))\r\n",
        "\r\n",
        "    masked_lms = []\r\n",
        "    covered_indexes = set()\r\n",
        "    for index in cand_indexes:\r\n",
        "        if len(masked_lms) >= num_to_predict:\r\n",
        "            break\r\n",
        "        if index in covered_indexes:\r\n",
        "            continue\r\n",
        "        covered_indexes.add(index)\r\n",
        "\r\n",
        "        masked_token = None\r\n",
        "        # 80% of the time, replace with [MASK]\r\n",
        "        if rng.random() < 0.8:\r\n",
        "            masked_token = \"[MASK]\"\r\n",
        "        else:\r\n",
        "            # 10% of the time, keep original\r\n",
        "            if rng.random() < 0.5:\r\n",
        "                masked_token = tokens[index]\r\n",
        "            # 10% of the time, replace with random word\r\n",
        "            else:\r\n",
        "                masked_token = vocab_words[rng.randint(\r\n",
        "                    0, len(vocab_words) - 1)]\r\n",
        "\r\n",
        "        output_tokens[index] = masked_token\r\n",
        "\r\n",
        "        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\r\n",
        "\r\n",
        "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\r\n",
        "\r\n",
        "    masked_lm_positions = []\r\n",
        "    masked_lm_labels = []\r\n",
        "    for p in masked_lms:\r\n",
        "        masked_lm_positions.append(p.index)\r\n",
        "        masked_lm_labels.append(p.label)\r\n",
        "\r\n",
        "    return (output_tokens, masked_lm_positions, masked_lm_labels)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vlQM0VYg08o"
      },
      "source": [
        "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\r\n",
        "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\r\n",
        "    while True:\r\n",
        "        total_length = len(tokens_a) + len(tokens_b)\r\n",
        "        if total_length <= max_num_tokens:\r\n",
        "            break\r\n",
        "\r\n",
        "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\r\n",
        "        assert len(trunc_tokens) >= 1\r\n",
        "\r\n",
        "        # We want to sometimes truncate from the front and sometimes from the\r\n",
        "        # back to add more randomness and avoid biases.\r\n",
        "        if rng.random() < 0.5:\r\n",
        "            del trunc_tokens[0]\r\n",
        "        else:\r\n",
        "            trunc_tokens.pop()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_KZARawhibQ"
      },
      "source": [
        "Apply: Create training instances out of Germeval Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsjIooVzhsal",
        "outputId": "85ddd2ff-8003-461a-9aaf-69c44576836b"
      },
      "source": [
        "random.seed(12345)\r\n",
        "np.random.seed(12345)\r\n",
        "    \r\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\") \r\n",
        "rng = random.Random(12345)\r\n",
        "    \r\n",
        "instances = create_training_instances(\r\n",
        "        input_files = 'germeval.txt', \r\n",
        "        tokenizer = tokenizer, \r\n",
        "        max_seq_length = 320, \r\n",
        "        dupe_factor = 10, \r\n",
        "        short_seq_prob = 0.1, \r\n",
        "        masked_lm_prob = 0.15, \r\n",
        "        max_predictions_per_seq = 40, \r\n",
        "        rng = rng)\r\n",
        "\r\n",
        "write_instance_to_example_files(\r\n",
        "    instances = instances, \r\n",
        "    tokenizer = tokenizer, \r\n",
        "    max_seq_length = 320, \r\n",
        "    max_predictions_per_seq = 40, \r\n",
        "    output_files = \"germeval.npz\")\r\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/01/2021 01:40:53 - INFO - root -   *** Example ***\n",
            "02/01/2021 01:40:53 - INFO - root -   tokens: [CLS] Karte [MASK] Vereinigte ##t ##chen ( ol ##d sch ##ool , ich [MASK] ) , die ich in der Schreibt ##isch ##schub ##lade aufbewahr ##e . Hab ##e ich unterwegs eine Idee , not ##iere ich sie mir im Handy . Ich gebe zu , [MASK] habe einen gewissen [MASK] ##ion ##arischen Eif ##er [MASK] [MASK] Anfang fand ich es sch ##ade , dass die liebsten Hör ##bücher meiner Kinder in keiner Buch ##handlung vor [MASK] ##ig waren . Da dachte [MASK] , [MASK] man doch irgendwie darauf aufmerksam machen muss ! Auch heute [unused815] ich noch Spaß [MASK] , schöne Bücher und Hör [MASK] vorzu ##stellen rät hoffe , dass ich sie ganz oft an die passen ##den Leser und Hör ##er bringen [MASK] . Ich bin eine Ku [MASK] ##lerin ! Bücher , Bücher [MASK] Bücher . 14 . Welche ##s war de ##in [MASK] [MASK] Blog ##er ##leb ##nis ? Ich fre ##ue mich immer darüber , wenn ich eine Rück ##meldung bekom ##me , dass einem Kind ein Buch / Hör ##buch , das [MASK] meiner Empfehlung geschenkt wurde , wirklich gut gefallen hat . Dem ##nächst werde [MASK] [MASK] meinen vielen Rezension ##sex ##empl ##aren einen öffentlichen Bücher ##schrank für Kinder eröffnen [UNK] [MASK] kleine Bücher ##ei [MASK] vor unserer Haust ##ür . Ich fre ##ue mich [MASK] darauf , ##assungen Kinder aus der Nachbarschaft [unused2389] [MASK] Lese ##freu ##de teil ##haben zu lassen [MASK] 15 [MASK] Was würde dir ohne die [UNK] Blog ##os ##ph ##äre [UNK] fehlen ? Zeichnungen wäre es sicher entsp ##annen ##d , wieder nur für mich [MASK] lesen [SEP] \" Re : DB Bahn Diese Kredit - Karte , wurde ##ierender den letzten [MASK] Jahren in den [MASK] [MASK] Costa Ric ##a [MASK] Maur ##iti ##us , Dub ##ai und auf den Se ##ych ##ellen akzeptiert , nur immer Probleme bei der DB in den ICE \" [SEP]\n",
            "02/01/2021 01:40:53 - INFO - root -   *** Example ***\n",
            "02/01/2021 01:40:53 - INFO - root -   tokens: [CLS] : / / www . fa ##ce ##bo ##ok . c ##om / 14 ##36 ##01 ##88 ##73 ##42 / post ##s / 101 ##53 ##14 ##86 ##07 ##78 ##23 [MASK] # 14 ##36 ##01 ##88 [MASK] ##42 [MASK] _ 101 [MASK] ##14 [MASK] ##07 ##78 ##23 ##43 [MASK] ##wa ##eld ##er - bot ##e . de [MASK] [MASK] ##W / F ##DS : Gä ##uba ##hn : [MASK] ändert Fahrplan - ##konzert ##wälder Bot ##e Pend ##ler aufge ##pass ##t : Die DB Bahn ä ##nde ##t ab dem 13 Staatsange Dezember den Fahrplan der # [MASK] ##uba ##hn und verspricht verkürzt ##e Reise ##zeiten . [MASK] [MASK] sich verkürzt ##e Reise ##zeiten für Fahrgäste zwischen Stuttgart und Sing ##en . Maßnahme ist [MASK] eines Fern [MASK] ##konzept ##s . t ##ru [MASK] positive Zug ##fahrt # Fahrt ##zeit _ und _ Schnell [MASK] : positive Zug [MASK] # Fahrt ##zeit _ und _ Schnell ##igkeit : positive [SEP] ##s . In Äthiop ##ien [MASK] derweil europäische Firmen großfläch ##ig und bill [MASK] Gemüse an , bezüglich zwei Tage nach der Ernte in den Küchen der Nobel ##hotel ##s von Dub ##ai teuer verkauft wird . In der ä ##thiop ##ischen Land ##bevölkerung gibt es Gewinner und Verlierer dieses Trend ##s . [MASK] ##oren haben viele [MASK] den [MASK] zu Acker ##bö ##den und Wasser . Gew [MASK] haben diejenigen , die sieben Tage in [MASK] Woche 12 bis 14 [MASK] pro Tag auf [MASK] Gemüse ##plant ##age arbeiten können und glücklich sind , von den [MASK] , 5 Euro Tages ##lohn ihre Familie ernähren zu können . Über Augen und Ohren gehen diese Geschichten unter [MASK] Haut . Während des Films über ##kommen einen ab ##wechseln ##d Wut [MASK] Schuld , Scham und [MASK] ##ath ##ie . Über allem gekürzt [MASK] ##t das konstant ##e Gefühl der Fassung ##slos ##igkeit . Die größte Stärke des Films [SEP]\n",
            "02/01/2021 01:40:53 - INFO - root -   *** Example ***\n",
            "02/01/2021 01:40:53 - INFO - root -   tokens: [CLS] , 14 : 36 ( vor [MASK] [MASK] ) Ich wollte demnächst von Berlin nach Ro ##ed ##by , [MASK] [MASK] damit ich in [MASK] nicht ums ##tei ##gen muss . Schl ##imm . . . Was ist schlimm ? Was ist ICE 29 ##00 für ein [MASK] und ist der Um ##stieg in Hamburg garantiert ? 40 ##2 , 41 ##1 , ein [MASK] eben . Komm ##t auf [MASK] P [MASK] ##lichkeit [MASK] , garantiert [MASK] da gar n ##ix , das sollte ##st du [MASK] . . . Nein , ich weiß nicht [MASK] was sich die Bahn da [MASK] [MASK] für Gemein ##heiten aus ##denkt . Wo [MASK] auch [MASK] [MASK] ##igt ##ingenieur dann in Hamburg in einen Zug um , der schon steht und Sitz ##plätze belegt sind ? Kön ##nte sein [MASK] aber eine Reserv ##ierung sollte ##st du [MASK] ? ! Wies ##o sollte ich [MASK] Reserv ##ierung haben ? Wenn ich in Gesund ##brunnen einst ##eige , brauche ich ja wohl [MASK] . Wo ##zu so viel Wind ? Das kam ja bisher nicht so ##o oft vor . Was denn für Wind ? [MASK] frag ##e nur , was da passiert . Jetzt informiert [MASK] sich schon vor der Reise und das ist auch [MASK] . Wenn jetzt die Antwort käme , \\ \" das ##-, jetzt jeden zweiten Tag so , weil . . . \\ \" , dann plan ##e ich [MASK] anders . [MASK] [MASK] Antwort [MASK] \\ \" [SEP] \" Bahn ##lärm ##gegner starten Pet [MASK] | Rhein - Zeitung May ##en Region . Die Initiat ##oren sind überzeugt , dass die Menschen im Mittel ##rhein ##tal jetzt entschieden [MASK] den Bau des Wester ##wald - Tau ##n ##us - [MASK] ##s kämpfen müssen , wenn sie den Lärm durch Güter ##züge in Zukunft verb [MASK] wollen ##hofen Mit einer in wenigen Wochen starten \" [SEP]\n",
            "02/01/2021 01:40:53 - INFO - root -   *** Example ***\n",
            "02/01/2021 01:40:53 - INFO - root -   tokens: [CLS] \" @ [MASK] ##f ##ue ##hre ##r _ t ##im Das [MASK] ##st Du doch aber aus den S - Bahn [MASK] Doppel ##stock ##zügen kennen ; - ) \" [SEP] \" R ##T [MASK] Wo ##id ##wa ##id : [MASK] [MASK] ##I ##EG ##EL ##ON [MASK] ##IN ##E Was haben die Grünen getan um [MASK] [MASK] ins System Bahn zu bringen ? Die Schweiz zahlt 7 [MASK] [MASK] ##viel wie D ##t ##l für d [MASK] \" [SEP]\n",
            "02/01/2021 01:40:53 - INFO - root -   *** Example ***\n",
            "02/01/2021 01:40:53 - INFO - root -   tokens: [CLS] \" Hannover D ##I ##RE ##K ##T Bahn hat Sanierung ##pez ICE - Strecke verzö ##gert - Hannover ##sche Allgemeine Up ##dat ##e : ( Bahn [MASK] Sanierung der [MASK] - Strecke verzö ##gert - Hannover ##sche Allgemeine ) - mehr : http : / / ha ##n ##nov ##er ##di ##re ##kt . ne ##t [MASK] b ##ahn - [MASK] - san ##ierung - [MASK] - [MASK] ##ce - stre ##cke - verz ##oe [MASK] ##wechs ha [MASK] ##nov ##ers ##che - allgemeine / [MASK] [MASK] [MASK] Bahn hat Sanierung der ICE - [MASK] verzö ##gert ##Ha ##n ##nov [MASK] ##che Allgemeine ##Die Deutsche [MASK] ist bereits vor mehreren [MASK] zu einer kompletten Sanierung der verschl ##issen [MASK] ICE - Strecke Hannover - Kassel [MASK] worden , dem [MASK] nicht nachgekommen . Das geht aus [MASK] Fragen ##katalog hervor , den das [UNK] Bahn - Chef Grube : Sperr ##ung der Strecke Hannover - [MASK] unverm ##eid ##bar ##Bus ##iness - Panorama [MASK] de ##Ha ##n ##nov ##er [UNK] [MASK] : Deutsche Bahn AG zur Strecken ##sperr ##ung ##DI ##E W ##EL [MASK] ##Bahn reagiert [MASK] Kritik an Sperr ##ung zwischen Hannover und Kassel ##Finanz ##Nach ##richten . de ##Süd ##deutsche . de - H ##N ##A [MASK] de - [MASK] [MASK] Tra ##vel . de ##All ##e [MASK] Artikel [UNK] \" [SEP] [MASK] R [MASK] @ i ##C ##raft [MASK] ##M ##C : e ##y , @ DB _ Bahn führt mal so ##was wie ne ##n Internet ##freunde [MASK] Tarif ein . Dank ##e \" [SEP]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRJYm-QCgY0z"
      },
      "source": [
        "# The actual training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1w6Ex2-ZS-N"
      },
      "source": [
        "class BertForMTPostTraining(BertPreTrainedModel):\r\n",
        "    def __init__(self, config):\r\n",
        "        super(BertForMTPostTraining, self).__init__(config)\r\n",
        "        self.bert = BertModel(config)\r\n",
        "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\r\n",
        "        self.qa_outputs = torch.nn.Linear(config.hidden_size, 2)\r\n",
        "        self.apply(self.init_bert_weights)\r\n",
        "\r\n",
        "    def forward(self, mode, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None, start_positions=None, end_positions=None):\r\n",
        "        \r\n",
        "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\r\n",
        "        \r\n",
        "        if mode==\"review\":\r\n",
        "            prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\r\n",
        "            if masked_lm_labels is not None and next_sentence_label is not None:\r\n",
        "                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\r\n",
        "                masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))        \r\n",
        "                next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\r\n",
        "                total_loss = masked_lm_loss + next_sentence_loss\r\n",
        "                \r\n",
        "                return total_loss\r\n",
        "            else:\r\n",
        "                return prediction_scores, seq_relationship_score\r\n",
        "\r\n",
        "        elif mode==\"squad\":\r\n",
        "            logits = self.qa_outputs(sequence_output)\r\n",
        "            start_logits, end_logits = logits.split(1, dim=-1)\r\n",
        "            start_logits = start_logits.squeeze(-1)\r\n",
        "            end_logits = end_logits.squeeze(-1)\r\n",
        "\r\n",
        "            if start_positions is not None and end_positions is not None:\r\n",
        "                # If we are on multi-GPU, split add a dimension\r\n",
        "                if len(start_positions.size()) > 1:\r\n",
        "                    start_positions = start_positions.squeeze(-1)\r\n",
        "                if len(end_positions.size()) > 1:\r\n",
        "                    end_positions = end_positions.squeeze(-1)\r\n",
        "                # sometimes the start/end positions are outside our model inputs, we ignore these terms\r\n",
        "                ignored_index = start_logits.size(1)\r\n",
        "                start_positions.clamp_(0, ignored_index)\r\n",
        "                end_positions.clamp_(0, ignored_index)\r\n",
        "\r\n",
        "                loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)\r\n",
        "                start_loss = loss_fct(start_logits, start_positions)\r\n",
        "                end_loss = loss_fct(end_logits, end_positions)\r\n",
        "                qa_loss = (start_loss + end_loss) / 2\r\n",
        "                return qa_loss\r\n",
        "            else:\r\n",
        "                return start_logits, end_logits\r\n",
        "        else:\r\n",
        "            raise Exception(\"unknown mode.\")\r\n",
        "\r\n",
        "\r\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\r\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\r\n",
        "                    level=logging.INFO)\r\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up3NaA-iZUHy"
      },
      "source": [
        "def warmup_linear(x, warmup=0.002):\r\n",
        "    if x < warmup:\r\n",
        "        return x/warmup\r\n",
        "    return 1.0 - x"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cge4g_LIZYil",
        "outputId": "1cf9f03f-f507-4cb0-baef-a5919701cbfc"
      },
      "source": [
        "# Training arguments\r\n",
        "\r\n",
        "gradient_accumulation_steps = 2\r\n",
        "train_batch_size = int(16 / gradient_accumulation_steps)   \r\n",
        "review_data_dir = ''\r\n",
        "num_train_steps = 3\r\n",
        "fp16 = False\r\n",
        "learning_rate = 3e-5\r\n",
        "warmup_proportion = 0.1\r\n",
        "loss_scale = 2\r\n",
        "save_checkpoints_steps = 10000\r\n",
        "output_dir = ''\r\n",
        "random.seed(12345)\r\n",
        "np.random.seed(12345)\r\n",
        "torch.manual_seed(12345)\r\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f83914792a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pgUitTNZ9SC",
        "outputId": "9f508723-a475-4db6-9a17-9748b121ded1"
      },
      "source": [
        "review_train_examples = np.load(os.path.join(review_data_dir, \"germeval.npz\") )\r\n",
        "    \r\n",
        "# load bert pre-train data.\r\n",
        "review_train_data = TensorDataset(\r\n",
        "    torch.from_numpy(review_train_examples[\"input_ids\"]),\r\n",
        "    torch.from_numpy(review_train_examples[\"segment_ids\"]),\r\n",
        "    torch.from_numpy(review_train_examples[\"input_mask\"]),        \r\n",
        "    torch.from_numpy(review_train_examples[\"masked_lm_ids\"]),\r\n",
        "    torch.from_numpy(review_train_examples[\"next_sentence_labels\"]) )\r\n",
        "    \r\n",
        "review_train_dataloader = DataLoader(review_train_data, \r\n",
        "                                     sampler=RandomSampler(review_train_data), \r\n",
        "                                     batch_size = train_batch_size , \r\n",
        "                                     drop_last=True)\r\n",
        "\r\n",
        "# we do not have any validation for pretuning\r\n",
        "model = BertForMTPostTraining.from_pretrained(\"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased.tar.gz\") # \"bert-base-german-cased\"\r\n",
        "\r\n",
        "model.cuda()\r\n",
        "        \r\n",
        "# Prepare optimizer\r\n",
        "param_optimizer = [(k, v) for k, v in model.named_parameters() if v.requires_grad==True]\r\n",
        "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\r\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\r\n",
        "optimizer_grouped_parameters = [\r\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\r\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\r\n",
        "    ]\r\n",
        "t_total = num_train_steps\r\n",
        "        \r\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\r\n",
        "                             lr = learning_rate,\r\n",
        "                             warmup = warmup_proportion,\r\n",
        "                             t_total = t_total)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/01/2021 02:01:34 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased.tar.gz from cache at /root/.pytorch_pretrained_bert/4983b4079d9a070d00f914e5e669c1b1376f201367e3507366bc8a9de9d62fd0.d994c35add5d5ceb797a0bce40d17a4abb02c9c37b804e92c448d31d8de34734\n",
            "02/01/2021 02:01:34 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/4983b4079d9a070d00f914e5e669c1b1376f201367e3507366bc8a9de9d62fd0.d994c35add5d5ceb797a0bce40d17a4abb02c9c37b804e92c448d31d8de34734 to temp dir /tmp/tmp5i96vdyu\n",
            "02/01/2021 02:01:38 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "02/01/2021 02:01:42 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForMTPostTraining not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc2uLiWjaDsB"
      },
      "source": [
        "global_step=0\r\n",
        "step=0\r\n",
        "batch_loss=0.\r\n",
        "model.train()\r\n",
        "model.zero_grad()\r\n",
        "    \r\n",
        "training=True\r\n",
        "\r\n",
        "review_iter=iter(review_train_dataloader)\r\n",
        "    \r\n",
        "while training:\r\n",
        "    try:\r\n",
        "        batch = next(review_iter)\r\n",
        "    except:\r\n",
        "        review_iter=iter(review_train_dataloader)\r\n",
        "        batch = next(review_iter)\r\n",
        "        \r\n",
        "    batch = tuple(t.cuda() for t in batch)\r\n",
        "    \r\n",
        "    input_ids, segment_ids, input_mask, masked_lm_ids, next_sentence_labels = batch\r\n",
        "    \r\n",
        "    review_loss = model(\"review\", input_ids.long(), segment_ids.long(), input_mask.long(), masked_lm_ids.long(), next_sentence_labels.long(), None, None)\r\n",
        "\r\n",
        "\r\n",
        "    batch = tuple(t.cuda() for t in batch)\r\n",
        "    input_ids, segment_ids, input_mask, start_positions, end_positions = batch\r\n",
        "\r\n",
        "    loss=review_loss \r\n",
        "\r\n",
        "    if gradient_accumulation_steps > 1:\r\n",
        "        loss = loss / gradient_accumulation_steps\r\n",
        "    batch_loss+=loss\r\n",
        "    if fp16:\r\n",
        "        optimizer.backward(loss)\r\n",
        "    else:\r\n",
        "        loss.backward()\r\n",
        "    \r\n",
        "    if (step + 1) % gradient_accumulation_steps == 0:\r\n",
        "        # modify learning rate with special warm up BERT uses\r\n",
        "        lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\r\n",
        "        for param_group in optimizer.param_groups:\r\n",
        "            param_group['lr'] = lr_this_step\r\n",
        "        optimizer.step()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        global_step += 1\r\n",
        "        if global_step % 50 ==0:\r\n",
        "            logging.info(\"step %d batch_loss %f \", global_step, batch_loss)\r\n",
        "        batch_loss=0.\r\n",
        "\r\n",
        "        if global_step % save_checkpoints_steps==0:\r\n",
        "            model.float()\r\n",
        "            torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model_\"+str(global_step)+\".bin\") )\r\n",
        "            if args.fp16:\r\n",
        "                model.half()\r\n",
        "        if global_step>=num_train_steps:\r\n",
        "            training=False\r\n",
        "            break\r\n",
        "    step+=1\r\n",
        "model.float()\r\n",
        "torch.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\") )"
      ],
      "execution_count": 77,
      "outputs": []
    }
  ]
}