{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_AE_Task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30007caa60f84ca0a316f99a3fdbeefd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c3cdac682e5f49e4aa8d542963119493",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_96027d66a898475bad8c68415ca2cb6d",
              "IPY_MODEL_d2758d74dc6448818b8055c35032f07d"
            ]
          }
        },
        "c3cdac682e5f49e4aa8d542963119493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96027d66a898475bad8c68415ca2cb6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_63eac45023b341c28c20d4bfa1a3fd48",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 254728,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 254728,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72c1ded681f343009507a83f60609564"
          }
        },
        "d2758d74dc6448818b8055c35032f07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f4c6c16df54b45eb8befe41c1d1ba04c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 255k/255k [00:00&lt;00:00, 617kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee863dbff4684b75bfc9a5507e9e2ce2"
          }
        },
        "63eac45023b341c28c20d4bfa1a3fd48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72c1ded681f343009507a83f60609564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f4c6c16df54b45eb8befe41c1d1ba04c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee863dbff4684b75bfc9a5507e9e2ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HdsxsrSbTRY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d95a7a6b-5fd7-47ab-a265-47fdb06368d4"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Get the GPU device name.\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "# The device name should look like the following:\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cd5bb073bb20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Get the GPU device name.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# The device name should look like the following:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/test_util.py\u001b[0m in \u001b[0;36mgpu_device_name\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0;34m\"\"\"Returns the name of a GPU device if available or the empty string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"GPU\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/device_lib.py\u001b[0m in \u001b[0;36mlist_local_devices\u001b[0;34m(session_config)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mserialized_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   return [\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pywrap_device_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m   ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJUtV9LwBlYE",
        "outputId": "e4f2265b-8506-445a-dddd-99444c31fefb"
      },
      "source": [
        "# Identify and specify the GPU as the device\r\n",
        "import torch\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcYLIj4WBoNI",
        "outputId": "2fefe203-56d9-4a54-e1f0-fa2e2ca5af2b"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjTHOjrrFuMo"
      },
      "source": [
        "# !pip install pytorch_pretrained_bert\r\n",
        "# !pip install transformers==3.5.1\r\n",
        "import os\r\n",
        "import logging\r\n",
        "import argparse\r\n",
        "import random\r\n",
        "import json\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# from pytorch_pretrained_bert.tokenization import BertTokenizer\r\n",
        "from transformers import BertTokenizer\r\n",
        "from pytorch_pretrained_bert.modeling import BertModel, BertPreTrainedModel\r\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\r\n",
        "\r\n",
        "\r\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\r\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\r\n",
        "                    level = logging.INFO)\r\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LscW1xANNMa",
        "outputId": "ae70bdb9-b55e-4984-d263-a88661644ff6"
      },
      "source": [
        "# !pip show transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: transformers\n",
            "Version: 3.5.1\n",
            "Summary: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Patrick von Platen, Sylvain Gugger, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors\n",
            "Author-email: thomas@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: tokenizers, requests, tqdm, dataclasses, regex, filelock, sacremoses, sentencepiece, protobuf, packaging, numpy\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B48vgy9jsiO2"
      },
      "source": [
        "# **Functions and classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKx1-PEjGEdJ"
      },
      "source": [
        "def warmup_linear(x, warmup=0.002):\r\n",
        "    if x < warmup:\r\n",
        "        return x/warmup\r\n",
        "    return 1.0 - x\r\n",
        "\r\n",
        "class InputExample(object):\r\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\r\n",
        "        \"\"\"Constructs a InputExample.\r\n",
        "\r\n",
        "        Args:\r\n",
        "            guid: Unique id for the example.\r\n",
        "            text_a: string. The untokenized text of the first sequence. For single\r\n",
        "            sequence tasks, only this sequence must be specified.\r\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\r\n",
        "            Only must be specified for sequence pair tasks.\r\n",
        "            label: (Optional) string. The label of the example. This should be\r\n",
        "            specified for train and dev examples, but not for test examples.\r\n",
        "        \"\"\"\r\n",
        "        self.guid = guid\r\n",
        "        self.text_a = text_a\r\n",
        "        self.text_b = text_b\r\n",
        "        self.label = label\r\n",
        "\r\n",
        "def create_examples(lines, set_type):\r\n",
        "    examples = []\r\n",
        "    for (i, ids) in enumerate(lines[\"data\"]):\r\n",
        "        guid = \"%s-%s\" % (set_type, ids )\r\n",
        "        text_a = lines[\"data\"][ids]['tokens']\r\n",
        "        label = lines[\"data\"][ids]['labels']\r\n",
        "        examples.append(\r\n",
        "            InputExample(guid=guid, text_a=text_a, label=label) )\r\n",
        "    return examples   \r\n",
        "\r\n",
        "def read_json(input_file):\r\n",
        "    \"\"\"Reads a json file for tasks in sentiment analysis.\"\"\"\r\n",
        "    with open(input_file) as f:\r\n",
        "        return json.load(f)\r\n",
        "        \r\n",
        "def get_train_examples(data_dir):\r\n",
        "    \"\"\"See base class.\"\"\"\r\n",
        "    return create_examples(\r\n",
        "        read_json(data_dir), \"train\")\r\n",
        "\r\n",
        "def get_dev_examples(data_dir):\r\n",
        "    \"\"\"See base class.\"\"\"\r\n",
        "    return create_examples(\r\n",
        "        read_json(data_dir), \"dev\")    \r\n",
        "\r\n",
        "def get_test_examples(data_dir):\r\n",
        "    \"\"\"See base class.\"\"\"\r\n",
        "    return create_examples(\r\n",
        "        read_json(data_dir), \"test\")    \r\n",
        "    \r\n",
        "class BertForSequenceLabeling(BertPreTrainedModel):\r\n",
        "    def __init__(self, config, num_labels=3):\r\n",
        "        super(BertForSequenceLabeling, self).__init__(config)\r\n",
        "        self.num_labels = num_labels\r\n",
        "        self.bert = BertModel(config)\r\n",
        "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\r\n",
        "        self.apply(self.init_bert_weights)\r\n",
        "\r\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\r\n",
        "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\r\n",
        "        sequence_output = self.dropout(sequence_output)\r\n",
        "        logits = self.classifier(sequence_output)\r\n",
        "\r\n",
        "        if labels is not None:\r\n",
        "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\r\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n",
        "            return loss\r\n",
        "        else:\r\n",
        "            return logits\r\n",
        "\r\n",
        "\r\n",
        "class InputFeatures(object):\r\n",
        "    \"\"\"A single set of features of data.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\r\n",
        "        self.input_ids = input_ids\r\n",
        "        self.input_mask = input_mask\r\n",
        "        self.segment_ids = segment_ids\r\n",
        "        self.label_id = label_id\r\n",
        "\r\n",
        "\r\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, mode):\r\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\" #check later if we can merge this function with the SQuAD preprocessing \r\n",
        "    label_map = {}\r\n",
        "    for (i, label) in enumerate(label_list):\r\n",
        "        label_map[label] = i\r\n",
        "\r\n",
        "    features = []\r\n",
        "    for (ex_index, example) in enumerate(examples):\r\n",
        "        if mode!=\"ae\":\r\n",
        "            tokens_a = tokenizer.tokenize(example.text_a)\r\n",
        "        else: #only do subword tokenization.\r\n",
        "            tokens_a, labels_a, example.idx_map= tokenizer.subword_tokenize([token.lower() for token in example.text_a], example.label )\r\n",
        "\r\n",
        "        tokens_b = None\r\n",
        "        if example.text_b:\r\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\r\n",
        "\r\n",
        "        if tokens_b:\r\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\r\n",
        "            # length is less than the specified length.\r\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\r\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\r\n",
        "        else:\r\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\r\n",
        "            if len(tokens_a) > max_seq_length - 2:\r\n",
        "                tokens_a = tokens_a[0:(max_seq_length - 2)]\r\n",
        "\r\n",
        "        tokens = []\r\n",
        "        segment_ids = []\r\n",
        "        tokens.append(\"[CLS]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "        for token in tokens_a:\r\n",
        "            tokens.append(token)\r\n",
        "            segment_ids.append(0)\r\n",
        "        tokens.append(\"[SEP]\")\r\n",
        "        segment_ids.append(0)\r\n",
        "\r\n",
        "        if tokens_b:\r\n",
        "            for token in tokens_b:\r\n",
        "                tokens.append(token)\r\n",
        "                segment_ids.append(1)\r\n",
        "            tokens.append(\"[SEP]\")\r\n",
        "            segment_ids.append(1)\r\n",
        "\r\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "\r\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n",
        "        # tokens are attended to.\r\n",
        "        input_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "        # Zero-pad up to the sequence length.\r\n",
        "        while len(input_ids) < max_seq_length:\r\n",
        "            input_ids.append(0)\r\n",
        "            input_mask.append(0)\r\n",
        "            segment_ids.append(0)\r\n",
        "\r\n",
        "        assert len(input_ids) == max_seq_length\r\n",
        "        assert len(input_mask) == max_seq_length\r\n",
        "        assert len(segment_ids) == max_seq_length\r\n",
        "\r\n",
        "        if mode!=\"ae\":\r\n",
        "            label_id = label_map[example.label]\r\n",
        "        else:\r\n",
        "            label_id = [-1] * len(input_ids) #-1 is the index to ignore\r\n",
        "            #truncate the label length if it exceeds the limit.\r\n",
        "            lb=[label_map[label] for label in labels_a]\r\n",
        "            if len(lb) > max_seq_length - 2:\r\n",
        "                lb = lb[0:(max_seq_length - 2)]\r\n",
        "            label_id[1:len(lb)+1] = lb\r\n",
        "\r\n",
        "        features.append(\r\n",
        "                InputFeatures(\r\n",
        "                        input_ids=input_ids,\r\n",
        "                        input_mask=input_mask,\r\n",
        "                        segment_ids=segment_ids,\r\n",
        "                        label_id=label_id))\r\n",
        "    return features\r\n",
        "\r\n",
        "class ABSATokenizer(BertTokenizer):     \r\n",
        "    def subword_tokenize(self, tokens, labels): # for AE\r\n",
        "        split_tokens, split_labels= [], []\r\n",
        "        idx_map=[]\r\n",
        "        for ix, token in enumerate(tokens):\r\n",
        "            sub_tokens=self.wordpiece_tokenizer.tokenize(token)\r\n",
        "            for jx, sub_token in enumerate(sub_tokens):\r\n",
        "                split_tokens.append(sub_token)\r\n",
        "                if labels[ix]==\"B\" and jx>0:\r\n",
        "                    split_labels.append(\"I\")\r\n",
        "                else:\r\n",
        "                    split_labels.append(labels[ix])\r\n",
        "                idx_map.append(ix)\r\n",
        "        return split_tokens, split_labels, idx_map    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmHqMmKBsSY4"
      },
      "source": [
        "# **Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nHVcLuQGwC_"
      },
      "source": [
        "train_batch_size = 32            \r\n",
        "num_train_epochs = 4      \r\n",
        "max_seq_length = 100\r\n",
        "learning_rate = 3e-5\r\n",
        "warmup_proportion = 0.1\r\n",
        "do_valid = True\r\n",
        "pretrained_model = \"bert-base-german-cased\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULQuGOXDsK70"
      },
      "source": [
        "# **Final Preprocessing of train examples**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "30007caa60f84ca0a316f99a3fdbeefd",
            "c3cdac682e5f49e4aa8d542963119493",
            "96027d66a898475bad8c68415ca2cb6d",
            "d2758d74dc6448818b8055c35032f07d",
            "63eac45023b341c28c20d4bfa1a3fd48",
            "72c1ded681f343009507a83f60609564",
            "f4c6c16df54b45eb8befe41c1d1ba04c",
            "ee863dbff4684b75bfc9a5507e9e2ce2"
          ]
        },
        "id": "yw13aFIpQgWQ",
        "outputId": "529d930b-54c5-4da1-ac09-b588546df8d2"
      },
      "source": [
        "label_list = ['O', 'B', 'I']\r\n",
        "\r\n",
        "tokenizer = ABSATokenizer.from_pretrained(pretrained_model)\r\n",
        "train_examples = get_train_examples(data_dir = \"train.json\")\r\n",
        "num_train_steps = int(len(train_examples) / train_batch_size) * num_train_epochs\r\n",
        "\r\n",
        "train_features = convert_examples_to_features(examples = train_examples, \r\n",
        "                                              label_list = label_list, \r\n",
        "                                              max_seq_length = max_seq_length, \r\n",
        "                                              tokenizer = tokenizer, \r\n",
        "                                              mode = \"ae\")\r\n",
        "\r\n",
        "logger.info(\"***** Running training *****\")\r\n",
        "logger.info(\"  Num examples = %d\", len(train_examples))\r\n",
        "logger.info(\"  Batch size = %d\", train_batch_size)\r\n",
        "logger.info(\"  Num steps = %d\", num_train_steps)\r\n",
        "\r\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\r\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\r\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\r\n",
        "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\r\n",
        "\r\n",
        "train_data = TensorDataset(all_input_ids, all_segment_ids, all_input_mask, all_label_ids)\r\n",
        "\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 13:15:27 - INFO - filelock -   Lock 140096976734136 acquired on /root/.cache/torch/transformers/da299cdd121a3d71e1626f2908dda0d02658f42e925a3d6abd8273ec08cf41a6.31ccc255fc2bad3578089a3997f16b286498ba78c0adc43b5bb2a3f9a0d2c85c.lock\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30007caa60f84ca0a316f99a3fdbeefd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=254728.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 13:15:28 - INFO - filelock -   Lock 140096976734136 released on /root/.cache/torch/transformers/da299cdd121a3d71e1626f2908dda0d02658f42e925a3d6abd8273ec08cf41a6.31ccc255fc2bad3578089a3997f16b286498ba78c0adc43b5bb2a3f9a0d2c85c.lock\n",
            "02/09/2021 13:15:28 - INFO - __main__ -   ***** Running training *****\n",
            "02/09/2021 13:15:28 - INFO - __main__ -     Num examples = 2\n",
            "02/09/2021 13:15:28 - INFO - __main__ -     Batch size = 32\n",
            "02/09/2021 13:15:28 - INFO - __main__ -     Num steps = 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK4pd5mxsFLi"
      },
      "source": [
        "# **Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xw0cG1hQHUm",
        "outputId": "637e99ee-d1ec-4db6-ceec-819c9989f1b6"
      },
      "source": [
        "valid_examples = get_dev_examples(data_dir = \"dev.json\")\r\n",
        "valid_features= convert_examples_to_features(\r\n",
        "    valid_examples, label_list, max_seq_length, tokenizer, \"ae\")\r\n",
        "valid_all_input_ids = torch.tensor([f.input_ids for f in valid_features], dtype=torch.long)\r\n",
        "valid_all_segment_ids = torch.tensor([f.segment_ids for f in valid_features], dtype=torch.long)\r\n",
        "valid_all_input_mask = torch.tensor([f.input_mask for f in valid_features], dtype=torch.long)\r\n",
        "valid_all_label_ids = torch.tensor([f.label_id for f in valid_features], dtype=torch.long)\r\n",
        "valid_data = TensorDataset(valid_all_input_ids, valid_all_segment_ids, valid_all_input_mask, valid_all_label_ids)\r\n",
        "\r\n",
        "logger.info(\"***** Running validations *****\")\r\n",
        "logger.info(\"  Num orig examples = %d\", len(valid_examples))\r\n",
        "logger.info(\"  Num split examples = %d\", len(valid_features))\r\n",
        "logger.info(\"  Batch size = %d\",  train_batch_size)\r\n",
        "\r\n",
        "valid_sampler = SequentialSampler(valid_data)\r\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size= train_batch_size)    \r\n",
        "\r\n",
        "best_valid_loss=float('inf')\r\n",
        "valid_losses=[]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 13:15:34 - INFO - __main__ -   ***** Running validations *****\n",
            "02/09/2021 13:15:34 - INFO - __main__ -     Num orig examples = 3\n",
            "02/09/2021 13:15:34 - INFO - __main__ -     Num split examples = 3\n",
            "02/09/2021 13:15:34 - INFO - __main__ -     Batch size = 32\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bTMV2LCsBCf"
      },
      "source": [
        "# **Model Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6TeQeaWoIRx",
        "outputId": "60d68d7e-6bac-4c03-bad8-34fc9e0eb4d0"
      },
      "source": [
        "model = BertForSequenceLabeling.from_pretrained(\"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased.tar.gz\", num_labels = len(label_list) )\r\n",
        "model.cuda()\r\n",
        "# Prepare optimizer\r\n",
        "param_optimizer = [(k, v) for k, v in model.named_parameters() if v.requires_grad==True]\r\n",
        "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\r\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\r\n",
        "optimizer_grouped_parameters = [\r\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\r\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\r\n",
        "    ]\r\n",
        "t_total = 12 # num_train_steps\r\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\r\n",
        "                     lr= learning_rate,\r\n",
        "                     warmup= warmup_proportion,\r\n",
        "                     t_total=t_total)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 13:15:37 - INFO - pytorch_pretrained_bert.file_utils -   https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased.tar.gz not found in cache, downloading to /tmp/tmpr6zyibdm\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 407595242/407595242 [00:13<00:00, 29927693.73B/s]\n",
            "02/09/2021 13:15:51 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpr6zyibdm to cache at /root/.pytorch_pretrained_bert/4983b4079d9a070d00f914e5e669c1b1376f201367e3507366bc8a9de9d62fd0.d994c35add5d5ceb797a0bce40d17a4abb02c9c37b804e92c448d31d8de34734\n",
            "02/09/2021 13:15:52 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /root/.pytorch_pretrained_bert/4983b4079d9a070d00f914e5e669c1b1376f201367e3507366bc8a9de9d62fd0.d994c35add5d5ceb797a0bce40d17a4abb02c9c37b804e92c448d31d8de34734\n",
            "02/09/2021 13:15:52 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpr6zyibdm\n",
            "02/09/2021 13:15:52 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased.tar.gz from cache at /root/.pytorch_pretrained_bert/4983b4079d9a070d00f914e5e669c1b1376f201367e3507366bc8a9de9d62fd0.d994c35add5d5ceb797a0bce40d17a4abb02c9c37b804e92c448d31d8de34734\n",
            "02/09/2021 13:15:52 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/4983b4079d9a070d00f914e5e669c1b1376f201367e3507366bc8a9de9d62fd0.d994c35add5d5ceb797a0bce40d17a4abb02c9c37b804e92c448d31d8de34734 to temp dir /tmp/tmp2_4g_73_\n",
            "02/09/2021 13:15:56 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "02/09/2021 13:15:59 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceLabeling not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "02/09/2021 13:15:59 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceLabeling: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwFu6ZKxr3XM"
      },
      "source": [
        "# **Actual training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S89f-vBGppGu",
        "outputId": "b5f00bc2-dd39-4706-8c0d-fc4d41671445"
      },
      "source": [
        "global_step = 0\r\n",
        "model.train()\r\n",
        "for _ in range(num_train_epochs):\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "        batch = tuple(t.cuda() for t in batch)\r\n",
        "        input_ids, segment_ids, input_mask, label_ids = batch\r\n",
        "        loss = model(input_ids, segment_ids, input_mask, label_ids)\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\r\n",
        "        for param_group in optimizer.param_groups:\r\n",
        "            param_group['lr'] = lr_this_step\r\n",
        "        optimizer.step()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        global_step += 1\r\n",
        "        #>>>> perform validation at the end of each epoch .\r\n",
        "    if do_valid:\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            losses=[]\r\n",
        "            valid_size=0\r\n",
        "            for step, batch in enumerate(valid_dataloader):\r\n",
        "                batch = tuple(t.cuda() for t in batch) # multi-gpu does scattering it-self\r\n",
        "                input_ids, segment_ids, input_mask, label_ids = batch\r\n",
        "                loss = model(input_ids, segment_ids, input_mask, label_ids)\r\n",
        "                losses.append(loss.data.item()*input_ids.size(0) )\r\n",
        "                valid_size+=input_ids.size(0)\r\n",
        "            valid_loss=sum(losses)/valid_size\r\n",
        "            logger.info(\"validation loss: %f\", valid_loss)\r\n",
        "            valid_losses.append(valid_loss)\r\n",
        "        if valid_loss<best_valid_loss:\r\n",
        "            torch.save(model, \"model.pt\" )\r\n",
        "            best_valid_loss=valid_loss\r\n",
        "        model.train()\r\n",
        "if do_valid:\r\n",
        "    with open(\"valid.json\", \"w\") as fw:\r\n",
        "        json.dump({\"valid_losses\": valid_losses}, fw)\r\n",
        "else:\r\n",
        "    torch.save(model, \"model.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
            "02/09/2021 13:16:27 - INFO - __main__ -   validation loss: 1.342445\n",
            "02/09/2021 13:16:29 - INFO - __main__ -   validation loss: 0.399739\n",
            "02/09/2021 13:16:30 - INFO - __main__ -   validation loss: 0.271355\n",
            "02/09/2021 13:16:33 - INFO - __main__ -   validation loss: 0.325429\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKVeZkL8s1IS"
      },
      "source": [
        "# **Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnKOlbz2s3ag",
        "outputId": "ddc1d2b3-d789-4d1e-b715-ff21fefe14d9"
      },
      "source": [
        "eval_batch_size = 8\r\n",
        "eval_examples = get_test_examples(data_dir = \"test.json\")\r\n",
        "eval_features = convert_examples_to_features(eval_examples, label_list, max_seq_length, tokenizer, \"ae\")\r\n",
        "\r\n",
        "logger.info(\"***** Running evaluation *****\")\r\n",
        "logger.info(\"  Num examples = %d\", len(eval_examples))\r\n",
        "logger.info(\"  Batch size = %d\", eval_batch_size)\r\n",
        "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\r\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\r\n",
        "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\r\n",
        "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\r\n",
        "eval_data = TensorDataset(all_input_ids, all_segment_ids, all_input_mask, all_label_ids)\r\n",
        "# Run prediction for full data\r\n",
        "eval_sampler = SequentialSampler(eval_data)\r\n",
        "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=eval_batch_size)\r\n",
        "\r\n",
        "model = torch.load(\"model.pt\")\r\n",
        "model.cuda()\r\n",
        "model.eval()\r\n",
        "\r\n",
        "full_logits=[]\r\n",
        "full_label_ids=[]\r\n",
        "for step, batch in enumerate(eval_dataloader):\r\n",
        "    batch = tuple(t.cuda() for t in batch)\r\n",
        "    input_ids, segment_ids, input_mask, label_ids = batch\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "        logits = model(input_ids, segment_ids, input_mask)\r\n",
        "\r\n",
        "    logits = logits.detach().cpu().numpy()\r\n",
        "    label_ids = label_ids.cpu().numpy()\r\n",
        "\r\n",
        "    full_logits.extend(logits.tolist() )\r\n",
        "    full_label_ids.extend(label_ids.tolist() )\r\n",
        "\r\n",
        "\r\n",
        "with open(\"predictions_ae.json\", \"w\") as fw:\r\n",
        "    assert len(full_logits)==len(eval_examples)\r\n",
        "    #sort by original order for evaluation\r\n",
        "    recs={}\r\n",
        "    for qx, ex in enumerate(eval_examples):\r\n",
        "        recs[int(ex.guid.split(\"-\")[1]) ]={\"sentence\": ex.text_a, \"idx_map\": ex.idx_map, \"logit\": full_logits[qx][1:]} #skip the [CLS] tag.\r\n",
        "    full_logits=[recs[qx][\"logit\"] for qx in recs.keys()]\r\n",
        "    raw_X=[recs[qx][\"sentence\"] for qx in recs.keys()]\r\n",
        "    idx_map=[recs[qx][\"idx_map\"] for qx in recs.keys()]\r\n",
        "    json.dump({\"logits\": full_logits, \"raw_X\": raw_X, \"idx_map\": idx_map}, fw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "02/09/2021 13:17:28 - INFO - __main__ -   ***** Running evaluation *****\n",
            "02/09/2021 13:17:28 - INFO - __main__ -     Num examples = 2\n",
            "02/09/2021 13:17:28 - INFO - __main__ -     Batch size = 8\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSaJkFuZWc6Z",
        "outputId": "b3d55b75-43d6-4ade-fb15-837cc97ffe32"
      },
      "source": [
        "# lb=np.argmax(logit[1])\r\n",
        "# lb\r\n",
        "# logit[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.48275089263916, -1.4218887090682983, -1.606652855873108]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    }
  ]
}